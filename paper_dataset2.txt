paper_id	venue	authors	year	title	index_keys	author_keys	abstract
7D117592	Knowledge Discovery and Data Mining	usama fayyad + paul s bradley + cheng yang	2001	Efficient discovery of error-tolerant frequent itemsets in high dimensions	 + Decision support systems + collaborative filtering + sparse data + Retrieval tasks and goals + data mining + Information systems applications + Clustering and classification + Information retrieval + Data analytics + web browsing + Data mining + Clustering + Mathematics and statistics + Information systems + Physical sciences and engineering + Applied computing + transaction data + high dimensional data + clustering	Error-tolerant frequent itemset + clustering + high dimensions + query selectivity estimation + collaborative filtering	We present a generalization of frequent itemsets allowing for the notion of errors in the itemset definition. We motivate the problem and present an efficient algorithm that identifies error-tolerant frequent clusters of items in transactional data (customer-purchase data, web browsing data, text, etc.). The algorithm exploits sparseness of the underlying data to find large groups of items that are correlated over database records (rows). The notion of transaction coverage allows us to extend the algorithm and view it as a fast clustering algorithm for discovering segments of similar transactions in binary sparse data. We evaluate the new algorithm on three real-world applications: clustering high-dimensional data, query selectivity estimation and collaborative filtering. Results show that the algorithm consistently uncovers structure in large sparse databases that other traditional clustering algorithms fail to find.
779B84A1	Knowledge Discovery and Data Mining	pavel berkhin + jonathan d beche + dee jay randall	2001	Interactive path analysis of web site traffic	path analysis +  + linear regression + Web applications + Paths and connectivity problems + World Wide Web + Graph theory + Information systems + Web services + decision trees + noise reduction + Discrete mathematics + Graph algorithms + Mathematics of computing + logistic regression + feature selection	No keyword found	The goal of Path Analysis is to understand visitors' navigation of a Web site. The fundamental analysis component is a path. A path is a finite sequence of elements, typically representing URLs or groups of URLs. A full path is an abstraction of a visit or a session, which can contain attributes described below. Subpaths represent interesting subsequences of the full paths.Path Analysis provides user-configurable extraction, filtering, preprocessing, noise reduction, descriptive statistics and detailed analysis of three basic specific objects: elements, (sub)paths, and couples of elements. In each case, lists of frequent objects --- subject to particular filtering and sorting --- are available. We call the corresponding interactive tools Element, Path, and Couple Analyzers.We also allow in-depth exploration of individual elements, paths, and couples: Element Explorer investigates composition and convergence of traffic through an element and allows conditioning based on the number of preceding/succeeding steps. Path Explorer visualizes in and out flows of a path and attrition rate along the path. Couple Explorer presents distinct paths connecting couple elements, along with measures of their association and some additional statistics.
79F12BAC	Knowledge Discovery and Data Mining	bernhard seeger + jenspeter dittrich	2001	GESS: a scalable similarity-join algorithm for mining large data sets in high dimensional spaces	 + Cluster analysis + em algorithm + feature space + Learning paradigms + Information systems applications + Computing methodologies + principal component + Data mining + Unsupervised learning + Information systems + transaction data + high dimensional data + data replication + Machine learning + mixture models	No keyword found	The similarity join is an important operation for mining high-dimensional feature spaces. Given two data sets, the similarity join computes all tuples (x, y) that are within a distance &egr;.One of the most efficient algorithms for processing similarity-joins is the Multidimensional-Spatial Join (MSJ) by Koudas and Sevcik. In our previous work --- pursued for the two-dimensional case --- we found however that MSJ has several performance shortcomings in terms of CPU and I/O cost as well as memory-requirements. Therefore, MSJ is not generally applicable to high-dimensional data.In this paper, we propose a new algorithm named Generic External Space Sweep (GESS). GESS introduces a modest rate of data replication to reduce the number of expensive distance computations. We present a new cost-model for replication, an I/O model, and an inexpensive method for duplicate removal. The principal component of our algorithm is a highly flexible replication engine.Our analytical model predicts a tremendous reduction of the number of expensive distance computations by several orders of magnitude in comparison to MSJ (factor 107). In addition, the memory requirements of GESS are shown to be lower by several orders of magnitude. Furthermore, the I/O cost of our algorithm is by factor 2 better (independent from the fact whether replication occurs or not). Our analytical results are confirmed by a large series of simulations and experiments with synthetic and real high-dimensional data sets.
7A373959	Knowledge Discovery and Data Mining	jiawei han + wen jin + anthony k h tung	2001	Mining top-n local outliers in large databases	 + data mining + Information systems applications + Data mining + outlier detection + Nonparametric statistics + Information systems + nearest neighbor search + log likelihood + clustering + Probability and statistics + Mathematics of computing + cutting plane + Distribution functions	No keyword found	"Outlier detection is an important task in data mining with numerous applications, including credit card fraud detection, video surveillance, etc. A recent work on outlier detection has introduced a novel notion of local outlier in which the degree to which an object is outlying is dependent on the density of its local neighborhood, and each object can be assigned a Local Outlier Factor (LOF) which represents the likelihood of that object being an outlier. Although the concept of local outliers is a useful one, the computation of LOF values for every data objects requires a large number of &kgr;-nearest neighbors searches and can be computationally expensive. Since most objects are usually not outliers, it is useful to provide users with the option of finding only n most outstanding local outliers, i.e., the top-n data objects which are most likely to be local outliers according to their LOFs. However, if the pruning is not done carefully, finding top-n outliers could result in the same amount of computation as finding LOF for all objects. In this paper, we propose a novel method to efficiently find the top-n local outliers in large databases. The concept of ""micro-cluster"" is introduced to compress the data. An efficient micro-cluster-based local outlier mining algorithm is designed based on this concept. As our algorithm can be adversely affected by the overlapping in the micro-clusters, we proposed a meaningful cut-plane solution for overlapping data. The formal analysis and experiments show that this method can achieve good performance in finding the most outstanding local outliers."
778ADF35	Knowledge Discovery and Data Mining	agma j m traina + caetano traina + christos faloutsos + spiros papadimitriou	2001	Tri-plots: scalable tools for multidimensional data mining	 + Visualization + Life and medical sciences + data mining + information retrieval + data processing + patterns + Information systems applications + Human-centered computing + knowledge discovery + classification + Data mining + parallel processing + Visualization application domains + feature vector + Information systems + Geographic visualization + Applied computing + Spatial-temporal systems + Consumer health + scaling factor + Health informatics	No keyword found	We focus on the problem of finding patterns across two large, multidimensional datasets. For example, given feature vectors of healthy and of non-healthy patients, we want to answer the following questions: Are the two clouds of points separable? What is the smallest/largest pair-wise distance across the two datasets? Which of the two clouds does a new point (feature vector) come from?We propose a new tool, the tri-plot, and its generalization, the pq-plot, which help us answer the above questions. We provide a set of rules on how to interpret a tri-plot, and we apply these rules on synthetic and real datasets. We also show how to use our tool for classification, when traditional methods (nearest neighbor, classification trees) may fail.
6D25DF12	Knowledge Discovery and Data Mining	shuching chen + chengcui zhang + jeff strickrott + meiling shyu	2001	Multimedia Data Mining for Traffic Video Sequences	object tracking + background subtraction + data processing + traffic flow	Multimedia data mining + spatiotemporal relationships + multimedia augmented transition network (MATN) + object tracking	"
In this paper, a multimedia data mining framework for discovering important but previously unknown knowledge such as vehicle identification, traffic flow, and the spatio-temporal relations of the vehicles at the intersections from traffic video sequences is proposed. The proposed multimedia data mining framework analyzes the traffic video sequences by using background subtraction, image/video segmentation, object tracking, and modeling with multimedia augmented transition network (MATN) model and multimedia input strings, in the domain of traffic monitoring over an intersection. The spatio-temporal relationships of the vehicle objects in each frame are discovered and accurately captured and modeled. Such an additional level of sophistication enabled by the proposed multimedia data-mining framework in terms of spatio-temporal tracking generates a capability for automation. This capability alone can significantly influence and enhance current data processing and implementation strategies for several problems vis-Ã -vis traffic operations. A real-life traffic video sequence is used to illustrate the effectiveness of the proposed multimedia data mining framework.
"
75B3A900	Knowledge Discovery and Data Mining	heikki mannila + padhraic smyth + igor v cadez	2001	Probabilistic modeling of transaction data with applications to profiling, visualization, and prediction	 + association rule + em algorithm + Simulation types and techniques + Information systems applications + Computing methodologies + mixture model + Data mining + Electronic commerce + outlier detection + probabilistic model + Information systems + Applied computing + Modeling and simulation + transaction data + interactive visualization + mixture models	+mixture models+profiles+transaction data	"Transaction data is ubiquitous in data mining applications. Examples include market basket data in retail commerce, telephone call records in telecommunications, and Web logs of individual page-requests at Web sites. Profiling consists of using historical transaction data on individuals to construct a model of each individual's behavior. Simple profiling techniques such as histograms do not generalize well from sparse transaction data. In this paper we investigate the application of probabilistic mixture models to automatically generate profiles from large volumes of transaction data. In effect, the mixture model represents each individual's behavior as a linear combination of ""basis transactions."" We evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram-based techniques, as well as being relatively scalable, interpretable, and flexible. In addition we point to applications in outlier detection, customer ranking, interactive visualization, and so forth. The paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules."
781CA345	Knowledge Discovery and Data Mining	j neal richter + stephen d durbin + bikramjit banerjee + doug warner	2001	Mining user session data to facilitate user interaction with a customer service knowledge base in RightNow Web	knowledge base +  + collaborative filtering + Collaborative and social computing systems and tools + Collaborative and social computing + Distributed artificial intelligence + Information systems applications + Computing methodologies + Human-centered computing + World Wide Web + Information systems + clustering + Artificial intelligence + swarm intelligence + multi agent system	+Categories and Subject Descriptors H.2.8 [Database Applications+Data Mining+H.3.5 [Onl ine Information Services+1.2.11 [Distributed Artificial Intelligence+Intelligent agents+1.2.11 [Distributed Ar- tificial Intel l igence+Multiagent systems General Terms Human Factors+Algorithms Keywords User session+user data+self-help+customer service+multi- agent system	"RightNow Web is an integrated software package for web-based customer service that has, at its core, a database of answers to frequently asked questions (FAQs). One major design goal is to facilitate end-user interaction with this dynamic document collection, i.e. make it as easy and efficient as possible for users to browse the collection and locate desired information. To this end, we perform several types of analysis on the session tracking database that records user navigation histories. First, using both explicit and implicit measures of user satisfaction, we infer a ""solved count"" representing the average utility of an FAQ. Second, using the user navigation patterns we construct a link matrix representing connections between FAQs. The technique of building up the link matrix and using it to advise users on related information amounts to a form of the ""swarm intelligence"" method of finding optimal paths. Both solved count and the link matrix are continuously updated as users interact with the site; furthermore, they are periodically ""aged"" to emphasize recent activity. The synergistic combination of these techniques allows users to learn from the database in a more effective manner, as evidenced by usage statistics."
5919C2D7	Knowledge Discovery and Data Mining	mukkai s krishnamoorthy + mohammed j zaki + john r punin	2001	LOGML: Log Markup Language for Web Usage Mining	 + web usage mining + complex structure + web accessibility + data cleaning + markup language		"
Web Usage Mining refers to the discovery of interesting information from user navigational behavior as stored in web access logs. While extracting simple information from web logs is easy, mining complex structural information is very challenging. Data cleaning and preparation constitute a very significant effort before mining can even be applied. We propose two new XML applications, XGMML and LOGML to help us in this task. XGMML is a graph description language and LOGML is a web-log report description language. We generate a web graph in XGMML format for a web site using the web robot of the WWWPal system. We generate web-log reports in LOGML format for a web site from web log files and the web graph. We further illustrate the usefulness of LOGML in web usage mining; we show the simplicity with which mining algorithms (for extracting increasingly complex frequent patterns) can be specified and implemented efficiently using LOGML.
"
7525DF47	Knowledge Discovery and Data Mining	hang li + kenji yamanishi	2001	Mining from open answers in questionnaire data	 + Triggers and rules + association rule + survey data + correspondence analysis + association rules + Information systems applications + Data mining + customer relationship management + Information systems + Database management system engines + brand image + Data management systems + text mining	+Survey+Questionnaire Data+Open Question+Classification Rules+Association Rules+Correspondence Analysis 1. ANALYSIS OF OPEN ANSWERS	Surveys are an important part of marketing and customer relationship management, and open answers (i.e., answers to open questions) in particular may contain valuable information and provide an important basis for making business decisions. We have developed a text mining system that provides a new way for analyzing open answers in questionnaire data. The product is able to perform the following two functions: (A) accurate extraction of characteristics for individual analysis targets, (B) accurate extraction of the relationships among characteristics of analysis targets. In this paper, we describe the working of our text mining system. It employs two statistical learning techniques: rule analysis and Correspondence Analysis for performing the two functions. Our text mining system has already been put into use by a number of large corporations in Japan in the performance of text mining on various types of survey data, including open answers about brand images, open answers about company images, complaints about products, comments written on home pages, business reports, and help desk records. In this it has been found to be useful in forming a basis for effective business decisions.
8044D952	Knowledge Discovery and Data Mining	raymond j mooney + joydeep ghosh + sugato basu + krupakar v pasupuleti	2001	Evaluating the novelty of text-mined rules using lexical knowledge	 + Triggers and rules + data mining + Document capture + Database management system engines + Information systems + Applied computing + rule based + wordnet + Document management and text processing + Data management systems + text mining + Document analysis + semantic distance	+both human subjects and by our algorithm. By comput- ing correlation coefficients between pairs of human ratings and between human and automatic ratings+we found that	In this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet, a lexical knowledge-base of English words. We assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance, more is the novelty of the rule. The novelty of rules extracted by the DiscoTEX text-mining system on Amazon.com book descriptions were evaluated by both human subjects and by our algorithm. By computing correlation coefficients between pairs of human ratings and between human and automatic ratings, we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another. @Text mining
789DDF24	Knowledge Discovery and Data Mining	llew mason + ron kohavi + zijian zheng	2001	Real world performance of association rule algorithms	 + association rule + market basket analysis + data mining + association rules + Information systems applications + exponential growth + affinity analysis + Data mining + benchmark + Information systems	eol>Data Mining + Association Rules + Benchmark + Comparisons + Frequent Itemsets + Market Basket Analysis + Affinity Analysis	This study compares five well-known association rule algorithms using three real-world datasets and an artificial dataset. The experimental results confirm the performance improvements previously claimed by the authors on the artificial data, but some of these gains do not carry over to the real datasets, indicating overfitting of the algorithms to the IBM artificial dataset. More importantly, we found that the choice of algorithm only matters at support levels that generate more rules than would be useful in practice. For support levels that generate less than 1,000,000 rules, which is much more than humans can handle and is sufficient for prediction purposes where data is loaded into RAM, Apriori finishes processing in less than 10 minutes. On our datasets, we observed super-exponential growth in the number of rules. On one of our datasets, a 0.02% change in the support increased the number of rules from less than a million to over a billion, implying that outside a very narrow range of support values, the choice of algorithm is irrelevant.
7BD5198F	Knowledge Discovery and Data Mining	charu c aggarwal	2001	A human-computer cooperative system for effective high dimensional clustering	 + Retrieval tasks and goals + data mining + Information systems applications + Clustering and classification + Relational database query languages + Information retrieval + knowledge discovery + Data model extensions + Clustering + Data mining + Information systems + self organizing map + high dimensional data + Data management systems + Relational database model + Database design and models + Query languages	No keyword found	High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Therefore, techniques have recently been proposed to find clusters in hidden subspaces of the data. However, since the behavior of the data may vary considerably in different subspaces, it is often difficult to define the notion of a cluster with the use of simple mathematical formalizations. In fact, the meaningfulness and definition of a cluster is best characterized with the use of human intuition. In this paper, we propose a system which performs high dimensional clustering by effective cooperation between the human and the computer. The complex task of cluster creation is accomplished by a combination of human intuition and the computational support provided by the computer. The result is a system which leverages the best abilities of both the human and the computer in order to create very meaningful sets of clusters in high dimensionality.
7B6C0DF2	Knowledge Discovery and Data Mining	yungseop lee + andreas buja	2001	Data mining criteria for tree-based regression and classification	 + Cluster analysis + Trees + data mining + Learning paradigms + Information systems applications + Computing methodologies + classification tree + Graph theory + Regression analysis + Data mining + Unsupervised learning + Information systems + Statistical paradigms + Machine learning + Machine learning approaches + Discrete mathematics + Probability and statistics + Mathematics of computing + Factorization methods + Canonical correlation analysis	No keyword found	"This paper is concerned with the construction of regression and classification trees that are more adapted to data mining applications than conventional trees. To this end, we propose new splitting criteria for growing trees. Conventional splitting criteria attempt to perform well on both sides of a split by attempting a compromise in the quality of fit between the left and the right side. By contrast, we adopt a data mining point of view by proposing criteria that search for interesting subsets of the data, as opposed to modeling all of the data equally well. The new criteria do not split based on a compromise between the left and the right bucket; they effectively pick the more interesting bucket and ignore the other.As expected, the result is often a simpler characterization of interesting subsets of the data. Less expected is that the new criteria often yield whole trees that provide more interpretable data descriptions. Surprisingly, it is a ""flaw"" that works to their advantage: The new criteria have an increased tendency to accept splits near the boundaries of the predictor ranges. This so-called ""end-cut problem"" leads to the repeated peeling of small layers of data and results in very unbalanced but highly expressive and interpretable trees."
5932A388	Knowledge Discovery and Data Mining	joe ng + michael k ng + joshua zhexue huang + david w cheung + waiki ching	2001	A Cube Model and Cluster Analysis for Web Access Sessions	algorithm design + web accessibility + indexation + sequential analysis + cluster analysis + data structure + data representation + application server + data mining + three dimensions + categorical data + e commerce		Identification of the navigational patterns of casual visitors is an important step in online recommendation to convert casual visitors to customers in e-commerce. Clustering and sequential analysis are two primary techniques for mining navigational patterns from Web and application server logs. The characteristics of the log data and mining tasks require new data representation methods and analysis algorithms to be tested in the e-commerce environment. In this paper we present a cube model to represent Web access sessions for data mining. The cube model organizes session data into three dimensions. The COMPONENT dimension represents a session as a set of ordered components {c 1, c 2,..., c P }, in which each component c i indexes the ith visited page in the session. Each component is associated with a set of attributes describing the page indexed by it, such as the page ID, category and view time spent at the page. The attributes associated with each component are defined in the ATTRIBUTE dimension. The SESSION dimension indexes individual sessions. In the model, irregular sessions are converted to a regular data structure to which existing data mining algorithms can be applied while the order of the page sequences is maintained. A rich set of page attributes is embedded in the model for different analysis purposes. We also present some experimental results of using the partitional clustering algorithm to cluster sessions. Because the sessions are essentially sequences of categories, the k-modes algorithm designed for clustering categorical data and the clustering method using the Markov transition frequency (or probability) matrix, are used to cluster categorical sequences.
7B9C65A0	Knowledge Discovery and Data Mining	sholom m weiss + nitin indurkhya	2001	Solving regression problems with rule-based ensemble classifiers	Triggers and rules +  + regression tree + k means clustering + Retrieval tasks and goals + Information systems applications + Clustering and classification + Information retrieval + Regression analysis + Clustering + Data mining + decision rule + Database management system engines + Information systems + Statistical paradigms + rule based + Data management systems + log likelihood + clustering + Probability and statistics + Mathematics of computing + Robust regression	No keyword found	We describe a lightweight learning method that induces an ensemble of decision-rule solutions for regression problems. Instead of direct prediction of a continuous output variable, the method discretizes the variable by k-means clustering and solves the resultant classification problem. Predictions on new examples are made by averaging the mean values of classes with votes that are close in number to the most likely class. We provide experimental evidence that this indirect approach can often yield strong results for many applications, generally outperforming direct approaches such as regression trees and rivaling bagged regression trees.
77C11E4B	Knowledge Discovery and Data Mining	edwin m knorr + raymond t ng + ruben h zamar	2001	Robust space transformations for distance-based operations	robust estimator +  + Visualization + data mining + Information systems applications + Human-centered computing + randomized algorithm + outliers + outlier detection + Visualization application domains + Information systems + Theory of computation + Geographic visualization + Computational geometry + nearest neighbor search + Numerical analysis + Spatial-temporal systems + Mathematical analysis + Computation of transforms + Mathematics of computing + Randomness, geometry and discrete structures + robust statistics	+Data Mining+Outliers+Distance- based Operations+Robust Statistics+Robust Estimators	"For many KDD operations, such as nearest neighbor search, distance-based clustering, and outlier detection, there is an underlying &kgr;-D data space in which each tuple/object is represented as a point in the space. In the presence of differing scales, variability, correlation, and/or outliers, we may get unintuitive results if an inappropriate space is used.The fundamental question that this paper addresses is: ""What then is an appropriate space?"" We propose using a robust space transformation called the Donoho-Stahel estimator. In the first half of the paper, we show the key properties of the estimator. Of particular importance to KDD applications involving databases is the stability property, which says that in spite of frequent updates, the estimator does not: (a) change much, (b) lose its usefulness, or (c) require re-computation. In the second half, we focus on the computation of the estimator for high-dimensional databases. We develop randomized algorithms and evaluate how well they perform empirically. The novel algorithm we develop called the Hybrid-random algorithm is, in most cases, at least an order of magnitude faster than the Fixed-angle and Subsampling algorithms."
75CCACEF	Knowledge Discovery and Data Mining	yasuhiko morimoto	2001	Mining frequent neighboring class sets in spatial databases	 + Visualization + location based service + Information systems applications + Human-centered computing + euclidean distance + Data mining + Visualization application domains + Information systems + Theory of computation + mobile computer + Geographic visualization + Computational geometry + spatial database + Spatial-temporal systems + Randomness, geometry and discrete structures + mcmc	No keyword found	"We consider the problem of finding neighboring class sets. Objects of each instance of a neighboring class set are grouped using their Euclidean distances from each other. Recently, location-based services are growing along with mobile computing infrastructure such as cellular phones and PDAs. Therefore, we expect to see the development of spatial databases that contains very large number of access records including location information. The most typical type would be a database of point objects. Records of the objects may consist of ""requested service name,"" ""number of packet transmitted"" in addition to x and y coordinate values indicating where the request came from. The algorithm presented here efficiently finds sets of ""service names"" that were frequently close to each other in the spatial database. For example, it may find a frequent neighboring class set, where ""ticket"" and ""timetable"" are frequently requested close to each other. By recognizing this, location-based service providers can promote a ""ticket"" service for customers who access the ""timetable."""
802C44B0	Knowledge Discovery and Data Mining	yehuda koren + david harel	2001	Clustering spatial data using random walks	spatial data +  + Visualization + Probabilistic reasoning algorithms + Data mining + Information systems + random walk + Geographic visualization + Discrete mathematics + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + cluster analysis + Markov-chain Monte Carlo methods + Probabilistic algorithms + Retrieval tasks and goals + Information systems applications + Human-centered computing + Clustering and classification + Information retrieval + Graph theory + Clustering + Visualization application domains + spatial database + Spatial-temporal systems + log likelihood + clustering + Graph algorithms	No keyword found	Discovering significant patterns that exist implicitly in huge spatial databases is an important computational task. A common approach to this problem is to use cluster analysis. We propose a novel approach to clustering, based on the deterministic analysis of random walks on a weighted graph generated from the data. Our approach can decompose the data into arbitrarily shaped clusters of different sizes and densities, overcoming noise and outliers that may blur the natural decomposition of the data. The method requires only O(n log n) time, and one of its variants needs only constant space.
76D180FB	Knowledge Discovery and Data Mining	scott spangler + jeffrey thomas kreulen	2001	Knowledge base maintenance using knowledge gap analysis	knowledge base +  + Cluster analysis + distance metric + text clustering + Learning paradigms + Information systems applications + Computing methodologies + Information retrieval + knowledge management + Unsupervised learning + Information systems + cost effectiveness + Machine learning + clustering + text mining	+Categories and Subject Descriptors H.2.8 [Information Systems+Data Mining+H.3.3[Information Search & Retrieval	"As the web and e-business have proliferated, the practice of using customer facing knowledge bases to augment customer service and support operations has increased. This can be a very efficient, scalable and cost effective way to share knowledge. The effectiveness and cost savings are proportional to the utility of the information within the knowledge base and inversely proportional to the amount of labor required in maintaining the knowledge. To address this issue, we have developed an algorithm and methodology to increase the utility of the information within a knowledge base while greatly reducing the labor required.In this paper, we describe an implementation of an algorithm and methodology for comparing a knowledge base to a set of problem tickets to determine which categories and subcategories are not well addressed within the knowledge base. We utilize text clustering on problem ticket text to determine a set of problem categories. We then compare each knowledge base solution document to each problem category centroid using a cosine distance metric. The distance between the ""closest"" solution document and the corresponding centroid becomes the basis of that problem category's ""knowledge gap"". Our claim is that this gap metric serves as a useful method for quickly and automatically determining which problem categories have no relevant solutions in a knowledge base. We have implemented our approach, and we present the results of performing a knowledge gap analysis on a set of support center problem tickets."
0B94683F	Knowledge Discovery and Data Mining	junichi takeuchi + kenji yamanishi	2001	Discovering outlier filtering rules from un-labeled data	 + supervised learning + outlier detection + unsupervised learning		"

"
75CD4112	Knowledge Discovery and Data Mining	jeffery howard + d r mani + andrew betz + james h drew + piew datta	2001	Estimating business targets	 + Cluster analysis + nearest neighbor + Learning paradigms + Machine learning + Information systems applications + Computing methodologies + Data mining + nearest neighbor method + Unsupervised learning + Information systems	+Categories and Subject Descriptors H.2.8[Database Management+Database Applications--data mining+1.5.3[Pattem Recognition+Statistical Keywords Nearest neighbor	Determining and setting maximal revenue expectations or other business performance targets---whether it is for regional company divisions or individual customers---can have profound financial implications. Operational techniques are changed, staffing levels are altered and management attention is re-focused---all in the name of expectations. In practice these expectations are often derived in an ad hoc manner. To address this unsupervised task, we combine nearest neighbor methods and classical statistical methods and derive a new solution to the classical econometric task of frontier analysis. We apply our methodology to two real world business problems in Verizon, a major telecommunications provider in the United States, more specifically in the print yellow page division Verizon Information Services: (1) identifying under marketed customers for targeted upselling campaigns and focused sales attention, and (2) benchmarking regional directory divisions to incent performance improvements. Our analysis uncovers some commercially useful aspects of these domains and by conservative estimates can increase revenue by several million dollars in each domain.
7FDE291E	Knowledge Discovery and Data Mining	eser kandogan	2001	Visualizing multi-dimensional clusters, trends, and outliers using star coordinates	 + Visualization + Information systems applications + Human-centered computing + knowledge discovery + Data mining + Visualization application domains + Information systems + Geographic visualization + Spatial-temporal systems + hierarchical clustering + Probability and statistics + interactive visualization + Mathematics of computing + Distribution functions	No keyword found	Interactive visualizations are effective tools in mining scientific, engineering, and business data to support decision-making activities. Star Coordinates is proposed as a new multi-dimensional visualization technique, which supports various interactions to stimulate visual thinking in early stages of knowledge discovery process. In Star Coordinates, coordinate axes are arranged on a two-dimensional surface, where each axis shares the same origin point. Each multi-dimensional data element is represented by a point, where each attribute of the data contributes to its location through uniform encoding. Interaction features of Star Coordinates provide users the ability to apply various transformations dynamically, integrate and separate dimensions, analyze correlations of multiple dimensions, view clusters, trends, and outliers in the distribution of data, and query points based on data ranges. Our experience with Star Coordinates shows that it is particularly useful for the discovery of hierarchical clusters, and analysis of multiple factors providing insight in various real datasets including telecommunications churn.
75382B4E	Knowledge Discovery and Data Mining	w k heuser + martin j wieczorek + udo grimmer + edgar hotz + gholamreza nakhaeizadeh	2001	REVI-MINER, a KDD-environment for deviation detection and analysis of warranty and goodwill cost statements in automotive industry	 + automotive industry + data cleaning + data mining + process model + machine learning	+General Terms Management+Measurement+Economics. Keywords Data Mining+deviation detection+data cleaning	REVI-MINER is a KDD-environment which supports the detection and analysis of deviations in warranty and goodwill cost statements. The system was developed within the framework of a cooperation between DaimlerChrysler Research & Technology and Global Service and Parts (GSP) and is based upon the CRISP-DM methodology as a widely accepted process model for the solution of Data Mining problems. Also, we have implemented different approaches based on Machine learning and statistics which can be utilized for data cleaning in the preprocessing phase. The Data Mining models applied have been developed by using a statistical deviation detection approach. The tool supports controllers in their task of auditing the authorized repair shops. In this paper we describe the development phases which have led to REVI-MINER.
78D2DA0C	Knowledge Discovery and Data Mining	vasant honavar + dianne cook + doina caragea	2001	Gaining insights into support vector machine pattern classifiers using projection-based tour methods	 + visualization + support vector machines + Simulation types and techniques + Learning paradigms + Computing methodologies + classification + support vector machine + Modeling and simulation + Visual analytics + Supervised learning + Classification and regression trees + Machine learning + multivariate data + Machine learning approaches + Supervised learning by classification	+cation+support vector machines+tours+classification+mul	This paper discusses visual methods that can be used to understand and interpret the results of classification using support vector machines (SVM) on data with continuous real-valued variables. SVM induction algorithms build pattern classifiers by identifying a maximal margin separating hyperplane from training examples in high dimensional pattern spaces or spaces induced by suitable nonlinear kernel transformations over pattern spaces. SVM have been demonstrated to be quite effective in a number of practical pattern classification tasks. Since the separating hyperplane is defined in terms of more than two variables it is necessary to use visual techniques that can navigate the viewer through high-dimensional spaces. We demonstrate the use of projection-based tour methods to gain useful insights into SVM classifiers with linear kernels on 8-dimensional data.
7562FA36	Knowledge Discovery and Data Mining	christoph hueglin + francesco vannotti	2001	Data mining techniques to improve forecast accuracy in airline business	logistic regression model +  + decision tree + predictive model + prediction model + data mining + Information systems applications + classification tree + logistic regression + Information systems	+Categories and Subject Descriptors H.2.8 [Database Applications+Data Mining. General Terms Algorithms+Measurement+Performance. Keywords Decision tree+CART+logistic regression+predictive model	Predictive models developed by applying Data Mining techniques are used to improve forecasting accuracy in the airline business. In order to maximize the revenue on a flight, the number of seats available for sale is typically higher than the physical seat capacity (overbooking). To optimize the overbooking rate, an accurate estimation of the number of no-show passengers (passengers who hold a valid booking but do not appear at the gate to board for the flight) is essential. Currently, no-shows on future flights are estimated from the number of no-shows on historical flights averaged on booking class level. In this work, classification trees and logistic regression models are applied to estimate the probability that an individual passenger turns out to be a no-show. Passenger information stored in the reservation system of the airline is either directly used as explanatory variable or used to create attributes that have an impact on the probability of a passenger to be a no-show. The total number of no-shows in each booking class or on the total flight is then obtained by accumulating the individual no-show probabilities over the entity of interest. We show that this forecasting approach is more accurate than the currently used method. In addition, the selected models lead to a deepened insight into passenger behavior.
5EAE61D4	Knowledge Discovery and Data Mining	dimitrios katsaros + alexandros nanopoulos + yannis manolopoulos	2001	Exploiting Web Log Mining for Web Cache Enhancement	association rule + association rules + operating system + prediction	Prediction + Web Log Mining + Web Caching + Prefetching + Association rules	Improving the performance of the Web is a crucial requirement, since its popularity resulted in a large increase in the user perceived latency. In this paper, we describe a Web caching scheme that capitalizes on prefetching. Prefetching refers to the mechanism of deducing forthcoming page accesses of a client, based on access log information. Web log mining methods are exploited to provide effective prediction of Web-user accesses. The proposed scheme achieves a coordination between the two techniques (i.e., caching and prefetching). The prefetched documents are accommodated in a dedicated part of the cache, to avoid the drawback of incorrect replacement of requested documents. The requirements of the Web are taken into account, compared to the existing schemes for buffer management in database and operating systems. Experimental results indicate the superiority of the proposed method compared to the previous ones, in terms of improvement in cache performance.
77BD177B	Knowledge Discovery and Data Mining	bing liu + wynne hsu + yiming ma	2001	Identifying non-actionable association rules	 + Triggers and rules + association rule + prediction model + data mining + Information systems applications + Computing methodologies + Data mining + Information systems + Database management system engines + Modeling and simulation + Data management systems + Model development and analysis	+Non-actionable rules+rule interestingness	Building predictive models and finding useful rules are two important tasks of data mining. While building predictive models has been well studied, finding useful rules for action still presents a major problem. A main obstacle is that many data mining algorithms often produce too many rules. Existing research has shown that most of the discovered rules are actually redundant or insignificant. Pruning techniques have been developed to remove those spurious and/or insignificant rules. In this paper, we argue that being a significant rule (or a non-redundant rule), however, does not mean that it is a potentially useful rule for action. Many significant rules (unpruned rules) are in fact not actionable. This paper studies this issue and presents an efficient algorithm to identify these non-actionable rules. Experiment results on many real-life datasets show that the number of non-actionable rules is typically quite large. The proposed technique thus enables the user to focus on fewer rules and to be assured that the remaining rules are non-redundant and potentially useful for action.
772BA642	Knowledge Discovery and Data Mining	yao wang + dongping fang + john chen + christopher jeris + tom chiu	2001	A robust and scalable clustering algorithm for mixed type attributes in large database environment	 + Visualization + Retrieval tasks and goals + data mining + Information systems applications + Human-centered computing + Clustering and classification + Information retrieval + likelihood function + Clustering + Data mining + probabilistic model + Visualization application domains + Information systems + Geographic visualization + k means algorithm + Spatial-temporal systems + log likelihood + hierarchical clustering + clustering	+clusters+noisy data	Clustering is a widely used technique in data mining applications to discover patterns in the underlying data. Most traditional clustering algorithms are limited to handling datasets that contain either continuous or categorical attributes. However, datasets with mixed types of attributes are common in real life data mining problems. In this paper, we propose a distance measure that enables clustering data with both continuous and categorical attributes. This distance measure is derived from a probabilistic model that the distance between two clusters is equivalent to the decrease in log-likelihood function as a result of merging. Calculation of this measure is memory efficient as it depends only on the merging cluster pair and not on all the other clusters. Zhang et al [8] proposed a clustering method named BIRCH that is especially suitable for very large datasets. We develop a clustering algorithm using our distance measure based on the framework of BIRCH. Similar to BIRCH, our algorithm first performs a pre-clustering step by scanning the entire dataset and storing the dense regions of data records in terms of summary statistics. A hierarchical clustering algorithm is then applied to cluster the dense regions. Apart from the ability of handling mixed type of attributes, our algorithm differs from BIRCH in that we add a procedure that enables the algorithm to automatically determine the appropriate number of clusters and a new strategy of assigning cluster membership to noisy data. For data with mixed type of attributes, our experimental results confirm that the algorithm not only generates better quality clusters than the traditional k-means algorithms, but also exhibits good scalability properties and is able to identify the underlying number of clusters in the data correctly. The algorithm is implemented in the commercial data mining tool Clementine 6.0 which supports the PMML standard of data mining model deployment.
7F2BCAD8	Knowledge Discovery and Data Mining	inderjit s dhillon	2001	Co-clustering documents and words using bipartite spectral graph partitioning	 + Retrieval tasks and goals + document clustering + Information systems applications + Clustering and classification + Information retrieval + Graph theory + bipartite graph + Clustering + Data mining + Information systems + Numerical analysis + Mathematical analysis + Computations on matrices + Discrete mathematics + log likelihood + clustering + Graph algorithms + Mathematics of computing + graph partitioning + complete graph	No keyword found	Both document clustering and word clustering are well studied problems. Most existing algorithms cluster documents and words separately but not simultaneously. In this paper we present the novel idea of modeling the document collection as a bipartite graph between documents and words, using which the simultaneous clustering problem can be posed as a bipartite graph partitioning problem. To solve the partitioning problem, we use a new spectral co-clustering algorithm that uses the second left and right singular vectors of an appropriately scaled word-document matrix to yield good bipartitionings. The spectral algorithm enjoys some optimality properties; it can be shown that the singular vectors solve a real relaxation to the NP-complete graph bipartitioning problem. We present experimental results to verify that the resulting co-clustering algorithm works well in practice.
750AE0C9	Knowledge Discovery and Data Mining	alina beygelzimer + sheng ma + changshing perng	2001	Fast ordering of large categorical datasets for better visualization	 + Simulation types and techniques + Modeling and simulation + Visual analytics + wordnet + Data management systems + Computing methodologies + categorical data + Data model extensions + Database design and models + semantic distance + Information systems	No keyword found	An important issue in visualizing categorical data is how to order categorical values. The focus of this paper is on constructing such orderings efficiently without compromising their visual quality.
7A36A062	Knowledge Discovery and Data Mining	jiong yang + philip s yu + wei wang	2001	Infominer: mining surprising periodic patterns	information gain + computational biology +  + association rule + Learning paradigms + Information systems applications + Computing methodologies + Data mining + Information systems + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + Supervised learning by classification	No keyword found	In this paper, we focus on mining surprising periodic patterns in a sequence of events. In many applications, e.g., computational biology, an infrequent pattern is still considered very significant if its actual occurrence frequency exceeds the prior expectation by a large margin. The traditional metric, such as support, is not necessarily the ideal model to measure this kind of surprising patterns because it treats all patterns equally in the sense that every occurrence carries the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence. A more suitable measurement, information, is introduced to naturally value the degree of surprise of each occurrence of a pattern as a continuous and monotonically decreasing function of its probability of occurrence. This would allow patterns with vastly different occurrence probabilities to be handled seamlessly. As the accumulated degree of surprise of all repetitions of a pattern, the concept of information gain is proposed to measure the overall degree of surprise of the pattern within a data sequence. The bounded information gain property is identified to tackle the predicament caused by the violation of the downward closure property by the information gain measure and in turn provides an efficient solution to this problem. Empirical tests demonstrate the efficiency and the usefulness of the proposed model.
795A137C	Knowledge Discovery and Data Mining	geoffrey i webb	2001	Discovering associations with numeric variables	 + association rule + Machine learning + Information systems applications + Computing methodologies + Information retrieval + Data mining + Information systems	+Impact Rule+Association Rule+Numeric Data+Search	This paper further develops Aumann and Lindell's [3] proposal for a variant of association rules for which the consequent is a numeric variable. It is argued that these rules can discover useful interactions with numeric data that cannot be discovered directly using traditional association rules with discretization. Alternative measures for identifying interesting rules are proposed. Efficient algorithms are presented that enable these rules to be discovered for dense data sets for which application of Auman and Lindell's algorithm is infeasible.
769E32EC	Knowledge Discovery and Data Mining	w nick street + yongseog kim	2001	A streaming ensemble algorithm (SEA) for large-scale classification	 + Information systems applications + Information systems	+Categories and Subject Descriptors H.2.8 [ In format ion Systems+Database Management-- Database Applications[Data Mining] General Terms	Ensemble methods have recently garnered a great deal of attention in the machine learning community. Techniques such as Boosting and Bagging have proven to be highly effective but require repeated resampling of the training data, making them inappropriate in a data mining context. The methods presented in this paper take advantage of plentiful data, building separate classifiers on sequential chunks of training points. These classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy. The result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data, requires approximately constant memory, and adjusts quickly to concept drift.
7C7FCC3B	Knowledge Discovery and Data Mining	philip s yu + bing liu + yiming ma	2001	Discovering unexpected information from your competitors' web sites	 + web mining + Sorting and searching + Web applications + World Wide Web + Business process management + Information systems + Theory of computation + Enterprise computing + Applied computing + information extraction + Web services + Data structures design and analysis + Design and analysis of algorithms	No keyword found	Ever since the beginning of the Web, finding useful information from the Web has been an important problem. Existing approaches include keyword-based search, wrapper-based information extraction, Web query and user preferences. These approaches essentially find information that matches the user's explicit specifications. This paper argues that this is insufficient. There is another type of information that is also of great interest, i.e., unexpected information, which is unanticipated by the user. Finding unexpected information is useful in many applications. For example, it is useful for a company to find unexpected information bout its competitors, e.g., unexpected services and products that its competitors offer. With this information, the company can learn from its competitors and/or design counter measures to improve its competitiveness. Since the number of pages of a typical commercial site is very large and there are also many relevant sites (competitors), it is very difficult for a human user to view each page to discover the unexpected information. Automated assistance is needed. In this paper, we propose a number of methods to help the user find various types of unexpected information from his/her competitors' Web sites. Experiment results show that these techniques are very useful in practice and also efficient.
7C2A69C6	Knowledge Discovery and Data Mining	heikki mannila + marko salmenkivi	2001	Finding simple intensity descriptions from event sequence data	 + markov chain monte carlo + web accessibility + posterior distribution + maximum likelihood + Information systems applications + bayesian approach + Probability and statistics + Mathematics of computing + mcmc + Information systems	+Categories and Subject Descriptors H.2.8 [ In format ion Systems+Data Mining+G.3 [Probability and Statistics+Stochastic Processes Keywords event sequence+intensity modeling+MCMC	Sequences of events are an important type of data arising in various applications, including telecommunications, bio-statistics, web access analysis, etc. A basic approach to modeling such sequences is to find the underlying intensity functions describing the expected number of events per time unit. Typically, the intensity functions are assumed to be piecewise constant. We therefore consider different ways of fitting intensity models to event sequence data. We start by considering a Bayesian approach using Markov chain Monte Carlo (MCMC) methods with varying number of pieces. These methods can be used to produce posterior distributions on the intensity functions and they can also accomodate covariates. The drawback is that they are computationally intensive and thus are not very suitable for data mining applications in which large numbers of intensity functions have to be estimated. We consider dynamic programming approaches to finding the change points in the intensity functions. These methods can find the maximum likelihood intensity function in O(n2k) time for a sequence of n events and k different pieces of intensity. We show that simple heuristics can be used to prune the number of potential change points, yielding speedups of several orders of magnitude. The results of the improved dynamic programming method correspond very closely with the posterior averages produced by the MCMC methods.
7C373092	Knowledge Discovery and Data Mining	daryl pregibon + william h dumouchel	2001	Empirical bayes screening for multi-item associations	 + Decision support systems + association rule + data mining + association rules + Information systems applications + adverse event + knowledge discovery + statistical model + Data analytics + confidence limit + Data mining + low frequency + Mathematics and statistics + Information systems + Physical sciences and engineering + Applied computing + poisson model + statistical models	eol>Association rules + empirical Bayes methods + gamma-Poisson model + market basket problem + shrinkage estimation	"This paper considers the framework of the so-called ""market basket problem"", in which a database of transactions is mined for the occurrence of unusually frequent item sets. In our case, ""unusually frequent"" involves estimates of the frequency of each item set divided by a baseline frequency computed as if items occurred independently. The focus is on obtaining reliable estimates of this measure of interestingness for all item sets, even item sets with relatively low frequencies. For example, in a medical database of patient histories, unusual item sets including the item ""patient death"" (or other serious adverse event) might hopefully be flagged with as few as 5 or 10 occurrences of the item set, it being unacceptable to require that item sets occur in as many as 0.1% of millions of patient reports before the data mining algorithm detects a signal. Similar considerations apply in fraud detection applications. Thus we abandon the requirement that interesting item sets must contain a relatively large fixed minimal support, and adopt a criterion based on the results of fitting an empirical Bayes model to the item set counts. The model allows us to define a 95% Bayesian lower confidence limit for the ""interestingness"" measure of every item set, whereupon the item sets can be ranked according to their empirical Bayes confidence limits. For item sets of size J > 2, we also distinguish between multi-item associations that can be explained by the observed J(J-1)/2 pairwise associations, and item sets that are significantly more frequent than their pairwise associations would suggest. Such item sets can uncover complex or synergistic mechanisms generating multi-item associations. This methodology has been applied within the U.S. Food and Drug Administration (FDA) to databases of adverse drug reaction reports and within AT&T to customer international calling histories. We also present graphical techniques for exploring and understanding the modeling results."
8156EEAE	Knowledge Discovery and Data Mining	zoran obradovic + aleksandar lazarevic	2001	The distributed boosting algorithm	 + boosting + Parallel and distributed DBMSs + Sorting and searching + Learning paradigms + Computing methodologies + Database management system engines + Information systems + Theory of computation + Supervised learning + Classification and regression trees + Machine learning + Data structures design and analysis + Machine learning approaches + Data management systems + Supervised learning by classification + Design and analysis of algorithms	eol>Boosting + distributed learning + classifier ensembles	In this paper, we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that cannot be merged at a single location. Our distributed boosting algorithm can also be used as a parallel classification technique, where a massive database that cannot fit into main computer memory is partitioned into disjoint subsets for a more efficient analysis. In the proposed method, at each boosting round the classifiers are first learned from disjoint datasets and then exchanged amongst the sites. Finally the classifiers are combined into a weighted voting ensemble on each disjoint data set. The ensemble that is applied to an unseen test set represents an ensemble of ensembles built on all distributed sites. In experiments performed on four large data sets the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring less memory and less computational time. In addition, the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases.
7D59DB12	Knowledge Discovery and Data Mining	christos faloutsos + zhiqiang bi + flip korn	2001	"The ""DGX"" distribution for mining massive, skewed data"	 + data mining + Information systems applications + Computing methodologies + skewed distribution + zipf s law + Data mining + outlier detection + maximum likelihood estimation + Language resources + Information systems + Modeling and simulation + lognormal distribution + probability distribution + maximum likelihood estimate + Probability and statistics + Natural language processing + Mathematics of computing + Model development and analysis + Artificial intelligence + Model verification and validation + Distribution functions	DGX + Zipf's law + rank-frequency plot + frequency-count plot + maximum likelihood estimation + lognormal distribution + outlier detection	Skewed distributions appear very often in practice. Unfortunately, the traditional Zipf distribution often fails to model them well. In this paper, we propose a new probability distribution, the Discrete Gaussian Exponential (DGX), to achieve excellent fits in a wide variety of settings; our new distribution includes the Zipf distribution as a special case. We present a statistically sound method for estimating the DGX parameters based on maximum likelihood estimation (MLE). We applied DGX to a wide variety of real world data sets, such as sales data from a large retailer chain, us-age data from AT&T, and Internet clickstream data; in all cases, DGX fits these distributions very well, with almost a 99% correlation coefficient in quantile-quantile plots. Our algorithm also scales very well because it requires only a single pass over the data. Finally, we illustrate the power of DGX as a new tool for data mining tasks, such as outlier detection.
7C9F2BFD	Knowledge Discovery and Data Mining	joseph c vanderwaart + ricardo silva + jonathan moody	2001	Data filtering for automatic classification of rocks from reflectance spectra	genetic algorithm +  + algorithms + performance + Machine learning + Computing methodologies + mcmc	No keyword found	The ability to identify the mineral composition of rocks and soils is an important tool for the exploration of geological sites. For instance, NASA intends to design robots that are sufficiently autonomous to perform this task on planetary missions. Spectrometer readings provide one important source of data for identifying sites with minerals of interest. Reflectance spectrometers measure intensities of light reflected from surfaces over a range of wavelengths. Spectral intensity patterns may in some cases be sufficiently distinctive for proper identification of minerals or classes of minerals. For some mineral classes, carbonates for example, specific short spectral intervals are known to carry a distinctive signature. Finding similar distinctive spectral ranges for other mineral classes is not an easy problem. We propose and evaluate data-driven techniques that automatically search for spectral ranges optimized for specific minerals. In one set of studies, we partition the whole interval of wavelengths available in our data into sub-intervals, or bins, and use a genetic algorithm to evaluate a candidate selection of subintervals. As alternatives to this computationally expensive search technique, we present an entropy-based heuristic that gives higher scores for wavelengths more likely to distinguish between classes, as well as other greedy search procedures. Results are presented for four different classes, showing reasonable improvements in identifying some, but not all, of the mineral classes tested.
78E12268	Knowledge Discovery and Data Mining	charu c aggarwal + srinivasan parthasarathy	2001	Mining massively incomplete data sets by conceptual reconstruction	 + Triggers and rules + Visualization + data mining + Information systems applications + Human-centered computing + knowledge discovery + Data mining + Visualization application domains + Information systems + Database management system engines + self organizing map + Data management systems + Scientific visualization	No keyword found	Incomplete data sets have become almost ubiquitous in a wide variety of application domains. Common examples can be found in climate and image data sets, sensor data sets and medical data sets. The incompleteness in these data sets may arise from a number of factors: in some cases it may simply be a reflection of certain measurements not being available at the time; in others the information may be lost due to partial system failure; or it may simply be a result of users being unwilling to specify attributes due to privacy concerns. When a significant fraction of the entries are missing in all of the attributes, it becomes very difficult to perform any kind of reasonable extrapolation on the original data. For such cases, we introduce the novel idea of conceptual reconstruction, in which we create effective conceptual representations on which the data mining algorithms can be directly applied. The attraction behind the idea of conceptual reconstruction is to use the correlation structure of the data in order to express it in terms of concepts rather the original dimensions. As a result, the reconstruction procedure estimates only those conceptual aspects of the data which can be mined from the incomplete data set, rather than force errors created by extrapolation. We demonstrate the effectiveness of the approach on a variety of real data sets.
750A871B	Knowledge Discovery and Data Mining	junichi takeuchi + kenji yamanishi	2001	Discovering outlier filtering rules from unlabeled data: combining a supervised learner with an unsupervised learner	 + association rule + Retrieval tasks and goals + supervised learning + Sorting and searching + Information retrieval + Information extraction + outlier detection + unsupervised learning + Information systems + Theory of computation + Document filtering + Data structures design and analysis + Probability and statistics + Mathematics of computing + Design and analysis of algorithms + Distribution functions	No keyword found	This paper is concerned with the problem of detecting outliers from unlabeled data. In prior work we have developed SmartSifter, which is an on-line outlier detection algorithm based on unsupervised learning from data. On the basis of SmartSifter this paper yields a new framework for outlier filtering using both supervised and unsupervised learning techniques iteratively in order to make the detection process more effective and more understandable. The outline of the framework is as follows: In the first round, for an initial dataset, we run SmartSifter to give each data a score, with a high score indicating a high possibility of being an outlier. Next, giving positive labels to a number of higher scored data and negative labels to a number of lower scored data, we create labeled examples. Then we construct an outlier filtering rule by supervised learning from them. Here the rule is generated based on the principle of minimizing extended stochastic complexity. In the second round, for a new dataset, we filter the data using the constructed rule, then among the filtered data, we run SmartSifter again to evaluate the data in order to update the filtering rule. Applying of our framework to the network intrusion detection, we demonstrate that 1) it can significantly improve the accuracy of SmartSifter, and 2) outlier filtering rules can help the user to discover a general pattern of an outlier group.
7635CA7C	Knowledge Discovery and Data Mining	bryan nelson + fateh a tipu + eric bibelnieks + chidanand apte + ramesh natarajan + d s campbell + edwin p d pednault	2001	Segmentation-based modeling for advanced targeted marketing	 + decision tree + consumer behavior + decision trees + linear regression + Information systems applications + statistical model + logistic regression + business intelligence + feature selection + Information systems	+Categories and Subject Descriptors H.2.8 [Database Applications+Data mining. General Terms Algorithms+Measurement+Performance. Keywords Segmentation-based models	Fingerhut Business Intelligence (BI) has a long and successful history of building statistical models to predict consumer behavior. The models constructed are typically segmentation-based models in which the target audience is split into subpopulations (i.e., customer segments) and individually tailored statistical models are then developed for each segment. Such models are commonly employed in the direct-mail industry; however, segmentation is often performed on an ad-hoc basis without directly considering how segmentation affects the accuracy of the resulting segment models. Fingerhut BI approached IBM Research with the problem of how to build segmentation-based models more effectively so as to maximize predictive accuracy. The IBM Advanced Targeted Marketing-Single EventsTM (IBM ATM-SETM) solution is the result of IBM Research and Fingerhut BI directing their efforts jointly towards solving this problem. This paper presents an evaluation of ATM-SE's modeling capabilities using data from Fingerhut's catalog mailings.
NE8801	Knowledge Discovery and Data Mining	Charles Elkan	2001	Magical thinking in data mining: lessons from CoIL challenge 2000.	 + Information systems applications + Computing methodologies + Regression analysis + Data mining + Information systems + Statistical paradigms + Machine learning + Machine learning approaches + Probability and statistics + Mathematics of computing + Factorization methods + Canonical correlation analysis + Distribution functions	No keyword found	CoIL challenge 2000 was a supervised learning contest that attracted 43 entries. The authors of 29 entries later wrote explanations of their work. This paper discusses these reports and reaches three main conclusions. First, naive Bayesian classifiers remain competitive in practice: they were used by both the winning entry and the next best entry. Second, identifying feature interactions correctly is important for maximizing predictive accuracy: this was the difference between the winning classifier and all others. Third and most important, too many researchers and practitioners in data mining do not appreciate properly the issue of statistical significance and the danger of overfitting. Given a dataset such as the one for the CoIL contest, it is pointless to apply a very complicated learning algorithm, or to perform a very time-consuming model search. In either ease, one is likely to overfit the training data and to fool oneself in estimating predictive accuracy and in discovering useful correlations.
7FE29CCC	Knowledge Discovery and Data Mining	hank hoek + teresa mah + ying li	2001	Funnel report mining for the MSN network	 + Trees + correspondence analysis + data mining + association rules + Information systems applications + Information retrieval + Graph theory + Data mining + Information systems + web usage mining + Information storage systems + Discrete mathematics + text mining + Mathematics of computing	No keyword found	Data mining research has long concentrated on the five main areas: clustering, association discovery, classification, forecasting and sequential patterns. Web data mining projects are concerned mainly with text mining, user segmentation, forecasting web usage and analyzing users' clickstream patterns. We present a new type of web usage mining called funnel analysis or funnel report mining. A funnel report is a study of the retention behavior among a series of pages or sites. For example, of all hits on the home page of www.msn.com, what percentages of those are followed by hits to moneycentral.msn.com? What percentage of www.msn.com hits are followed by moneycentral.msn.com, and then www.msnbc.com? What are the most interesting funnels starting with www.msn.com? Where does the greatest drop off rate occur after a user has hit MSNBC? Funnel reports are extremely useful in e-business because they give product planners an idea of how usable and well-structured their site is. From our experience performing web usage mining for the MSN network of sites, funnel reports are requested even more than user segmentation analyses, site affiliation studies and classification exercises. In this paper, we define a framework for funnel analysis and provide a tree-based solution we have been using successfully to extract all relevant funnels using only one scan of the data file.
770CA8BE	Knowledge Discovery and Data Mining	dmitry a pavlov + padhraic smyth	2001	Probabilistic query models for transaction data	 + bayesian network + data structure + Probabilistic reasoning algorithms + mixture model + Database management system engines + Information systems + Database query processing + Theory and algorithms for application domains + Theory of computation + personalization + Database query processing and optimization (theory) + Data management systems + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + clickstream data + Database theory + Markov-chain Monte Carlo methods + data preprocessing + Probabilistic algorithms + Information retrieval + model building + Information retrieval query processing + transaction data	No keyword found	We investigate the application of Bayesian networks, Markov random fields, and mixture models to the problem of query answering for transaction data sets. We formulate two versions of the querying problem: the query selectivity estimation (i.e., finding exact counts for tuples in a data set) and the query generalization problem (i.e., computing the probability that a tuple will occur in new data). We show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an ADTree data structure. In an extension of our earlier work on this topic we propose several new schemes for query answering based on the compressed representation that avoid direct scans of the data at query time. Experimental results on real-world transaction data sets provide insights into various tradeoffs involving the offline time for model-building, the online time for query-answering, the memory footprint of the compressed data, and the accuracy of the estimate provided to the query.
76524BE9	Knowledge Discovery and Data Mining	stefan kramer + christoph helma + luc de raedt	2001	Molecular feature mining in HIV data	 + Decision support systems + Life and medical sciences + data mining + Health care information systems + Information systems applications + Data analytics + outliers + Mathematics and statistics + Physical sciences and engineering + Information systems + Applied computing + database system + Systems biology + Genetics + robust statistics + Computational biology	No keyword found	We present the application of Feature Mining techniques to the Developmental Therapeutics Program's AIDS antiviral screen database. The database consists of 43576 compounds, which were measured for their capability to protect human cells from HIV-1 infection. According to these measurements, the compounds were classified as either active, moderately active or inactive. The distribution of classes is extremely skewed: Only 1.3 % of the molecules is known to be active, and 2.7 % is known to be moderately active.Given this database, we were interested in molecular substructures (i.e., features) that are frequent in the active molecules, and infrequent in the inactives. In data mining terms, we focused on features with a minimum support in active compounds and a maximum support in inactive compounds. We analyzed the database using the levelwise version space algorithm that forms the basis of the inductive query and database system MOLFEA (Molecular Feature Miner). Within this framework, it is possible to declaratively specify the features of interest, such as the frequency of features on (possibly different) datasets as well as on the generality and syntax of them. Assuming that the detected substructures are causally related to biochemical mechanisms, it should be possible to facilitate the development of new pharmaceuticals with improved activities.
5B6968C5	Knowledge Discovery and Data Mining	farnoush banaei kashani + cyrus shahabi	2001	A Framework for Efficient and Anonymous Web Usage Mining Based on Client-Side Tracking	real time + web usage mining + web personalization + data collection + data mining		Web Usage Mining (WUM), a natural application of data mining techniques to the data collected from user interactions with the web, has greatly concerned both academia and industry in recent years. Through WUM, we are able to gain a better understanding of both the web and web user access patterns; a knowledge that is crucial for realization of full economic potential of the web. In this chapter, we describe a framework for WUM that particularly satisfies the challenging requirements of the web personalization applications. For on-line and anonymous web personalization to be effective, WUM must be accomplished in real-time as accurately as possible. On the other hand, the analysis tier of the WUM system should allow compromise between scalability and accuracy to be applicable to real-life web-sites with numerous visitors. Within our WUM framework, we introduce a distributed user tracking approach for accurate, efficient, and scalable collection of the usage data. We also propose a new model, the Feature Matrices (FM) model, to capture and analyze users access patterns. With FM, various features of the usage data can be captured with flexible precision so that we can trade off accuracy for scalability based on the specific application requirements. Moreover, due to low update complexity of the model, FM can adapt to user behavior changes in real-time. Finally, we define a novel similarity measure based on FM that is specifically designed for accurate classification of partial navigation patterns in real-time. Our extensive experiments with both synthetic and real data verify correctness and efficacy of our WUM framework for efficient web personalization.
7ADA365D	Knowledge Discovery and Data Mining	c c chen + meng chang chen + yeali sun	2001	PVA: a self-adaptive personal view agent system	 + personalization + Information storage systems + www + Human-centered computing + Information retrieval + Human computer interaction (HCI) + machine learning + Information systems	+Algorithms+Management+Design+Experimentation+Theory. Keywords WWW+Personalization+Personal view+Machine learning	In this paper, we present PVA, an adaptive personal view information agent system to track, learn and manage, user's interests in Internet documents. When user's interests change, PVA, in not only the contents, but also in the structure of user profile, is modified to adapt to the changes. Experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems.
801254AC	Knowledge Discovery and Data Mining	eamonn keogh + michael pazzani + selina chu	2001	Ensemble-index: a new approach to indexing large databases	 + discrete fourier transform + similarity search + data mining + indexation + Record storage alternatives + time series + discrete wavelet transform + Record storage systems + Information retrieval + fourier transform + Information systems + Document representation + dimensionality reduction + Information storage systems + singular value decomposition	eol>Time series + indexing and retrieval + dimensionality reduction + similarity search + data mining	The problem of similarity search (query-by-content) has attracted much research interest. It is a difficult problem because of the inherently high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier Transform (DFT), the Discrete Wavelet Transform (DWT) and Piecewise Polynomial Approximation. In this work, we introduce a novel framework for using ensembles of two or more representations for more efficient indexing. The basic idea is that instead of committing to a single representation for an entire dataset, different representations are chosen for indexing different parts of the database. The representations are chosen based upon a local view of the database. For example, sections of the data that can achieve a high fidelity representation with wavelets are indexed as wavelets, but highly spectral sections of the data are indexed using the Fourier transform. At query time, it is necessary to search several small heterogeneous indices, rather than one large homogeneous index. As we will theoretically and empirically demonstrate this results in much faster query response times.
77D83230	Knowledge Discovery and Data Mining	pedro domingos + geoff hulten + laurie spencer	2001	Mining time-changing data streams	 + data streams + Learning paradigms + Information systems applications + Computing methodologies + random sampling + very large database + machine learning + Data mining + Information systems + decision tree + stationary distribution + concept drift + Supervised learning + Classification and regression trees + decision trees + Machine learning + Machine learning approaches + time change + Supervised learning by classification	+streams+subsampling	Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
7EC4E6E2	Knowledge Discovery and Data Mining	ron kohavi	2001	Mining e-commerce data: the good, the bad, and the ugly	web server + electronic commerce + e commerce + data mining + design + application server	eol>E-commerce + data mining + application server + web server + web site architecture	"
Organizations conducting Electronic Commerce (e-commerce) can greatly benefit from the insight that data mining of transactional and clickstream data provides. Such insight helps not only to improve the electronic channel (e.g., a web site), but it is also a learning vehicle for the bigger organization conducting business at brick-and-mortar stores. The e-commerce site serves as an early alert system for emerging patterns and a laboratory for experimentation. For successful data mining, several ingredients are needed and e-commerce provides all the right ones (the Good). Web server logs, which are commonly used as the source of data for mining e-commerce data, were designed to debug web servers, and the data they provide is insufficient, requiring the use of heuristics to reconstruct events. Moreover, many events are never logged in web server logs, limiting the source of data for mining (the Bad). Many of the problems of dealing with web server log data can be resolved by properly architecting the ecommerce sites to generate data needed for mining. Even with a good architecture, however, there are challenging problems that remain hard to solve (the Ugly). Lessons and metrics based on mining real e-commerce data are presented.
"
75DA5F6F	Knowledge Discovery and Data Mining	charles elkan + bianca zadrozny	2001	Learning and making decisions when costs and probabilities are both unknown	 + collaborative filtering + Probabilistic algorithms + unbiased estimator + bayesian learning + Continuous optimization + data mining + Information systems applications + Linear programming + Probabilistic reasoning algorithms + sample selection bias + Data mining + cost estimation + Information systems + Theory of computation + decision tree + Mathematical analysis + clustering + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + Mathematical optimization + Design and analysis of algorithms + Markov-chain Monte Carlo methods	No keyword found	In many data mining domains, misclassification costs are different for different examples, in the same way that class membership probabilities are example-dependent. In these domains, both costs and probabilities are unknown for test examples, so both cost estimators and probability estimators must be learned. After discussing how to make optimal decisions given cost and probability estimates, we present decision tree and naive Bayesian learning methods for obtaining well-calibrated probability estimates. We then explain how to obtain unbiased estimators for example-dependent costs, taking into account the difficulty that in general, probabilities and costs are not independent random variables, and the training examples for which costs are known are not representative of all examples. The latter problem is called sample selection bias in econometrics. Our solution to it is based on Nobel prize-winning work due to the economist James Heckman. We show that the methods we propose perform better than MetaCost and all other known methods, in a comprehensive experimental comparison that uses the well-known, large, and challenging dataset from the KDD'98 data mining contest.
7A9D2056	Knowledge Discovery and Data Mining	changtien lu + shashi shekhar + pusheng zhang	2001	Detecting graph-based spatial outliers: algorithms and applications (a summary of results)	 + Visualization + distance metric + Information systems applications + Human-centered computing + Graph theory + outlier detection + Visualization application domains + Information systems + Geographic visualization + structured data + statistical test + Spatial-temporal systems + Discrete mathematics + Probability and statistics + Mathematics of computing + Graph algorithms + Distribution functions	+Spatial Data Mining+Spatial Graphs	Identification of outliers can lead to the discovery of unexpected, interesting, and useful knowledge. Existing methods are designed for detecting spatial outliers in multidimensional geometric data sets, where a distance metric is available. In this paper, we focus on detecting spatial outliers in graph structured data sets. We define statistical tests, analyze the statistical foundation underlying our approach, design several fast algorithms to detect spatial outliers, and provide a cost model for outlier detection procedures. In addition, we provide experimental results from the application of our algorithms on a Minneapolis-St.Paul(Twin Cities) traffic dataset to show their effectiveness and usefulness.
0A7FA22D	Knowledge Discovery and Data Mining	gary m weiss	2001	Predicting Telecommunication Equipment Failures from Sequences of Network Alarms	data mining + knowledge engineering + knowledge base + expert system + network performance		"
The computer and telecommunication industries rely heavily on knowledge-based expert systems to manage the performance of their networks. These expert systems are developed by knowledge engineers, who must first interview domain experts to extract the pertinent knowledge. This knowledge acquisition process is laborious and costly, and typically is better at capturing qualitative knowledge than quantitative knowledge. This is a liability, especially for domains like the telecommunication domain, where enormous amounts of data are readily available for analysis. Data mining holds tremendous promise for the development of expert systems for monitoring network performance since it provides a way of automatically identifying subtle, yet important, patterns in data. This case study describes a project in which a temporal data mining system called Timeweaver is used to identify faulty telecommunication equipment from logs of network alarm messages.
"
769EE40A	Knowledge Discovery and Data Mining	jochen garcke + michael griebel	2001	Data mining with sparse grids using simplicial basis functions	 + approximation + feature space + curse of dimensionality + Continuous optimization + data mining + Information systems applications + Linear programming + sparse grids + classification + Data mining + Information systems + Theory of computation + Mathematical analysis + Mathematics of computing + Mathematical optimization + Design and analysis of algorithms	data mining + classi cation + approximation + sparse grids + combination technique + simplicial discretization	Recently we presented a new approach [18] to the classification problem arising in data mining. It is based on the regularization network approach but, in contrast to other methods which employ ansatz functions associated to data points, we use a grid in the usually high-dimensional feature space for the minimization process. To cope with the curse of dimensionality, we employ sparse grids [49]. Thus, only O(hn-1nd-1) instead of O(hn-d) grid points and unknowns are involved. Here d denotes the dimension of the feature space and hn = 2-n gives the mesh size. We use the sparse grid combination technique [28] where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension. The sparse grid solution is then obtained by linear combination. In contrast to our former work, where d-linear functions were used, we now apply linear basis functions based on a simplicial discretization. This allows to handle more dimensions and the algorithm needs less operations per data point.We describe the sparse grid combination technique for the classification problem, give implementational details and discuss the complexity of the algorithm. It turns out that the method scales linearly with the number of given data points. Finally we report on the quality of the classifier built by our new method on data sets with up to 10 dimensions. It turns out that our new method achieves correctness rates which are competitive to that of the best existing methods.
7A069D1A	Knowledge Discovery and Data Mining	balaji padmanabhan + steven o kimbrough + zhiqiang zheng	2001	Personalization from incomplete data: what you don't know can hurt	 + data preprocessing + personalization + probabilistic + data gathering + Machine learning + Information systems applications + Computing methodologies + Data mining + data collection + clickstream data + Information systems	eol>Incomplete data + learning + personalization + clickstream data + data preprocessing + probabilistic clipping	Clickstream data collected at any web site (site-centric data) is inherently incomplete, since it does not capture users' browsing behavior across sites (user-centric data). Hence, models learned from such data may be subject to limitations, the nature of which has not been well studied. Understanding the limitations is particularly important since most current personalization techniques are based on site-centric data only. In this paper, we empirically examine the implications of learning from incomplete data in the context of two specific problems: (a) predicting if the remainder of any given session will result in a purchase and (b) predicting if a given user will make a purchase at any future session. For each of these problems we present new algorithms for fast and accurate data preprocessing of clickstream data. Based on a comprehensive experiment on user-level clickstream data gathered from 20,000 users' browsing behavior, we demonstrate that models built on user-centric data outperform models built on site-centric data for both prediction tasks.
7C1B5DF2	Knowledge Discovery and Data Mining	stuart russell + nikunj c oza	2001	Experimental comparisons of online and batch versions of bagging and boosting	 + online algorithm + Machine learning theory + Online learning algorithms + Learning settings + Computing methodologies + Online learning theory + ensemble learning + Theory and algorithms for application domains + Models of computation + Theory of computation + Online algorithms + Interactive computation + Machine learning + Design and analysis of algorithms + mcmc	No keyword found	Bagging and boosting are well-known ensemble learning methods. They combine multiple learned base models with the aim of improving generalization performance. To date, they have been used primarily in batch mode, i.e., they require multiple passes through the training data. In previous work, we presented online bagging and boosting algorithms that only require one pass through the training data and presented experimental results on some relatively small datasets. Through additional experiments on a variety of larger synthetic and real datasets, this paper demonstrates that our online versions perform comparably to their batch counterparts in terms of classification accuracy. We also demonstrate the substantial reduction in running time we obtain with our online algorithms because they require fewer passes through the training data.
7528C39D	Knowledge Discovery and Data Mining	dekang lin + patrick pantel	2001	DIRT @SBT@discovery of inference rules from text	artificial intelligent +  + natural language processing + boosting + information retrieval + Computing methodologies + Graph theory + Language resources + inference rule + Discrete mathematics + Natural language processing + Graph algorithms + Mathematics of computing + Artificial intelligence	No keyword found	"In this paper, we propose an unsupervised method for discovering inference rules from text, such as ""X is author of Y â X wrote Y"", ""X solved Y â X found a solution to Y"", and ""X caused Y â Y is triggered by X"". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
7F3E6934	Knowledge Discovery and Data Mining	pat langley + jungsoon yoo + annaka kalton + kiri l wagstaff	2001	Generalized clustering, supervised learning, and data assignment	 + supervised learning + Machine learning + Computing methodologies + clustering	No keyword found	Clustering algorithms have become increasingly important in handling and analyzing data. Considerable work has been done in devising effective but increasingly specific clustering algorithms. In contrast, we have developed a generalized framework that accommodates diverse clustering algorithms in a systematic way. This framework views clustering as a general process of iterative optimization that includes modules for supervised learning and instance assignment. The framework has also suggested several novel clustering methods. In this paper, we investigate experimentally the efficacy of these algorithms and test some hypotheses about the relation between such unsupervised techniques and the supervised methods embedded in them.
7B55F6B1	Knowledge Discovery and Data Mining	glenn fung + o l mangasarian	2001	Proximal support vector machine classifiers	Cluster analysis +  + support vector machines + feature space + Learning paradigms + Computing methodologies + Information systems applications + linear program + Data mining + Unsupervised learning + Information systems + support vector machine + Machine learning + linear equations	No keyword found	Instead of a standard support vector machine (SVM) that classifies points by assigning them to one of two disjoint half-spaces, points are classified by assigning them to the closest of two parallel planes (in input or feature space) that are pushed apart as far as possible. This formulation, which can also be interpreted as regularized least squares and considered in the much more general context of regularized networks [8, 9], leads to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equations. In contrast, standard SVMs solve a quadratic or a linear program that require considerably longer computational time. Computational results on publicly available datasets indicate that the proposed proximal SVM classifier has comparable test set correctness to that of standard SVM classifiers, but with considerably faster computational time that can be an order of magnitude faster. The linear proximal SVM can easily handle large datasets as indicated by the classification of a 2 million point 10-attribute set in 20.8 seconds. All computational results are based on 6 lines of MATLAB code.
7D982A2A	Knowledge Discovery and Data Mining	h h zhang + qiang yang + tianyi li	2001	Mining web logs for prediction models in WWW caching and prefetching	 + prediction model + Web services + web accessibility + Information storage systems + data mining + Information systems applications + Web applications + Information retrieval + World Wide Web + Data mining + Information systems	eol>Web Log Mining + Application to Caching and Prefetching on the WWW	Web caching and prefetching are well known strategies for improving the performance of Internet systems. When combined with web log mining, these strategies can decide to cache and prefetch web documents with higher accuracy. In this paper, we present an application of web log mining to obtain web-document access patterns and use these patterns to extend the well-known GDSF caching policies and prefetching policies. Using real web logs, we show that this application of data mining can achieve dramatic improvement to web-access performance.
7EA2DCCE	Knowledge Discovery and Data Mining	dekang lin + patrick pantel	2001	Induction of semantic classes from natural language text	 + boosting + Computing methodologies + Document capture + low frequency + Database management system engines + Information systems + Applied computing + Document management and text processing + Data management systems + Natural language processing + Artificial intelligence + natural language + Document analysis	No keyword found	Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus.
5DF80F2B	Knowledge Discovery and Data Mining	michael hahsler + maximillian jahn + andreas geyerschulz	2001	A Customer Purchase Incidence Model Applied to Recommender Services	data collection + customer behavior + information good + recommender system		In this contribution we transfer a customer purchase incidence model for consumer products which is based on Ehrenbergâs repeat-buying theory to Web-based information products. Ehrenbergâs repeat-buying theory successfully describes regularities on a large number of consumer product markets. We show that these regularities exist in electronic markets for information goods, too, and that purchase incidence models provide a well founded theoretical base for recommender and alert services. The article consists of two parts. In the first part Ehrenbergâs repeat-buying theory and its assumptions are reviewed and adapted for web-based information markets. Second, we present the empirical validation of the model based on data collected from the information market of the Virtual University of the Vienna University of Economics and Business Administration from September 1999 to May 2001.
781E0479	Knowledge Discovery and Data Mining	saharon rosset + uri eick + nurit vatnik + izhak idan + einat neumann	2001	Evaluation of prediction models for marketing campaigns	 + Operations research + prediction model + Simulation types and techniques + comparative modeling + Computing methodologies + Continuous simulation + Marketing + Enterprise computing + Applied computing + Modeling and simulation + confidence interval + confidence intervals + Model development and analysis + Model verification and validation	+Marketing Campaigns+Performance Measures+Confidence Intervals	We consider prediction-model evaluation in the context of marketing-campaign planning. In order to evaluate and compare models with specific campaign objectives in mind, we need to concentrate our attention on the appropriate evaluation-criteria. These should portray the model's ability to score accurately and to identify the relevant target population. In this paper we discuss some applicable model-evaluation and selection criteria, their relevance for campaign planning, their robustness under changing population distributions, and their employment when constructing confidence intervals. We illustrate our results with a case study based on our experience from several projects.
752FD8AD	Knowledge Discovery and Data Mining	chris h q ding + xiaofeng he + hongyuan zha	2001	A spectral method to separate disconnected and nearly-disconnected web graph components	 + connected component + Data mining + Information systems + spectral method + Theory of computation + Discrete mathematics + Mathematics of computing + graph partitioning + depth first search + objective function + perturbation analysis + Retrieval tasks and goals + Sorting and searching + Information systems applications + Clustering and classification + Information retrieval + Graph theory + Clustering + eigenvectors + Data structures design and analysis + log likelihood + clustering + Graph algorithms + Design and analysis of algorithms + laplacian matrix + breadth first search	No keyword found	Separation of connected components from a graph with disconnected graph components mostly use breadth-first search (BFS) or depth-first search (DFS) graph algorithms. Here we propose a new algebraic method to separate disconnected and nearly-disconnected components. This method is based on spectral graph partitioning, following a key observation that disconnected components will show up, after properly sorted, as step-function like curve in the lowest eigenvectors of the Laplacian matrix of the graph. Following an perturbative analysis framework, we systematically analyzed the graph structures, first on the disconnected subgraph case, and second on the effects of adding edges sparsely connecting different subgraphs as a perturbation. Several new results are derived, providing insights to spectral methods and related clustering objective function. Examples are given illustrating the concepts and results our methods. Comparing to the standard graph algorithms, this method has the same O(âE â + âVâlog(âVâ)) complexity, but is easier to implement (using readily available eigensolvers). Further more the method can easily identify articulation points and bridges on nearly-disconnected graphs. Segmentation of a real example of Web graph for query amazon is given. We found that each disconnected or nearly-disconnected components forms a cluster on a clear topic.
76ADCC6E	Knowledge Discovery and Data Mining	petteri sevon + vesa ollikainen + hannu toivonen	2001	TreeDT: gene mapping by tree disequilibrium test	 + algorithms + Visualization + Life and medical sciences + Information systems applications + Human-centered computing + Data mining + Visualization application domains + data consistency + Information systems + Applied computing + gene mapping + permutation test + Probability and statistics + Mathematics of computing + genetic marker + Scientific visualization	eol>Gene mapping + algorithms + permutation tests + prefix trees	We introduce and evaluate TreeDT, a novel gene mapping method which is based on discovering and assessing tree-like patterns in genetic marker data. Gene mapping aims at discovering a statistical connection from a particular disease or trait to a narrow region in the genome. In a typical case-control setting, data consists of genetic markers typed for a set of disease-associated chromosomes and a set of control chromosomes. A computer scientist would view this data as a set of strings.TreeDT extracts, essentially in the form of substrings and prefix trees, information about the historical recombinations in the population. This information is used to locate fragments potentially inherited from a common diseased founder, and to map the disease gene into the most likely such fragment. The method measures for each chromosomal location the disequilibrium of the prefix tree of marker strings starting from the location, to assess the distribution of disease-associated chromosomes.We evaluate experimentally the performance of TreeDT on realistic, simulated data sets, and comparisons to state of the art methods (TDT, HPM) show that TreeDT is very competitive.
5D12F159	Knowledge Discovery and Data Mining	mong li lee + wynne hsu + ji zhang	2001	Image mining: issues, frameworks and techniques	image classification + artificial intelligent + association rule mining + data mining + machine learning + image retrieval + object recognition + image processing + computer vision	Image mining + image indexing and retrieval + object recognition + image classification + image clustering + association rule mining	"
Advances in image acquisition and storage technology have led to tremendous growth in significantly large and detailed image databases. These images, if analyzed, can reveal useful information to the human users. Image mining deals with the extraction of implicit knowledge, image data relationship, or other patterns not explicitly stored in the images. Image mining is more than just an extension of data mining to image domain. It is an interdisciplinary endeavor that draws upon expertise in computer vision, image processing, image retrieval, data mining, machine learning, database, and artificial intelligence. Despite the development of many applications and algorithms in the individual research fields cited above, research in image mining is still in its infancy. In this paper, we will examine the research issues in image mining, current developments in image mining, particularly, image mining frameworks, state-of-the-art techniques and systems. We will also identify some future research directions for image mining at the end of this paper.
"
5FA7FA1A	Knowledge Discovery and Data Mining	simeon j simoff + robert p biukaghai	2001	Multimedia mining of collaborative virtual workspaces: an integrative framework for extracting and integrating collaborative process knowledge	knowledge extraction + virtual environment	Multimedia data mining + knowledge collaborative virtual environments	"
Collaborative virtual environments are becoming an intrinsic part of professional practices. In addition to providing collaboration support, they have the potential to collect vast amounts of multimedia data about the actions and content of such collaborative activities. The aim of this research is to utilize this data effectively, extract meaningful insights out of it and feed discovered knowledge back into the environment. The paper presents a framework for integrating multimedia data mining techniques with collaborative virtual environments, starting from early conceptual development. Discovered patterns are deposited in an organisational memory, which makes these available within the virtual environment. Some of the ideas are illustrated by an example from the application to collaborative spaces developed in LiveNet, a virtual workspace design system.
"
7C5F4D95	Knowledge Discovery and Data Mining	finn arup nielsen + c lee giles + steve lawrence + david m pennock	2001	Extracting collective probabilistic forecasts from web games	 + Personal computers and PC applications + data mining + Web applications + world wide web + World Wide Web + knowledge discovery + Data mining + Contextual software domains + Information systems + science and technology + Software organization and properties + Software and its engineering + Economics + Massively multiplayer online games + Information systems applications + Virtual worlds software + Computers in other domains + Law, social and behavioral sciences + Computer games + Applied computing + Interactive games + Web services + stock exchange + Multimedia information systems	+H.2.8 [Database Management+Database Applications	"Game sites on the World Wide Web draw people from around the world with specialized interests, skills, and knowledge. Data from the games often reflects the players' expertise and will to win. We extract probabilistic forecasts from data obtained from three online games: the Hollywood Stock Exchange (HSX), the Foresight Exchange (FX), and the Formula One Pick Six (F1P6) competition. We find that all three yield accurate forecasts of uncertain future events. In particular, prices of so-called ""movie stocks"" on HSX are good indicators of actual box office returns. Prices of HSX securities in Oscar, Emmy, and Grammy awards correlate well with observed frequencies of winning. FX prices are reliable indicators of future developments in science and technology. Collective predictions from players in the F1 competition serve as good forecasts of true race outcomes. In some cases, forecasts induced from game data are more reliable than expert opinions. We argue that web games naturally attract well-informed and well-motivated players, and thus offer a valuable and oft-overlooked source of high-quality data with significant predictive value."
775721FD	Knowledge Discovery and Data Mining	richard adderley + peter musgrove	2001	Data mining case study: modeling the behavior of offenders who commit serious sexual assaults	self organizing map +  + pattern analysis + data mining + Information systems applications + knowledge discovery + Data mining + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- data mining General Terms Experimentation+Verification Keywords Knowledge Discovery+Data Mining+Crime Pattern Analysis+Offender Behavior+Self Organizing Map	This paper looks at the use of a Self Organizing Map (SOM), to link of records of crimes of serious sexual attacks. Once linked a profile can be derived of the offender(s) responsible.The data was drawn from the major crimes database at the National Crime Faculty of the National Police Staff College Bramshill UK. The data was encoded from text by a small team of specialists working to a well-defined protocol. The encoded data was analyzed using SOMs. Two exercises were conducted. These resulted in the linking of several offences in to clusters each of which were sufficiently similar to have possibly been committed by the same offender(s). A number of clusters were used to form profiles of offenders. Some of these profiles were confirmed by independent analysts as either belonging to known offenders or appeared sufficiently interesting to warrant further investigation.The prototype was developed over 10 weeks. This contrasts with an in-house study using a conventional approach, which took 2 years to reach similar results. As a consequence of this study the NCF intends to pursue an in-depth follow up study.
772E722D	Knowledge Discovery and Data Mining	jose c pinheiro + diane lambert	2001	Mining a stream of transactions for customer patterns	 + histograms + transaction data + data mining + Information systems applications + empirical study + Probability and statistics + Mathematics of computing + Data mining + customer behavior + real time + Information systems	+tion from transaction data is challenging+though. The data	Transaction data can arrive at a ferocious rate in the order that transactions are completed. The data contain an enormous amount of information about customers, not just transactions, but extracting up-to-date customer information from an ever changing stream of data and mining it in real-time is a challenge. This paper describes a statistically principled approach to designing short, accurate summaries or signatures of high dimensional customer behavior that can be kept current with a stream of transactions. A signature database can then be used for data mining and to provide approximate answers to many kinds of queries about current customers quickly and accurately, as an empirical study of the calling patterns of 96,000 wireless customers who made about 18 million wireless calls over a three month period shows.
79B8535A	Knowledge Discovery and Data Mining	pedro domingos + matthew richardson	2001	Mining the network value of customers	profitability +  + collaborative filtering + social networks + data mining + direct marketing + Information systems applications + Computing methodologies + Law, social and behavioral sciences + Data mining + Information systems + social network + Logical and relational learning + Applied computing + Inductive logic learning + viral marketing + Machine learning + Machine learning approaches	+General Terms	One of the major applications of data mining is in helping companies determine which potential customers to market to. If the expected profit from a customer is greater than the cost of marketing to her, the marketing action for that customer is executed. So far, work in this area has considered only the intrinsic value of the customer (i.e, the expected profit from sales to her). We propose to model also the customer's network value: the expected profit from sales to other customers she may influence to buy, the customers those may influence, and so on recursively. Instead of viewing a market as a set of independent entities, we view it as a social network and model it as a Markov random field. We show the advantages of this approach using a social network mined from a collaborative filtering database. Marketing that exploits the network value of customers---also known as viral marketing---can be extremely effective, but is still a black art. Our work can be viewed as a step towards providing a more solid foundation for it, taking advantage of the availability of large relevant databases.
7F973955	Knowledge Discovery and Data Mining	marialuiza antonie + osmar r zaiane + alexandru coman	2001	Application of Data Mining Techniques for Medical Image Classification	 + data mining + breast cancer + neural network + association rule mining	eol>classification + medical imaging + association rule mining + neural networks + image categorization + image mining	"
Breast cancer represents the second leading cause of cancer deaths in women today and it is the most common type of cancer in women. This paper presents some experiments for tumour detection in digital mammography. We investigate the use of different data mining techniques, neural networks and association rule mining, for anomaly detection and classification. The results show that the two approaches performed well, obtaining a classification accuracy reaching over 70% percent for both techniques. Moreover, the experiments we conducted demonstrate the use and effectiveness of association rule mining in image categorization.
"
0587C1AC	Knowledge Discovery and Data Mining	valery a petrushin + ishwar k sethi + victor kulesh	2001	The PERSEUS Project: Creating Personalized Multimedia News Portal	audio analysis + information retrieval	intelligent information systems + video analysis + audio analysis + multimedia news	"
This paper describes the Perseus project, which is devoted to developing techniques and tools for creating personalized multimedia news portals. The purpose of a personalized multimedia news portal is to provide relevant information, selected from newswire sites on the Internet and augmented by video clips automatically extracted from TV broadcasts, based on the user's preferences. To create such an intelligent information system several techniques related to textual information retrieval, audio and video segmentation, and topic detection should be developed to work in accord. The approaches to event mining and tracking on the Internet, commercial detection and recognition in video and audio streams, and selection of relevant news video fragments, based on closed captioning and audio transcripts, are described.
"
7C3B3868	Knowledge Discovery and Data Mining	charles elkan	2001	Magical thinking in data mining: lessons from CoIL challenge 2000	 + statistical significance + nearest neighbor + supervised learning + data mining		"
CoIL challenge 2000 was a supervised learning contest that attracted 43 entries. The authors of 29 entries later wrote explanations of their work. This paper discusses these reports and reaches three main conclusions. First, naive Bayesian classifiers remain competitive in practice: they were used by both the winning entry and the next best entry. Second, identifying feature interactions correctly is important for maximizing predictive accuracy: this was the difference between the winning classifier and all others. Third and most important, too many researchers and practitioners in data mining do not appreciate properly the issue of statistical significance and the danger of overfitting. Given a dataset such as the one for the CoIL contest, it is pointless to apply a very complicated learning algorithm, or to perform a very time-consuming model search. In either case, one is likely to overfit the training data and to fool oneself in estimating predictive accuracy and in discovering useful correlations.
"
7A99B6DE	Knowledge Discovery and Data Mining	weiyin loh + johannes gehrke	2001	Advances in decision tree construction	decision tree		"
Tutorial Overview
"
7C1DB7C7	Knowledge Discovery and Data Mining	bing liu + wynne hsu + yiming ma	2001	Discovering the set of fundamental rule changes	 + Triggers and rules + association rule + Enterprise computing + Applied computing + data mining + Information systems applications + Data management systems + Business process management + Data mining + Information systems + Database management system engines	No keyword found	The world around us changes constantly. Knowing what has changed is an important part of our lives. For businesses, recognizing changes is also crucial. It allows businesses to adapt themselves to the changing market needs. In this paper, we study changes of association rules from one time period to another. One approach is to compare the supports and/or confidences of each rule in the two time periods and report the differences. This technique, however, is too simplistic as it tends to report a huge number of rule changes, and many of them are, in fact, simply the snowball effect of a small subset of fundamental changes. Here, we present a technique to highlight the small subset of fundamental changes. A change is fundamental if it cannot be explained by some other changes. The proposed technique has been applied to a number of real-life datasets. Experiments results show that the number of rules whose changes are unexplainable is quite small (about 20% of the total number of changes discovered), and many of these unexplainable changes reflect some fundamental shifts in the application domain.
7C8B3421	Knowledge Discovery and Data Mining	ella bingham + heikki mannila	2001	Random projection in dimensionality reduction: applications to image and text data	random matrix +  + Visualization + power method + information retrieval + Information systems applications + Human-centered computing + Data mining + Visualization application domains + Information systems + Geographic visualization + dimensionality reduction + Numerical analysis + high dimensional data + Spatial-temporal systems + Mathematical analysis + Computations on matrices + Mathematics of computing + principal component analysis	No keyword found	Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.
5CFFA0EA	Knowledge Discovery and Data Mining	george karypis + valerie guralnik	2001	A Scalable Algorithm for Clustering Protein Sequences	protein sequence		"

"
787BCB05	Knowledge Discovery and Data Mining	s a steed + bertis b little + ashley c lovell + w l johnston + roderick m rejesus	2002	Collusion in the U.S. crop insurance program: applied data mining	 + log linear model + data mining + Machine learning + Computing methodologies + log linear models + criminal justice	+Management+Measurement	"This paper quantitatively analyzes indicators of Agent (policy seller), Adjuster (indemnity claim adjuster), Producer (policy purchaser/holder) indemnity behavior suggestive of collusion in the United States Department of Agriculture (USDA) Risk Management Agency (RMA) national crop insurance program. According to guidance from the federal law and using six indicator variables of indemnity behavior, those entities equal to or exceeding 150% of the county mean (computed using a simple jackknife procedure) on all entity-relevant indicators were flagged as ""anomalous."" Log linear analysis was used to test (I) hierarchical node-node arrangements and (2) a non-recursive model of node information sharing. Chi-square distributed deviance statistic identified the optimal log linear model. The results of the applied data mining technique used here suggest that the non-recursive triplet and Agent-producer doublet collusion probabilistically accounts for the greatest proportion of waste, fraud, and abuse in the federal crop insurance program. Triplet and Agent-producer doublets need detailed investigation for possible collusion. Hence, this data mining technique provided a high level of confidence when 24 million records were quantitatively analyzed for possible fraud, waste, or other abuse of the crop insurance program administered by the USDA RMA, and suspect entities reported to USDA. This data mining technique can be applied where vast amounts of data are available to detect patterns of collusion or conspiracy as may be of interest to the criminal justice or intelligence agencies."
79EFA075	Knowledge Discovery and Data Mining	darya chudova + padhraic smyth	2002	Pattern discovery in sequences under a Markov assumption	computational biology +  + web mining + data mining + Symbolic and algebraic manipulation + Probabilistic reasoning algorithms + Data mining + Information systems + Theory of computation + dna sequence + personalization + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + Markov-chain Monte Carlo methods + Probabilistic algorithms + Computing methodologies + Information systems applications + lower bound + error rate + Machine learning + Symbolic and algebraic algorithms + markov models + shrinkage + Design and analysis of algorithms	No keyword found	"In this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences. An important real-world problem of this nature is motif discovery in DNA sequences. We investigate the fundamental aspects of this data mining problem that can make discovery ""easy"" or ""hard."" We present a general framework for characterizing learning in this context by deriving the Bayes error rate for this problem under a Markov assumption. The Bayes error framework demonstrates why certain patterns are much harder to discover than others. It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery. We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms, providing a lower bound on achievable performance. We discuss a number of fundamental issues that characterize sequential pattern discovery in this context, present a variety of empirical results to complement and verify the theoretical analysis, and apply our methodology to real-world motif-discovery problems in computational biology."
7A54188D	Knowledge Discovery and Data Mining	charu c aggarwal	2002	On effective classification of strings with wavelets	 + Rule learning + biological data + web mining + Learning paradigms + Computing methodologies + Knowledge representation and reasoning + Theory of computation + personalization + Supervised learning + Classification and regression trees + Machine learning + Data structures design and analysis + Machine learning approaches + markov models + shrinkage + Supervised learning by classification + Design and analysis of algorithms + Artificial intelligence + Pattern matching	No keyword found	In recent years, the technological advances in mapping genes have made it increasingly easy to store and use a wide variety of biological data. Such data are usually in the form of very long strings for which it is difficult to determine the most relevant features for a classification task. For example, a typical DNA string may be millions of characters long, and there may be thousands of such strings in a database. In many cases, the classification behavior of the data may be hidden in the compositional behavior of certain segments of the string which cannot be easily determined apriori. Another problem which complicates the classification task is that in some cases the classification behavior is reflected in global behavior of the string, whereas in others it is reflected in local patterns. Given the enormous variation in the behavior of the strings over different data sets, it is useful to develop an approach which is sensitive to both the global and local behavior of the strings for the purpose of classification. For this purpose, we will exploit the multi-resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristics at different levels of granularity. The resulting scheme turns out to be very effective in practice on a wide range of problems.
70BE39B7	Knowledge Discovery and Data Mining	andrew foss + weinan wang + chihoon lee + osmar r zaiane	2002	On Data Clustering Analysis: Scalability, Constraints, and Validation	data clustering + grouped data		Clustering is the problem of grouping data based on similarity. While this problem has attracted the attention of many researchers for many years, we are witnessing a resurgence of interest in new clustering techniques. In this paper we discuss some very recent clustering approaches and recount our experience with some of these algorithms. We also present the problem of clustering in the presence of constraints and discuss the issue of clustering validation.
79EF83C6	Knowledge Discovery and Data Mining	anna olecka	2002	Evaluating classifiers' performance in a constrained environment	 + Probabilistic computation + optimization problem + Statistical graphics + Continuous optimization + linear program + Linear programming + convex hull + Theory of computation + Statistical paradigms + Models of computation + Mathematical analysis + operations research + Probability and statistics + Mathematics of computing + Mathematical optimization + Design and analysis of algorithms + objective function	No keyword found	In this paper, we focus on methodology of finding a classifier with a minimal cost in presence of additional performance constraints. ROCCH analysis, where accuracy and cost are intertwined in the solution space, was a revolutionary tool for two-class problems. We propose an alternative formulation, as an optimization problem, commonly used in Operations Research. This approach extends the ROCCH analysis to allow for locating optimal solutions while outside constraints are present. Similarly to the ROCCH analysis, we combine cost and class distribution while defining the objective function. Rather than focusing on slopes of the edges in the convex hull of the solution space, however, we treat cost as an objective function to be minimized over the solution space, by selecting the best performing classifier(s) (one or more vertex in the solution space). The Linear Programming framework provides a theoretical and computational methodology for finding the vertex (classifier) which minimizes the objective function.
7567C17B	Knowledge Discovery and Data Mining	patrick glenisson + peter antal + geert fannes	2002	On the potential of domain literature for clustering and Bayesian network learning	 + Cluster analysis + bayesian network + data mining + Learning paradigms + Learning settings + Computing methodologies + knowledge discovery + statistical model + Unsupervised learning + Machine learning + text mining + clustering + bayesian networks	+data mining+clustering+Bayesian networks *These authors contributed equally to this work	Thanks to its increasing availability, electronic literature can now be a major source of information when developing complex statistical models where data is scarce or contains much noise. This raises the question of how to integrate information from domain literature with statistical data. Because quantifying similarities or dependencies between variables is a basic building block in knowledge discovery, we consider here the following question. Which vector representations of text and which statistical scores of similarity or dependency support best the use of literature in statistical models? For the text source, we assume to have annotations for the domain variables as short free-text descriptions and optionally to have a large literature repository from which we can further expand the annotations. For evaluation, we contrast the variables similarities or dependencies obtained from text using different annotation sources and vector representations with those obtained from measurement data or expert assessments. Specifically, we consider two learning problems: clustering and Bayesian network learning. Firstly, we report performance (against an expert reference) for clustering yeast genes from textual annotations. Secondly, we assess the agreement between text-based and data-based scores of variable dependencies when learning Bayesian network substructures for the task of modeling the joint distribution of clinical measurements of ovarian tumors.
7628FB59	Knowledge Discovery and Data Mining	mingsyan chen + chengru lin	2002	A robust and efficient clustering algorithm based on cohesion self-merging	Cluster analysis +  + association rule + indexing terms + data mining + Learning paradigms + data clustering + Symbolic and algebraic manipulation + Data mining + Information systems + Theory of computation + Retrieval tasks and goals + classification rule + Computing methodologies + Information systems applications + Clustering and classification + Information retrieval + Clustering + Unsupervised learning + Machine learning + Symbolic and algebraic algorithms + hierarchical clustering + Design and analysis of algorithms + index terms + linear time	No keyword found	Data clustering has attracted a lot of research attention in the field of computational statistics and data mining. In most related studies, the dissimilarity between two clusters is defined as the distance between their centroids, or the distance between two closest (or farthest) data points. However, all of these measurements are vulnerable to outliers, and removing the outliers precisely is yet another difficult task. In view of this, we propose a new similarity measurement referred to as cohesion, to measure the inter-cluster distances. By using this new measurement of cohesion, we design a two-phase clustering algorithm, called cohesion-based self-merging (abbreviated as CSM), which runs in linear time to the size of input data set. Combining the features of partitional and hierarchical clustering methods, algorithm CSM partitions the input data set into several small subclusters in the first phase, and then continuously merges the subclusters based on cohesion in a hierarchical manner in the second phase. As shown by our performance studies, the cohesion-based clustering is very robust and possesses the excellent tolerance to outliers in various workloads. More importantly, algorithm CSM is shown to be able to cluster the data sets of arbitrary shapes very efficiently, and provide better clustering results than those by prior methods.Index Terms: Data mining, data clustering, hierarchical clustering, partitional clustering
588A97E6	Knowledge Discovery and Data Mining	juzhen dong + ning zhong	2002	Mining Interesting Rules in Meningitis Data by Cooperatively Using GDT-RS and RSBR	rough sets + data mining + hybrid system + rough set		This paper describes an application of two rough sets based systems, namely GDT-RS and RSBR respectively, for mining if-then rules in a meningitis dataset. GDT-RS (Generalized Distribution Table and Rough Set) is a soft hybrid induction system, and RSBR (Rough Sets with Boolean Reasoning) is used for discretization of real valued attributes as a preprocessing step realized before the GDT-RS starts. We argue that discretization of continuous valued attributes is an important pre-processing step in the rule discovery process. We illustrate the quality of rules discovered by GDT-RS is strongly affected by the result of discretization.
79748520	Knowledge Discovery and Data Mining	deepak agarwal	2002	Shrinkage estimator generalizations of Proximal Support Vector Machines	 + kernel + ridge regression + bayesian model + classification + support vector machine + correlation + regression + Probability and statistics + kernel regression + Mathematics of computing + gaussian process	+classification+correlation+kernel+bias-variance tradeoff+regression	We give a statistical interpretation of Proximal Support Vector Machines (PSVM) proposed at KDD2001 as linear approximaters to (nonlinear) Support Vector Machines (SVM). We prove that PSVM using a linear kernel is identical to ridge regression, a biased-regression method known in the statistical community for more than thirty years. Techniques from the statistical literature to estimate the tuning constant that appears in the SVM and PSVM framework are discussed. Better shrinkage strategies that incorporate more than one tuning constant are suggested. For nonlinear kernels, the minimization problem posed in the PSVM framework is equivalent to finding the posterior mode of a Bayesian model defined through a Gaussian process on the predictor space. Apart from providing new insights, these interpretations help us attach an estimate of uncertainty to our predictions and enable us to build richer classes of models. In particular, we propose a new algorithm called PSVMMIX which is a combination of ridge regression and a Gaussian process model. Extension to the case of continuous response is straightforward and illustrated with example datasets.
5B9D0624	Knowledge Discovery and Data Mining	birgit hay + geert wets + koen vanhoof	2002	Web Usage Mining by Means of Multidimensional Sequence Alignment Methods	sequence alignment + web usage mining		In this article, a new algorithm called Multidimensional Sequence Alignment Method (MDSAM) is illustrated for mining navigation patterns on a web site. MDSAM examines sequences composed of several information types, such as visited pages and visiting time spent on pages. Besides, MDSAM handles large databases and uses heuristics to compute a multidimensional cost based on one-dimensional optimal trajectories. Empirical results show that MDSAM identifies profiles showing visited pages, visiting time spent on pages and the order in which pages are visited on a web site.
7C07DAE2	Knowledge Discovery and Data Mining	bin chen + christina yip chung	2002	CVS: a Correlation-Verification based Smoothing technique on information retrieval and term clustering	 + statistical significance + information retrieval + empirical study + Information retrieval + Information systems + enterprise system + Document representation + Interpolation + Numerical analysis + Mathematical analysis + text mining + smoothing + Mathematics of computing + query expansion + information retrieval system	+smoothing+query expansion+term clustering	As information volume in enterprise systems and in the Web grows rapidly, how to accurately retrieve information is an important research area. Several corpus based smoothing techniques have been proposed to address the data sparsity and synonym problems faced by information retrieval systems. Such smoothing techniques are often unable to discover and utilize the correlations among terms.We propose CVS, a Correlation-Verification based Smoothing method, that considers co-occurrence information in smoothing. Strongly correlated terms in a document are identified by their co-occurrence frequencies in the document. To avoid missing correlated terms with low co-occurrence frequencies but specific to the theme of the document, the joint distributions of terms in the document are compared with those in the corpus for statistical significance.A common approach to apply corpus based smoothing techniques to information retrieval is by refining the vector representations of documents. This paper investigates the effects of corpus based smoothing on information retrieval by query expansion using term clusters generated from a term clustering process. The results can also be viewed in light of the effects of smoothing on clustering.Empirical studies show that our approach outperforms previous corpus based smoothing techniques. It improves retrieval effectiveness by 14.6%. The results demonstrate that corpus based smoothing can be used for query expansion by term clustering.
6BAABC43	Knowledge Discovery and Data Mining	andre bergholz	2002	Coping with Sparsity in a Recommender System	recommender system + system performance		"In this paper we report experiments that we conducted using an implementation of a recommender system called âKnowledge Pump"" (KP) developed at Xerox. We repeat well-known methods such as the Pearson method, but also address common problems of recommender systems, in particular the sparsity problem. The sparsity problem is the problem of having too few ratings and hence too few correlations between users. We address this problem in two different manners. First, we introduce âtransitive correlations"", a mechanism to increase the number of correlations between existing users. Second, we add âagents"", artificial users that rate in accordance with some predefined preferences. We show that both ideas pay off, albeit in different ways: Transitive correlations provide a small help for virtually no price, whereas rating agents improve the coverage of the system significantly but also have a negative impact on the system performance."
5EED47C1	Knowledge Discovery and Data Mining	eepeng lim + kokleong ong + wee keong ng	2002	Mining Relationship Graphs for Effective Business Objectives	information system + artificial intelligent + human computer interface		Modern organization has two types of customer profiles: active and passive. Active customers contribute to the business goals of an organization, while passive customers are potential candidates that can be converted to active ones. Existing KDD techniques focused mainly on past data generated by active customers. The insights discovered apply well to active ones but may scale poorly with passive customers. This is because there is no attempt to generate know-how to convert passive customers into active ones. We propose an algorithm to discover relationship graphs using both types of profile. Using relationship graphs, an organization can be more effective in realizing its goals.
5C21E4E8	Knowledge Discovery and Data Mining	keyun hu + chunyi shi + yuchang lu + lili diao	2002	A Method to Boost Support Vector Machines	active learning + support vector machine + importance sampling + support vector		Combining boosting and Support Vector Machine (SVM) is proved to be beneficial, but it is too complex to be feasible. This paper introduces an efficient way to boost SVM. It embraces the idea of active learning to dynamically select âimportantâ samples into training sample set for constructing base classifiers. This method maintains a small training sample set with settled size in order to control the complexity of each base classifier. Other than construct each base SVM classifier directly, it uses the training samples only for finding support vectors. This way to combine boosting and SVM is proved to be accurate and efficient by experimental results.
772D2A1B	Knowledge Discovery and Data Mining	klaus julisch + marc dacier	2002	Mining intrusion detection alarms for actionable knowledge	 + Cluster analysis + conceptual clustering + data mining + Learning paradigms + Information systems applications + Computing methodologies + intrusion detection + Data mining + Unsupervised learning + Information systems + Theory and algorithms for application domains + Theory of computation + Database and storage security + intrusion detection system + Security and privacy + Machine learning + Theory of database privacy and security + Database theory	+cludes law suits+firewall reconfigurations+and the fixing of discovered vulnerabilities	In response to attacks against enterprise networks, administrators increasingly deploy intrusion detection systems. These systems monitor hosts, networks, and other resources for signs of security violations. The use of intrusion detection has given rise to another difficult problem, namely the handling of a generally large number of alarms. In this paper, we mine historical alarms to learn how future alarms can be handled more efficiently. First, we investigate episode rules with respect to their suitability in this approach. We report the difficulties encountered and the unexpected insights gained. In addition, we introduce a new conceptual clustering technique, and use it in extensive experiments with real-world data to show that intrusion detection alarms can be handled efficiently by using previously mined knowledge.
8150046B	Knowledge Discovery and Data Mining	helge ritter + jorg a walter	2002	On interactive visualization of high-dimensional data using the hyperbolic plane	 + 2 dimensional + hyperbolic plane + tree structure + hyperbolic space + Data mining + infoviz + Information systems + Modeling and simulation + high dimensional data + Visual analytics + Graphics input devices + Computer graphics + text mining + interactive visualization + distance function + Simulation types and techniques + Computing methodologies + Human-centered computing + Information systems applications + multi dimensional scaling + exponential growth + Human computer interaction (HCI) + Graphics systems and interfaces + Interaction devices + human computer interaction + Interaction paradigms	+browsing+text	"We propose a novel projection based visualization method for high-dimensional datasets by combining concepts from MDS and the geometry of the hyperbolic spaces. Our approach Hyperbolic Multi-Dimensional Scaling (H-MDS) extends earlier work [7] using hyperbolic spaces for visualization of tree structures data ( ""hyperbolic tree browser"" ).By borrowing concepts from multi-dimensional scaling we map proximity data directly into the 2-dimensional hyperbolic space (H2). This removes the restriction to ""quasihierarchical"", graph-based data -- limiting previous work. Since a suitable distance function can convert all kinds of data to proximity (or distance-based) data this type of data can be considered the most general.We used the circular PoincarÃ© model of the H2 which allows effective human-computer interaction: by moving the ""focus"" via mouse the user can navigate in the data without loosing the ""context"". In H2 the ""fish-eye"" behavior originates not simply by a non-linear view transformation but rather by extraordinary, non-Euclidean properties of the H2. Especially, the exponential growth of length and area of the underlying space makes the H2 a prime target for mapping hierarchical and (now also) high-dimensional data.We present several high-dimensional mapping examples including synthetic and real world data and a successful application for unstructured text. By analyzing and integrating multiple film critiques from news:rec.art.movies.reviews and the internet movie database, each movie becomes placed within the H2. Here the idea is, that related films share more words in their reviews than unrelated. Their semantic proximity leads to a closer arrangement. The result is a kind of high-level content structured display allowing the user to explore the ""space of movies""."
04A0CAE1	Knowledge Discovery and Data Mining	ghaleb m abdulla + terence critchlow + tina eliassirad	2002	Tina Eliassi-Rad, Terence Critchlow, Ghaleb Abdulla	 + Data compression + Data structures + Information retrieval + Data layout + Information systems + Database management system engines + Database query processing + Theory and algorithms for application domains + Theory of computation + Document representation + Search engine architectures and scalability + Database query processing and optimization (theory) + Search engine indexing + Data management systems + Database theory	No keyword found	"With the advent of fast computer systems, scientists are now able to generate terabytes of simulation data. Unfortunately, the sheer size of these data sets has made efficient exploration of them impossible. To aid scientists in gleaning insight from their simulation data, we have developed an ad-hoc query infrastructure. Our system, called AQSim (short for Ad-hoc Queries for Simulation) reduces the data storage requirements and query access times in two stages. First, it creates and stores mathematical and statistical models of the data at multiple resolutions. Second, it evaluates queries on the models of the data instead of on the entire data set. In this paper, we present two simple but effective statistical modeling techniques for simulation data. Our first modeling technique computes the ""true"" (unbiased) mean of systematic partitions of the data. It makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model. Our second statistical modeling technique uses the Andersen-Darling goodness-of-fit method on systematic partitions of the data. This method evaluates a model by how well it passes the normality test on the data. Both of our statistical models effectively answer range queries. At each resolution of the data, we compute the precision of our answer to the user's query by scaling the one-sided Chebyshev Inequalities with the original mesh's topology. We combine precisions at different resolutions by calculating their weighted average. Our experimental evaluations on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on multiple resolutions of large simulation data sets."
78C6ED89	Knowledge Discovery and Data Mining	jon kleinberg	2002	Bursty and hierarchical structure in streams	 + Email + state transition + Web applications + World Wide Web + Probabilistic reasoning algorithms + Information systems + Complexity classes + Theory of computation + viral marketing + computer science + Internet communications tools + text mining + Sequential Monte Carlo methods + Probability and statistics + Digital libraries and archives + Mathematics of computing + Markov-chain Monte Carlo methods + Probabilistic algorithms + queueing theory + technical report + social networks + direct marketing + Information systems applications + Computers in other domains + linear models + Applied computing + Complexity theory and logic + Computational complexity and cryptography	No keyword found	"A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise --- that the appearance of a topic in a document stream is signaled by a ""burst of activity,"" with certain features rising sharply in frequency as the topic emerges.The goal of the present work is to develop a formal approach for modeling such ""bursts,"" in such a way that they can be robustly and efficiently identified, and can provide an organizational framework for analyzing the underlying content. The approach is based on modeling the stream using an infinite-state automaton, in which bursts appear naturally as state transitions; in some ways, it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic. The resulting algorithms are highly efficient, and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream. Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them."
7D449E7B	Knowledge Discovery and Data Mining	mukund deshpande + george karypis + michihiro kuramochi	2002	Automated Approaches for Classifying Structures	classification + automation + chemical structure + data bases + graphs + algorithms + structures + domain knowledge		"

"
58DB107A	Knowledge Discovery and Data Mining	michael hahsler + andreas geyerschulz	2002	Comparing Two Recommender Algorithms with the Help of Recommendations by Peers	web usage mining + market research + recommender system + association rule		Since more and more Web sites, especially sites of retailers, offer automatic recommendation services using Web usage mining, evaluation of recommender algorithms has become increasingly important. In this paper we present a framework for the evaluation of different aspects of recommender systems based on the process of discovering knowledge in databases introduced by Fayyad et al. and we summarize research already done in this area. One aspect identified in the presented evaluation framework is widely neglected when dealing with recommender algorithms. This aspect is to evaluate how useful patterns extracted by recommender algorithms are to support the social process of recommending products to others, a process normally driven by recommendations by peers or experts. To fill this gap for recommender algorithms based on frequent itemsets extracted from usage data we evaluate the usefulness of two algorithms. The first recommender algorithm uses association rules, and the other algorithm is based on the repeat-buying theory known from marketing research. of usage data from an educational Internet information broker and compare useful recommendations identified by users from the target group of the broker (peers) with the recommendations produced by the algorithms. The results of the evaluation presented in this paper suggest that frequent itemsets from usage histories match the concept of useful recommendations expressed by peers with satisfactory accuracy (higher than 70%) and precision (between 60% and 90%). Also the evaluation suggests that both algorithms studied in the paper perform similar on real-world data if they are tuned properly.
7A9BD4AE	Knowledge Discovery and Data Mining	canasai kruengkrai + chuleerat jaruskulchai	2002	A parallel learning algorithm for text classification	 + em algorithm + parallel algorithm + supervised learning + Parallel computing methodologies + Computing methodologies + cluster computing + Parallel algorithms + naive bayes classifier + Language resources + Theory of computation + Models of computation + naive bayes + Concurrency + expectation maximization + Natural language processing + Design and analysis of algorithms + Artificial intelligence + Parallel computing models	+parallel expectation-maximization (EM) algorithm+naive Bayes	Text classification is the process of classifying documents into predefined categories based on their content. Existing supervised learning algorithms to automatically classify text need sufficient labeled documents to learn accurately. Applying the Expectation-Maximization (EM) algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled documents to augment the available labeled documents. Unfortunately, the time needed to learn with these large unlabeled documents is too high. This paper introduces a novel parallel learning algorithm for text classification task. The parallel algorithm is based on the combination of the EM algorithm and the naive Bayes classifier. Our goal is to improve the computational time in learning and classifying process. We studied the performance of our parallel algorithm on a large Linux PC cluster called PIRUN Cluster. We report both timing and accuracy results. These results indicate that the proposed parallel algorithm is capable of handling large document collections.
7BFA6066	Knowledge Discovery and Data Mining	yunling lu + reylong liu	2002	Incremental context mining for adaptive document classification	 + Document representation + Search engine architectures and scalability + Search engine indexing + Information retrieval + text mining + Information systems	No keyword found	Automatic document classification (DC) is essential for the management of information and knowledge. This paper explores two practical issues in DC: (1) each document has its context of discussion, and (2) both the content and vocabulary of the document database is intrinsically evolving. The issues call for adaptive document classification (ADC) that adapts a DC system to the evolving contextual requirement of each document category, so that input documents may be classified based on their contexts of discussion. We present an incremental context mining technique to tackle the challenges of ADC. Theoretical analyses and empirical results show that, given a text hierarchy, the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category. Based on the contextual requirements mined by the system, higher-precision DC may be achieved with better efficiency.
7601F94B	Knowledge Discovery and Data Mining	mark j embrechts + michinari momma + kristin p bennett	2002	MARK: a boosting algorithm for heterogeneous kernel models	 + domain knowledge + data mining + Symbolic and algebraic manipulation + Software notations and tools + Probabilistic reasoning algorithms + outliers + Theory of computation + Computer systems organization + Mathematical analysis + Computations on matrices + Systolic arrays + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + Markov-chain Monte Carlo methods + Software and its engineering + Probabilistic algorithms + kernel method + Computing methodologies + column generation + Parallel architectures + kernel function + support vector machine + Numerical analysis + Symbolic and algebraic algorithms + Design and analysis of algorithms + robust statistics + Compilers + Architectures	No keyword found	"Support Vector Machines and other kernel methods have proven to be very effective for nonlinear inference. Practical issues are how to select the type of kernel including any parameters and how to deal with the computational issues caused by the fact that the kernel matrix grows quadratically with the data. Inspired by ensemble and boosting methods like MART, we propose the Multiple Additive Regression Kernels (MARK) algorithm to address these issues. MARK considers a large (potentially infinite) library of kernel matrices formed by different kernel functions and parameters. Using gradient boosting/column generation, MARK constructs columns of the heterogeneous kernel matrix (the base hypotheses) on the fly and then adds them into the kernel ensemble. Regularization methods such as used in SVM, kernel ridge regression, and MART, are used to prevent overfitting. We investigate how MARK is applied to heterogeneous kernel ridge regression. The resulting algorithm is simple to implement and efficient. Kernel parameter selection is handled within MARK. Sampling and ""weak"" kernels are used to further enhance the computational efficiency of the resulting additive algorithm. The user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernels. MARK compares very favorably with SVM and kernel ridge regression on several benchmark datasets."
7BEA1245	Knowledge Discovery and Data Mining	rakesh agrawal + ramakrishnan srikant + johannes gehrke + alexandre evfimievski	2002	Privacy preserving mining of association rules	 + association rule + Rule learning + Cryptanalysis and other attacks + Computing / technology policy + Symbolic and algebraic manipulation + Computing methodologies + Information systems applications + Data mining + Knowledge representation and reasoning + Computer crime + Information systems + Theory of computation + Security and privacy + Intrusion/anomaly detection and malware mitigation + Symbolic and algebraic algorithms + Machine learning + Machine learning approaches + Cryptography + Social and professional topics + Design and analysis of algorithms + Artificial intelligence	No keyword found	"We present a framework for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions. While it is feasible to recover association rules and preserve privacy using a straightforward ""uniform"" randomization, the discovered rules can unfortunately be exploited to find privacy breaches. We analyze the nature of privacy breaches and propose a class of randomization operators that are much more effective than uniform randomization in limiting the breaches. We derive formulae for an unbiased support estimator and its variance, which allow us to recover itemset supports from randomized datasets, and show how to incorporate these formulae into mining algorithms. Finally, we present experimental results that validate the algorithm by applying it on real datasets."
597EDFC2	Knowledge Discovery and Data Mining	yulan liang + kingip lin + arpad kelemen	2002	Adaptive Generalized Estimation Equation with Bayes Classifier for the Job Assignment Problem	data mining + gibbs sampling + naive bayes classifier + assignment problem + multi layer perceptron + naive bayes + bayes classifier + generalized estimating equation + naive bayes algorithm + nearest neighbor + support vector machine		We propose combining advanced statistical approaches with data mining techniques to build classifiers to enhance decision-making models for the job assignment problem. Adaptive Generalized Estimation Equation (AGEE) approaches with Gibbs sampling under Bayesian framework and adaptive Bayes classifiers based on the estimations of AGEE models which uses modified Naive Bayes algorithm are proposed. The proposed classifiers have several important features. Firstly, it accounts for the correlation among the outputs and the indeterministic subjective noise into the estimation of parameters. Secondly, it reduces the number of attributes used to predict the class. Moreover, it drops the assumption of independence made by the Naive Bayes classifier. We apply our techniques to the problem of assigning jobs to Navy officers, with the goal of enhancing happiness for both the Navy and the officers. The classification results were compared with nearest neighbor, Multi-Layer Perceptron and Support Vector Machine approaches.
7819347A	Knowledge Discovery and Data Mining	jiuyong li + hong shen + rodney topor	2002	Construct robust rule sets for classification	 + association rule + Rule learning + data mining + classification rule + Information systems applications + Computing methodologies + Relational database query languages + relational database + Data mining + Knowledge representation and reasoning + association rule mining + Information systems + Machine learning + Machine learning approaches + Data management systems + Relational database model + Database design and models + Artificial intelligence + Query languages	+association rule+classification rule	We study the problem of computing classification rule sets from relational databases so that accurate predictions can be made on test data with missing attribute values. Traditional classifiers perform badly when test data are not as complete as the training data because they tailor a training database too much. We introduce the concept of one rule set being more robust than another, that is, able to make more accurate predictions on test data with missing attribute values. We show that the optimal class association rule set is as robust as the complete class association rule set. We then introduce the k-optimal rule set, which provides predictions exactly the same as the optimal class association rule set on test data with up to k missing attribute values. This leads to a hierarchy of k-optimal rule sets in which decreasing size corresponds to decreasing robustness, and they all more robust than a traditional classification rule set. We introduce two methods to find k-optimal rule sets, i.e. an optimal association rule mining approach and a heuristic approximate approach. We show experimentally that a k-optimal rule set generated by the optimal association rule mining approach performs better than that by the heuristic approximate approach and both rule sets perform significantly better than a typical classification rule set (C4.5Rules) on incomplete test data.
7CC64163	Knowledge Discovery and Data Mining	jaideep vaidya + chris clifton	2002	Privacy preserving association rule mining in vertically partitioned data	 + association rule + Parallel and distributed DBMSs + data mining + Information systems applications + machine learning + Data mining + association rule mining + Information systems + Database management system engines + evaluation + Theory and algorithms for application domains + Theory of computation + Database and storage security + transaction data + Security and privacy + Data management systems + clustering + Theory of database privacy and security + Database theory	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications-- Data mining+H.2.4 [Database Management+Systems-- Distributed atabases+H.2.7 [Database Management+Database Administration--Security+integrity+and protec	Privacy considerations often constrain data mining projects. This paper addresses the problem of association rule mining where transactions are distributed across sources. Each site holds some attributes of each transaction, and the sites wish to collaborate to identify globally valid association rules. However, the sites must not reveal individual transaction data. We present a two-party algorithm for efficiently discovering frequent itemsets with minimum support levels, without either site revealing individual transaction values.
7D280A3E	Knowledge Discovery and Data Mining	vincent s m tseng + chingpin kao	2002	Efficiently Mining Gene Expression Data via Integrated Clustering and Validation Techniques	microarray data + data mining		In recent years, the microarray techniques have received extensive attentions due to its wide applications in biomÃ©dical industry. The main advantage of microarray technique is it allows simultaneous studies of the expressions of thousands of genes in a single experiment. Analyzing the microarray data is a challenge that arises the applications of various clustering methods used for data mining. Although a number of clustering methods have been proposed, they can not meet the requirements of automation, high quality and high efficiency at the same time in analyzing gene expression data. In this paper, we propose an automatic and efficient clustering approach for mining gene expression data produced via microarray techniques. Through performance experiments on real data sets, the proposed method is shown to achieve higher efficiency, clustering quality and automation than other clustering methods.
7D50713A	Knowledge Discovery and Data Mining	mingyen thomas su + ke wang	2002	"Item selection by ""hub-authority"" profit ranking"	profitability +  + web pages + Symbolic and algebraic manipulation + Information systems applications + Computing methodologies + machine learning + Data mining + Information systems + evaluation + Theory of computation + Algebraic algorithms + Mathematical analysis + Symbolic and algebraic algorithms + clustering + Mathematics of computing + Mathematical optimization + Design and analysis of algorithms	No keyword found	"A fundamental problem in business and other applications is ranking items with respect to some notion of profit based on historical transactions. The difficulty is that the profit of one item not only comes from its own sales, but also from its influence on the sales of other items, i.e., the ""cross-selling effect"". In this paper, we draw an analogy between this influence and the mutual reinforcement of hub/authority web pages. Based on this analogy, we present a novel approach to the item ranking problem.We apply this ranking approach to solve two selection problems. In size-constrained selection, the maximum number of items that can be selected is fixed. In cost-constrained selection, there is no maximum number of items to be selected, but there is some cost associated with the selection of each item. In both cases, the question is what items should be selected to maximize the profit. Empirically, we show that this method finds profitable items in the presence of cross-selling effect."
06D0B1FC	Knowledge Discovery and Data Mining	shihfu chang + ana b benitez	2002	Multimedia Knowledge Integration, Summarization And Evaluation	bayesian network + bayesian learning + graph theory + knowledge integration		"
This paper presents new methods for automatically integrating, summarizing and evaluating multimedia knowledge. These are essential for multimedia applications to efficiently and coherently deal with multimedia knowledge at different abstraction levels such as perceptual and semantic knowledge (e.g., image clusters and word senses, respectively). The proposed methods include automatic techniques (1) for interrelating the concepts in the multimedia knowledge using probabilistic Bayesian learning, (2) for reducing the size of multimedia knowledge by clustering the concepts and collapsing the relationships among the clusters, and (3) for evaluating the quality of multimedia knowledge using notions from information and graph theory. Experiments show the potential of knowledge integration techniques for improving the knowledge quality, the importance of good concept distance measures for clustering and summarizing knowledge, and the usefulness of automatic measures for comparing the effects of different processing techniques on multimedia knowledge.
"
5B62F8FC	Knowledge Discovery and Data Mining	ahhwee tan + hong pan	2002	Adding Personality to Information Clustering	self organization + ease of use + information management		This article presents a new information management method called user-configurable clustering that integrates the flexibility of clustering systems in handling novel data and the ease of use of categorization systems in providing structure. Based on a predictive self-organizing network that performs synchronized clustering of information and preference vectors, a user can influence the clustering of information vectors by encoding his/her preferences as preference vectors. We illustrate a sample session to show how a user may create and personalize an information portfolio according to his/her preferences and how the system discovers novel information groupings while organizing familiar information according to user-defined themes.
790C3BCD	Knowledge Discovery and Data Mining	sholom m weiss + naval k verma	2002	A system for real-time competitive market intelligence	 + text analysis + Rule learning + Learning paradigms + Computing methodologies + Knowledge representation and reasoning + Logical and relational learning + Inductive logic learning + document retrieval + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + lifetime value + Supervised learning by classification + Artificial intelligence + real time + competitive analysis	No keyword found	A method is described for real-time market intelligence and competitive analysis. News stories are collected online for a designated group of companies. The goal is to detect critical differences in the text written about a company versus the text for its competitors. A solution is found by mapping the task into a non-stationary text categorization model. The overall design consists of the following components: (a) a real-time crawler that monitors newswires for stories about the competitors (b) a conditional document retriever that selects only those documents that meet the indicated conditions (c) text analysis techniques that convert the documents to a numerical format (d) rule induction methods for finding patterns in data (e) presentation techniques for displaying results. The method is extended to combine text with numerical measures, such as those based on stock prices and market capitalizations, that allow for more objective evaluations and projections.
7A955FF9	Knowledge Discovery and Data Mining	matthew r richardson + pedro domingos	2002	Mining knowledge-sharing sites for viral marketing	 + social networks + data mining + direct marketing + Information systems applications + Computing methodologies + Law, social and behavioral sciences + Data mining + linear models + probabilistic model + Information systems + social network + Logical and relational learning + Applied computing + Inductive logic learning + viral marketing + Machine learning + Machine learning approaches + linear model	eol>Probabilistic models + linear models + direct marketing + viral marketing + social networks + knowledge sharing	Viral marketing takes advantage of networks of influence among customers to inexpensively achieve large changes in behavior. Our research seeks to put it on a firmer footing by mining these networks from data, building probabilistic models of them, and using these models to choose the best viral marketing plan. Knowledge-sharing sites, where customers review products and advise each other, are a fertile source for this type of data mining. In this paper we extend our previous techniques, achieving a large reduction in computational cost, and apply them to data from a knowledge-sharing site. We optimize the amount of marketing funds spent on each customer, rather than just making a binary decision on whether to market to him. We take into account the fact that knowledge of the network is partial, and that gathering that knowledge can itself have a cost. Our results show the robustness and utility of our approach.
5EF1F7E4	Knowledge Discovery and Data Mining	yuxin ding + dityan yeung	2002	User Profiling for Intrusion Detection Using Dynamic and Static Behavioral Models	network security + instance based learning + cross entropy + maximum likelihood + hidden markov model + behavior modeling + hypothesis test + intrusion detection + anomaly detection		Intrusion detection has emerged as an important approach to network security. In this paper, we adopt an anomaly detection approach by detecting possible intrusions based on user profiles built from normal usage data. In particular, user profiles based on Unix shell commands are modeled using two different types of behavioral models. The dynamic modeling approach is based on hidden Markov models (HMM) and the principle of maximum likelihood, while the static modeling approach is based on event occurrence frequency distributions and the principle of minimum cross entropy. The novelty detection approach is adopted to estimate the model parameters using normal training data only. To determine whether a certain behavior is similar enough to the normal model and hence should be classified as normal, we use a scheme that can be justified from the perspective of hypothesis testing. Our experimental results show that static modeling outperforms dynamic modeling for this application. Moreover, the static modeling approach based on cross entropy is similar in performance to instance-based learning reported previously by others for the same dataset but with much higher computational and storage requirements than our method.
7AB88390	Knowledge Discovery and Data Mining	hichem frigui	2002	SyMP: an efficient clustering approach to identify clusters of arbitrary shapes in large data sets	Cluster analysis +  + gaussian mixture model + Retrieval tasks and goals + gaussian mixture models + Learning paradigms + Computing methodologies + Multimedia databases + Information systems applications + Clustering and classification + Information retrieval + Clustering + Data mining + Unsupervised learning + Information systems + Machine learning + clustering + oscillations + Multimedia information systems	+Gaussian Mixture Models+large datasets	We propose a new clustering algorithm, called SyMP, which is based on synchronization of pulse-coupled oscillators. SyMP represents each data point by an Integrate-and-Fire oscillator and uses the relative similarity between the points to model the interaction between the oscillators. SyMP is robust to noise and outliers, determines the number of clusters in an unsupervised manner, identifies clusters of arbitrary shapes, and can handle very large data sets. The robustness of SyMP is an intrinsic property of the synchronization mechanism. To determine the optimum number of clusters, SyMP uses a dynamic resolution parameter. To identify clusters of various shapes, SyMP models each cluster by multiple Gaussian components. The number of components is automatically determined using a dynamic intra-cluster resolution parameter. Clusters with simple shapes would be modeled by few components while clusters with more complex shapes would require a larger number of components. The scalable version of SyMP uses an efficient incremental approach that requires a simple pass through the data set. The proposed clustering approach is empirically evaluated with several synthetic and real data sets, and its performance is compared with CURE.
81757AD8	Knowledge Discovery and Data Mining	ashish sureka + peter r wurman + neeraj r joshi + h s shah	2002	Mining eBay: Bidding Strategies and Shill Detection	association rule + cross correlation		Millions of people participate in online auctions on websites such as eBay. The data available in these public markets offer interesting opportunities to study Internet auctions. We explore techniques for identifying common bidding patterns on eBay using data from eBay video game console auctions. The analysis reveals that there are certain bidding behaviors that appear frequently in the data, some of which have been previously identified and others which are new. We propose new attributes of bidding engagements and rules for classifying strategies. In addition, we suggest economic motivations that might lead to the identified behaviors. We then apply a clustering algorithm to look at each bidderâs behavior across several engagements, and find a few natural clusters. Finally, we apply association rule analysis to the data and find cases of likely shill behavior, but find no cross-correlation with any particular bidding behavior.
80F79801	Knowledge Discovery and Data Mining	andrew storey + marcdavid cohen	2002	Exploiting response models: optimizing cross-sell and up-sell opportunities in banking	profitability +  + Operations research + Approximation + data mining + assignment problem + Functional analysis + Business process management + cross selling + strategic planning + Marketing + Theory of computation + Enterprise computing + Applied computing + Mathematical analysis + return on investment + Approximation algorithms analysis + Mathematics of computing + expected value + Mathematical optimization + Design and analysis of algorithms + constrained optimization + customer behavior	+Cross-selling+Up-selling+Profit Optimization+Assignment Problems+Constrained Optimization. Response Models	The banking industry regularly mounts campaigns to improve customer value by offering new products to existing customers. In recent years this approach has gained significant momentum because of the increasing availability of customer data and the improved analysis capabilities in data mining. Typically, response models based on historical data are used to estimate the probability of a customer purchasing an additional product and the expected return from that additional purchase. Even with these computational improvements and accurate models of customer behavior, the problem of efficiently using marketing resources to maximize the return on marketing investment is a challenge. This problem is compounded because of the capability to launch multiple campaigns through several distribution channels over multiple time periods. The combination of alternatives creates a complicated array of possible actions. This paper presents a solution that answers the question of what products, if any, to offer to each customer in a way that maximizes the marketing return on investment. The solution is an improvement over the usual approach of picking the customers that have the largest expected value for a particular product because it is a global maximization from the viewpoint of the bank and allows for the effective implementation of business constraints across customers and business units. The approach accounts for limited resources, multiple sequential campaigns, and other business constraints. Furthermore, the solution provides insight into the cost of these constraints, in terms of decreased profits, and thus is an effective tool for both tactical campaign execution and strategic planning.
5F82C7A4	Knowledge Discovery and Data Mining	william perrizo + qin ding	2002	Association Rule Mining on Remotely Sensed Images Using P-trees	association rule + association rule mining + precision agriculture + data mining + tree structure + spatial data		Association Rule Mining, originally proposed for market basket data, has potential applications in many areas. Remote Sensed Imagery (RSI) data is one of the promising application areas. Extracting interesting patterns and rules from datasets composed of images and associated ground data, can be of importance in precision agriculture, community planning, resource discovery and other areas. However, in most cases the image data sizes are too large to be mined in a reasonable amount of time using existing algorithms. In this paper, we propose an approach to derive association rules on RSI data using Peano Count Tree (P-tree) structure. P-tree structure, proposed in our previous work, provides a lossless and compressed representation of image data. Based on P-trees, an efficient association rule mining algorithm P-ARM with fast support calculation and significant pruning techniques are introduced to improve the efficiency of the rule mining process. P-ARM algorithm is implemented and compared with FP-growth and Apriori algorithms. Experimental results showed that our algorithm is superior for association rule mining on RSI spatial data.
7665CFFE	Knowledge Discovery and Data Mining	ramesh c agarwal + vipin kumar + mahesh v joshi	2002	Predicting rare classes: can boosting make any weak learner strong?	semi supervised learning +  + boosting + Symbolic and algebraic manipulation + Computing methodologies + classification + Complexity classes + ensemble learning + Theory of computation + satisfiability + Mathematical analysis + Symbolic and algebraic algorithms + Computational complexity and cryptography + Mathematics of computing + Design and analysis of algorithms + Mathematical optimization	No keyword found	Boosting is a strong ensemble-based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner, as long as it satisfies the condition of yielding weighted accuracy > 0.5. In this paper, we analyze boosting with respect to this basic condition on the base learner, to see if boosting ensures prediction of rarely occurring events with high recall and precision. First we show that a base learner can satisfy the required condition even for poor recall or precision levels, especially for very rare classes. Furthermore, we show that the intelligent weight updating mechanism in boosting, even in its strong cost-sensitive form, does not prevent cases where the base learner always achieves high precision but poor recall or high recall but poor precision, when mapped to the original distribution. In either of these cases, we show that the voting mechanism of boosting falls to achieve good overall recall and precision for the ensemble. In effect, our analysis indicates that one cannot be blind to the base learner performance, and just rely on the boosting mechanism to take care of its weakness. We validate our arguments empirically on variety of real and synthetic rare class problems. In particular, using AdaCost as the boosting algorithm, and variations of PNrule and RIPPER as the base learners, we show that if algorithm A achieves better recall-precision balance than algorithm B, then using A as the base learner in AdaCost yields significantly better performance than using B as the base learner.
7A00BA42	Knowledge Discovery and Data Mining	junichi takeuchi + kenji yamanishi	2002	A unifying framework for detecting outliers and change points from non-stationary time series data	 + Probabilistic computation + change point detection + data mining + Probabilistic reasoning algorithms + moving average + Data mining + outlier detection + probabilistic model + Information systems + Theory of computation + auto regressive + Sequential Monte Carlo methods + Probability and statistics + time series data + Mathematics of computing + Markov-chain Monte Carlo methods + Probabilistic algorithms + data analysis + Information systems applications + time series + classification + Models of computation + change detection + expectation maximization	No keyword found	We are concerned with the issues of outlier detection and change point detection from a data stream. In the area of data mining, there have been increased interest in these issues since the former is related to fraud detection, rare event discovery, etc., while the latter is related to event/trend by change detection, activity monitoring, etc. Specifically, it is important to consider the situation where the data source is non-stationary, since the nature of data source may change over time in real applications. Although in most previous work outlier detection and change point detection have not been related explicitly, this paper presents a unifying framework for dealing with both of them on the basis of the theory of on-line learning of non-stationary time series. In this framework a probabilistic model of the data source is incrementally learned using an on-line discounting learning algorithm, which can track the changing data source adaptively by forgetting the effect of past data gradually. Then the score for any given data is calculated to measure its deviation from the learned model, with a higher score indicating a high possibility of being an outlier. Further change points in a data stream are detected by applying this scoring method into a time series of moving averaged losses for prediction using the learned model. Specifically we develop an efficient algorithms for on-line discounting learning of auto-regression models from time series data, and demonstrate the validity of our framework through simulation and experimental applications to stock market data analysis.
0428CA75	Knowledge Discovery and Data Mining	christopher m b taylor + john l pfaltz	2002	Closed Set Mining of Biological Data	data mining + biological data		"

"
5CAC2044	Knowledge Discovery and Data Mining	liu zongtian + mongli lee + wynne hsu + xie zhipeng	2002	SNNB: A Selective Neighborhood Based NaÃ¯ve Bayes for Lazy Learning	classification + naive bayes + bayes classifier		NaÃ¯ve Bayes is a probability-based classification method which is based on the assumption that attributes are conditionally mutually independent given the class label. Much research has been focused on improving the accuracy of NaÃ¯ve Bayes via eager learning. In this paper, we propose a novel lazy learning algorithm, Selective Neighbourhood based NaÃ¯ve Bayes (SNNB). SNNB computes different distance neighborhoods of the input new object, lazily learns multiple NaÃ¯ve Bayes classifiers, and uses the classifier with the highest estimated accuracy to make decision. The results of our experiments on 26 datasets show that our proposed SNNB algorithm is efficient and it outperforms NaÃ¯ve Bayes, and state-of-the-art classification methods NBTree, CBA, and C4.5 in terms of accuracy.
77C31EF3	Knowledge Discovery and Data Mining	bing liu + tong heng phang + haoran wu + xiaoli li	2002	A refinement approach to handling model misfit in text categorization	 + Probabilistic computation + data fitting + Probabilistic algorithms + Statistical graphics + data mining + information retrieval + Computing methodologies + Probabilistic reasoning algorithms + machine learning + Language resources + Statistical paradigms + Models of computation + Theory of computation + Natural language processing + Sequential Monte Carlo methods + Probability and statistics + bayesian classifier + Mathematics of computing + Artificial intelligence + Markov-chain Monte Carlo methods	+naÃ¯ve Bayesian classifier+Rocchio algorithm	Text categorization or classification is the automated assigning of text documents to pre-defined classes based on their contents. This problem has been studied in information retrieval, machine learning and data mining. So far, many effective techniques have been proposed. However, most techniques are based on some underlying models and/or assumptions. When the data fits the model well, the classification accuracy will be high. However, when the data does not fit the model well, the classification accuracy can be very low. In this paper, we propose a refinement approach to dealing with this problem of model misfit. We show that we do not need to change the classification technique itself (or its underlying model) to make it more flexible. Instead, we propose to use successive refinements of classification on the training data to correct the model misfit. We apply the proposed technique to improve the classification performance of two simple and efficient text classifiers, the Rocchio classifier and the naÃ¯ve Bayesian classifier. These techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a text classifier. Extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically. In particular, our refined model is able to improve the naÃ¯ve Bayesian or Rocchio classifier's prediction performance by 45% on average.
7981DB48	Knowledge Discovery and Data Mining	kevin chenchuan chang + jiawei han + hwanjo yu	2002	PEBL: positive example based learning for Web page classification using SVM	 + web pages + support vector machine + web mining + Supervised learning + Classification and regression trees + Learning paradigms + Machine learning + Machine learning approaches + Computing methodologies + Supervised learning by classification + Information systems	+and evaluation+1.7.m [Comput ing Methodologies	"Web page classification is one of the essential techniques for Web mining. Specifically, classifying Web pages of a user-interesting class is the first step of mining interesting information from the Web. However, constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples. For instance, in order to construct a ""homepage"" classifier, one needs to collect a sample of homepages (positive examples) and a sample of non-homepages (negative examples). In particular, collecting negative training examples requires arduous work and special caution to avoid biasing them. We introduce in this paper the Positive Example Based Learning (PEBL) framework for Web page classification which eliminates the need for manually collecting negative training examples in pre-processing. We present an algorithm called Mapping-Convergence (M-C) that achieves classification accuracy (with positive and unlabeled data) as high as that of traditional SVM (with positive and negative data). Our experiments show that when the M-C algorithm uses the same amount of positive examples as that of traditional SVM, the M-C algorithm performs as well as traditional SVM."
00AA3F32	Knowledge Discovery and Data Mining	lawrence b holder + istvan jonyer + diane j cook	2002	Concept Formation Using Graph Grammars	concept formation + machine learning + graph representation		"
Recognizing the expressive power of graph representation and the ability of certain graph grammars to generalize, we attempt to use graph grammar learning for concept formation. In this paper we describe our initial progress toward that goal, and focus on how certain graph grammars can be learned from examples. We also establish grounds for using graph grammars in machine learning tasks. Several examples are presented to highlight the validity of the approach.
"
75888E7D	Knowledge Discovery and Data Mining	daniel s weld + corin r anderson + pedro domingos	2002	Relational Markov models and their application to adaptive web navigation	 + sparse data + state space + web mining + web navigation + Information systems applications + Computing methodologies + e commerce + Data mining + probabilistic model + Information systems + difference set + personalization + Logical and relational learning + Inductive logic learning + markov model + Machine learning + Machine learning approaches + markov models + shrinkage	Markov models + relational probabilistic models + Web mining + personalization + shrinkage	Relational Markov models (RMMs) are a generalization of Markov models where states can be of different types, with each type described by a different set of variables. The domain of each variable can be hierarchically structured, and shrinkage is carried out over the cross product of these hierarchies. RMMs make effective learning possible in domains with very large and heterogeneous state spaces, given only sparse data. We apply them to modeling the behavior of web site users, improving prediction in our PROTEUS architecture for personalizing web sites. We present experiments on an e-commerce and an academic web site showing that RMMs are substantially more accurate than alternative methods, and make good predictions even when applied to previously-unvisited parts of the site.
7767905F	Knowledge Discovery and Data Mining	geneva g belford + rueyhsia li	2002	Instability of decision tree classification algorithms	Trees +  + association rule + tree structure + classification rule + data mining + Game tree search + decision tree classifier + Computing methodologies + confidence level + Record storage systems + Graph theory + Search methodologies + Information systems + decision tree + Information storage systems + B-trees + Discrete mathematics + Mathematics of computing + Directory structures + Discrete space search + Artificial intelligence	No keyword found	The instability problem of decision tree classification algorithms is that small changes in input training samples may cause dramatically large changes in output classification rules. Different rules generated from almost the same training samples are against human intuition and complicate the process of decision making. In this paper, we present fundamental theorems for the instability problem of decision tree classifiers. The first theorem gives the relationship between a data change and the resulting tree structure change (i.e. split change). The second theorem, Instability Theorem, provides the cause of the instability problem. Based on the two theorems, algorithmic improvements can be made to lessen the instability problem. Empirical results illustrate the theorem statements. The trees constructed by the proposed algorithm are more stable, noise-tolerant, informative, expressive, and concise. Our proposed sensitivity measure can be used as a metric to evaluate the stability of splitting predicates. The tree sensitivity is an indicator of the confidence level in rules and the effective lifetime of rules.
7BB3D075	Knowledge Discovery and Data Mining	r douglas martin + kjell konis + ruben h zamar + fatemah alqallaf	2002	Scalable robust covariance and correlation estimates for data mining	robust estimator +  + Statistical graphics + data mining + Symbolic and algebraic manipulation + correlation matrix + outliers + principal component + Data mining + outlier detection + Information systems + Statistical paradigms + Theory of computation + Semantics and reasoning + Mathematical analysis + Computations on matrices + Probability and statistics + Program reasoning + Mathematics of computing + mahalanobis distance + Information systems applications + Computing methodologies + Program verification + Numerical analysis + Symbolic and algebraic algorithms + Design and analysis of algorithms + robust statistics	+Outliers+Robust Statistics+Robust Estima- tors+Scalable Algorithm	Covariance and correlation estimates have important applications in data mining. In the presence of outliers, classical estimates of covariance and correlation matrices are not reliable. A small fraction of outliers, in some cases even a single outlier, can distort the classical covariance and correlation estimates making them virtually useless. That is, correlations for the vast majority of the data can be very erroneously reported; principal components transformations can be misleading; and multidimensional outlier detection via Mahalanobis distances can fail to detect outliers. There is plenty of statistical literature on robust covariance and correlation matrix estimates with an emphasis on affine-equivariant estimators that possess high breakdown points and small worst case biases. All such estimators have unacceptable exponential complexity in the number of variables and quadratic complexity in the number of observations. In this paper we focus on several variants of robust covariance and correlation matrix estimates with quadratic complexity in the number of variables and linear complexity in the number of observations. These estimators are based on several forms of pairwise robust covariance and correlation estimates. The estimators studied include two fast estimators based on coordinate-wise robust transformations embedded in an overall procedure recently proposed by [14]. We show that the estimators have attractive robustness properties, and give an example that uses one of the estimators in the new Insightful Miner data mining product.
77DF94B9	Knowledge Discovery and Data Mining	gediminas adomavicius + alexander tuzhilin	2002	Handling very large numbers of association rules in the analysis of microarray data	 + Triggers and rules + association rule + Rule learning + microarray data + data mining + association rules + Computing methodologies + Information systems applications + Knowledge representation and reasoning + Data mining + Database management system engines + Information systems + bioinformatics + weed management + Machine learning + Machine learning approaches + Data management systems + Artificial intelligence	No keyword found	The problem of analyzing microarray data became one of important topics in bioinformatics over the past several years, and different data mining techniques have been proposed for the analysis of such data. In this paper, we propose to use association rule discovery methods for determining associations among expression levels of different genes. One of the main problems related to the discovery of these associations is the scalability issue. Microarrays usually contain very large numbers of genes that are sometimes measured in 10,000s. Therefore, analysis of such data can generate a very large number of associations that can often be measured in millions. The paper addresses this problem by presenting a method that enables biologists to evaluate these very large numbers of discovered association rules during the post-analysis stage of the data mining process. This is achieved by providing several rule evaluation operators, including rule grouping, filtering, browsing, and data inspection operators, that allow biologists to validate multiple individual gane regulation patterns at a time. By iteratively applying these operators, biologists can explore a significant part of all the initially generated rules in an acceptable period of time and thus answer biological questions that are of a particular interest to him or her. To validate our method, we tested our system on the microarray data pertaining to the studies of environmental hazards and their influence of gane expression processes. As a result, we managed to answer several questions that were of interest to the biologists that had collected this data.
806F6C2A	Knowledge Discovery and Data Mining	william w cohen + jacob richman	2002	Learning to match and cluster large high-dimensional data sets for data integration	 + Cluster analysis + Personal computers and PC applications + Retrieval tasks and goals + Learning paradigms + Information systems applications + Computing methodologies + Clustering and classification + Information retrieval + Computers in other domains + data integrity + Clustering + Data mining + Unsupervised learning + Information systems + Applied computing + information extraction + high dimensional data + Machine learning + text mining + clustering	Learning + clustering + text mining + large datasets	Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities. In integrating databases found on the Web or obtained by using information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a particular domain. An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems, and is nearly always competitive with the best baseline system.
771FB04D	Knowledge Discovery and Data Mining	richard maclin + ayhan demiriz + kristin p bennett	2002	Exploiting unlabeled data in ensemble methods	 + Machine learning theory + Emerging technologies + neural network + Theory of computation + Theory and algorithms for application domains + decision tree + Mathematical analysis + Machine learning approaches + Discrete mathematics + Hardware + Mathematics of computing + Cellular neural networks + semi supervised learning + Trees + boosting + Mixed discrete-continuous optimization + Computing methodologies + Graph theory + classification + Circuit substrates + ensemble learning + Integer programming + Neural networks + Machine learning + Mathematical optimization + Design and analysis of algorithms	No keyword found	"An adaptive semi-supervised ensemble method, ASSEMBLE, is proposed that constructs classification ensembles based on both labeled and unlabeled data. ASSEMBLE alternates between assigning ""pseudo-classes"" to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data. Mathematically, this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled of data. Unlike alternative approaches, ASSEMBLE does not require a semi-supervised learning method for the base classifier. ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems. ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition. In addition, strong results on several benchmark datasets using both decision trees and neural networks support the proposed method."
5B380174	Knowledge Discovery and Data Mining	srinivasan parthasarathy + hui yang	2002	On the Use of Constrained Associations for Web Log Mining	association rule + association rule mining		In recent years there has been an increasing interest and a growing body of work in web usage mining as an underlying approach to capture and model the behavior of users on the web for business intelligence and browser performance enhancements. Web usage mining strategies range from strategies such as clustering and collaborative filtering, to accurately modeling sequential pattern navigation. However, many of these approaches suffer problems in terms of scalability and performance (especially online performance) due to the size and sparse nature of the data involved and the fact that many of the methods generate complex models that are less than amenable to an on-line decision making environment. In this paper, we present a new approach for mining web logs. Our approach discovers association rules that are constrained (and ordered) temporally. The approach relies on the simple premise that pages accessed recently have a greater influence on pages that will be accessed in the near future. The approach not only results in better predictions, it also prunes the rule-space significantly, thus enabling faster online prediction. Further refinements based on sequential dominance are also evaluated, and prove to be quite effective. Detailed experimental evaluation shows how the approach is quite effective in capturing a web userâs access patterns; consequently, our prediction model not only has good prediction accuracy, but also is more efficient in terms of space and time complexity. The approach is also likely to generalize for e-commerce recommendation systems.
5BD44029	Knowledge Discovery and Data Mining	jianchao han + nick cercone	2002	Interactive Construction of Classification Rules	domain knowledge + human perception		We introduce an interactive classifier construction system, CVizT, in which the entire process is visualized based on a multidimensional visualization technique, Table Lens. The CVizT system is a fully interactive approach. The appropriate visualization-based interaction capabilities are provided for the user to include human perception into the construction process. Our experiments with data sets from the UCI repository demonstrates that the CVizT system is straightforward and easily learned. The userâs preference and domain knowledge can also be integrated into the construction process.
7A814E19	Knowledge Discovery and Data Mining	jaime g carbonell + yiming yang + jian zhang + chun jin	2002	Topic-conditioned novelty detection	 + supervised learning + Machine learning + Computing methodologies + Information retrieval + feature selection + Information systems	+General Terms Design+Experimentation+Algorithm Keywords Novelty Detection+Named Entity+Feature Selection+Text	Automated detection of the first document reporting each new event in temporally-sequenced streams of documents is an open challenge. In this paper we propose a new approach which addresses this problem in two stages: 1) using a supervised learning algorithm to classify the on-line document stream into pre-defined broad topic categories, and 2) performing topic-conditioned novelty detection for documents in each topic. We also focus on exploiting named-entities for event-level novelty detection and using feature-based heuristics derived from the topic histories. Evaluating these methods using a set of broadcast news stories, our results show substantial performance gains over the traditional one-level approach to the novelty detection problem.
0A8C3333	Knowledge Discovery and Data Mining	jiawei han	2002	How Can Data Mining Help Bio-Data Analysis?	 + data analysis + data mining	Bio-medical data analysis + data mining + bioinformatics + data mining applications + research challenges	"
Recent progress in data mining research has led to the development of numerous efficient and scalable methods for mining interesting patterns in large databases. In the mean time, recent progress in biology, medical science, and DNA technology has led to the accumulation of tremendous amounts of bio-medical data that demands for in-depth analysis. The question becomes how to bridge the two fields, data mining and bioinformatics, for successful mining of bio-medical data. In this abstract, we analyze how data mining may help bio-medical data analysis and outline some research problems that may motivate the further developments of data mining tools for bio-data analysis.
"
764642C6	Knowledge Discovery and Data Mining	george kollios + carlotta domeniconi + dimitrios gunopulos + nick koudas + michail vlachos	2002	Non-linear dimensionality reduction techniques for classification and visualization	three dimensions +  + Data mining + Information systems + evaluation + Theory of computation + Modeling and simulation + high dimensional data + Mathematical analysis + intrinsic dimension + Approximation algorithms analysis + data visualization + Mathematics of computing + Model development and analysis + Computer vision + Approximation + Computer vision representations + Computing methodologies + Information systems applications + Simulation theory + Functional analysis + machine learning + clustering + Design and analysis of algorithms + Artificial intelligence	No keyword found	In this paper we address the issue of using local embeddings for data visualization in two and three dimensions, and for classification. We advocate their use on the basis that they provide an efficient mapping procedure from the original dimension of the data, to a lower intrinsic dimension. We depict how they can accurately capture the user's perception of similarity in high-dimensional data for visualization purposes. Moreover, we exploit the low-dimensional mapping provided by these embeddings, to develop new classification techniques, and we show experimentally that the classification accuracy is comparable (albeit using fewer dimensions) to a number of other classification procedures.
7A35DA9D	Knowledge Discovery and Data Mining	paul g sorenson + eleni stroulia + mohammad elramly	2002	From run-time behavior to usage scenarios: an interaction-pattern mining approach	 + web accessibility + software systems + Learning paradigms + Information systems applications + Symbolic and algebraic manipulation + Computing methodologies + Data mining + software requirements + Information systems + Theory of computation + Supervised learning + Classification and regression trees + behavior analysis + Symbolic and algebraic algorithms + Data structures design and analysis + Machine learning + Machine learning approaches + sequential pattern mining + software engineering + Supervised learning by classification + Design and analysis of algorithms + Pattern matching + functional requirement	+reengineefing	"A key challenge facing IT organizations today is their evolution towards adopting e-business practices that gives rise to the need for reengineering their underlying software systems. Any reengineering effort has to be aware of the functional requirements of the subject system, in order not to violate the integrity of its intended uses. However, as software systems get regularly maintained throughout their lifecycle, the documentation of their requirements often become obsolete or get lost. To address this problem of ""software requirements loss"", we have developed an interaction-pattern mining method for the recovery of functional requirements as usage scenarios. Our method analyzes traces of the run-time system-user interaction to discover frequently recurring patterns; these patterns correspond to the functionality currently exercised by the system users, represented as usage scenarios. The discovered scenarios provide the basis for reengineering the software system into web-accessible components, each one supporting one of the discovered scenarios. In this paper, we describe IPM2, our interaction-pattern discovery algorithm, we illustrate it with a case study from a real application and we give an overview of the reengineering process in the context of which it is employed."
5C07B025	Knowledge Discovery and Data Mining	enrique friasmartinez + vijay karamcheti	2002	A Customizable Behavior Model for Temporal Prediction of Web User Sequences	behavior modeling		One of the important Internet challenges in coming years will be the introduction of intelligent services and the creation of a more personalized environment for users. A key prerequisite for such services is the modeling of user behavior and a natural starting place for this are Web logs. In this paper we propose a model for predicting sequences of user accesses which is distinguished by two elements: it is customizable and it reflects sequentiality. Customizable, in this context, means that the proposed model can be adapted to the characteristics of the server to more accurately capture its behavior. The concept of sequentiality in our model consists of three elements: (1) preservation of the sequence of the click stream in the antecedent, (2) preservation of the sequence of the click stream in the consequent and (3) a measure of the gap between the antecedent and the consequent in terms of the number of user clicks.
75E4132D	Knowledge Discovery and Data Mining	mingsyan chen + chengru lin + changhung lee + philip s yu	2002	Distributed data mining in a chain store database of short transactions	distributed database +  + association rule + Rule learning + indexing terms + data mining + classification rule + Information systems applications + Computing methodologies + knowledge discovery + Data mining + Knowledge representation and reasoning + Information systems + Numerical analysis + Mathematical analysis + Number-theoretic computations + Machine learning + Machine learning approaches + Mathematics of computing + Artificial intelligence	No keyword found	In this paper, we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database. Specifically, the causality rule explored in this paper consists of a sequence of triggering events and a set of consequential events, and is designed with the capability of mining non-sequential, inter-transaction information. Hence, the causality rule mining provides a very general framework for rule derivation. Note, however, that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate sets and a distributed database, and in our opinion, cannot be dealt with by direct extensions from existing rule mining methods. Consequently, we devise in this paper a series of level matching algorithms, including Level Matching (abbreviatedly as LM), Level Matching with Selective Scan (abbreviatedly as LMS), and Distributed Level Matching (abbreviatedly as Distibuted LM), to minimize the computing cost needed for the distributed data mining of causality rules. In addition, the phenomena of time window constraints are also taken into consideration for the development of our algorithms. As a result of properly employing the technologies of level matching and selective scan, the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules. Scale-up experiments show that the proposed algorithms scale well with the number of sites and the number of customer transactions.Index Terms: knowledge discovery, distributed data mining causality rules, triggering events, consequential events
790B08D3	Knowledge Discovery and Data Mining	mohammed j zaki	2002	Efficiently mining frequent trees in a forest	Trees +  + pattern matching + web mining + social networks + direct marketing + data structure + Information systems applications + Computing methodologies + Record storage systems + Graph theory + Data mining + linear models + Information systems + viral marketing + Information storage systems + B-trees + Machine learning + Discrete mathematics + Directory structures + Mathematics of computing	No keyword found	Mining frequent trees is very useful in domains like bioinformatics, web mining, mining semistructured data, and so on. We formulate the problem of mining (embedded) subtrees in a forest of rooted, labeled, and ordered trees. We present TREEMINER, a novel algorithm to discover all frequent subtrees in a forest, using a new data structure called scope-list. We contrast TREEMINER with a pattern matching tree mining algorithm (PATTERNMATCHER). We conduct detailed experiments to test the performance and scalability of these methods. We find that TREEMINER outperforms the pattern matching approach by a factor of 4 to 20, and has good scaleup properties. We also present an application of tree mining to analyze real web logs for usage patterns.
78953FF4	Knowledge Discovery and Data Mining	xintao wu + jianping fan + kalpathi r subramanian	2002	B-EM: a classifier incorporating bootstrap with EM approach for data mining	 + em algorithm + image classification + data mining + probability distribution + Information systems applications + expectation maximization + classification + unsupervised learning + Information systems	+Categories and Subject Descriptors H.4.m [Information Systems+Miscellaneous Keywords Expectation Maximization+Classification+Supervised and Unsupervised learning+Bootstrap Method	This paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy. This is significant for many applications such as image classification where obtaining classification labels is expensive, while large unlabeled examples are easily available. We investigate an Expectation Maximization (EM) algorithm for learning from labeled and unlabeled data. The reason why unlabeled data boosts learning accuracy is because it provides the information about the joint probability distribution. A theoretical argument shows that the more unlabeled examples are combined in learning, the more accurate the result. We then introduce B-EM algorithm, based on the combination of EM with bootstrap method, to exploit the large unlabeled data while avoiding prohibitive I/O cost. Experimental results over both synthetic and real data sets that the proposed approach has a satisfactory performance.
751A414A	Knowledge Discovery and Data Mining	satoshi morinaga + kenji yamanishi + toshikazu fukushima + kenji tateishi	2002	Mining product reputations on the Web	 + survey data + Statistical graphics + web pages + search engine + Continuous optimization + correspondence analysis + Information systems applications + Data mining + customer relationship management + Information systems + Statistical paradigms + Theory of computation + Mathematical analysis + text mining + Probability and statistics + Mathematics of computing + lifetime value + Mathematical optimization + Design and analysis of algorithms + Stochastic control and optimization	No keyword found	Knowing the reputations of your own and/or competitors' products is important for marketing and customer relationship management. It is, however, very costly to collect and analyze survey data manually. This paper presents a new framework for mining product reputations on the Internet. It automatically collects people's opinions about target products from Web pages, and it uses text mining techniques to obtain the reputations of those products.On the basis of human-test samples, we generate in advance syntactic and linguistic rules to determine whether any given statement is an opinion or not, as well as whether such any opinion is positive or negative in nature. We first collect statements regarding target products using a general search engine, and then, using the rules, extract opinions from among them and attach three labels to each opinion, labels indicating the positive/negative determination, the product name itself, and an numerical value expressing the degree of system confidence that the statement is, in fact, an opinion. The labeled opinions are then input into an opinion database.The mining of reputations, i.e., the finding of statistically meaningful information included in the database, is then conducted. We specify target categories using label values (such as positive opinions of product A) and perform four types of text mining: extraction of 1) characteristic words, 2) co-occurrence words, 3) typical sentences, for individual target categories, and 4) correspondence analysis among multiple target categories.Actual marketing data is used to demonstrate the validity and effectiveness of the framework, which offers a drastic reduction in the overall cost of reputation analysis over that of conventional survey approaches and supports the discovery of knowledge from the pool of opinions on the web.
7B28E2D0	Knowledge Discovery and Data Mining	jaideep srivastava + pangning tan + vipin kumar	2002	Selecting the right interestingness measure for association patterns	contingency tables +  + data mining + Information systems applications + Data mining + contingency table + feature selection + association rule mining + Information systems	+Contingency tables+Associations	Many techniques for association rule mining and feature selection require a suitable metric to capture the dependencies among variables in a data set. For example, metrics such as support, confidence, lift, correlation, and collective strength are often used to determine the interestingness of association patterns. However, many such measures provide conflicting information about the interestingness of a pattern, and the best metric to use for a given application domain is rarely known. In this paper, we present an overview of various measures proposed in the statistics, machine learning and data mining literature. We describe several key properties one should examine in order to select the right measure for a given application domain. A comparative study of these properties is made using twenty one of the existing measures. We show that each measure has different properties which make them useful for some application domains, but not for others. We also present two scenarios in which most of the existing measures agree with each other, namely, support-based pruning and table standardization. Finally, we present an algorithm to select a small set of tables such that an expert can select a desirable measure by looking at just this small set of tables.
758B3082	Knowledge Discovery and Data Mining	spiros likothanassis + dimitris fragoudis + dimitris meretakis	2002	Integrating feature and instance selection for text classification	 + identification + support vector machine + Machine learning + Computing methodologies + majority vote + mutual information + feature selection	+Categories and Subject Descriptors 1.5.2 [Pattern Recognition+Design Methodology-Feature evaluation and selection. 1.5.4 [Pattern Recognition+Applications- Text processing	Instance selection and feature selection are two orthogonal methods for reducing the amount and complexity of data. Feature selection aims at the reduction of redundant features in a dataset whereas instance selection aims at the reduction of the number of instances. So far, these two methods have mostly been considered in isolation. In this paper, we present a new algorithm, which we call FIS (Feature and Instance Selection) that targets both problems simultaneously in the context of text classificationOur experiments on the Reuters and 20-Newsgroups datasets show that FIS considerably reduces both the number of features and the number of instances. The accuracy of a range of classifiers including NaÃ¯ve Bayes, TAN and LB considerably improves when using the FIS preprocessed datasets, matching and exceeding that of Support Vector Machines, which is currently considered to be one of the best text classification methods. In all cases the results are much better compared to Mutual Information based feature selection. The training and classification speed of all classifiers is also greatly improved.
6994FD68	Knowledge Discovery and Data Mining	ramakrishnan srikant	2002	Privacy Preserving Data Mining: Challenges and Opportunities	data mining		The goal of privacy preserving data mining is to develop accurate models without access to precise information in individual data records, thus finessing the conflict between privacy and data mining. In this talk, I will give an introduction to the techniques underlying privacy preserving data mining, and then discuss several application domains. In particular, recent events have led to an increased interest in applying data mining toward security related problems, leading to interesting technical challenges at the intersection of privacy, security and data mining.
78DEA716	Knowledge Discovery and Data Mining	olcay boz	2002	Extracting decision trees from trained neural networks	 + Machine learning theory + Emerging technologies + Game tree search + neural network + Theory and algorithms for application domains + Theory of computation + Computer systems organization + decision tree + Machine learning approaches + Discrete mathematics + Hardware + Mathematics of computing + Cellular neural networks + Discrete space search + Trees + Computing methodologies + Graph theory + Circuit substrates + Search methodologies + Neural networks + Machine learning + clustering + Other architectures + Artificial intelligence + Architectures	No keyword found	Neural Networks are successful in acquiring hidden knowledge in datasets. Their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans. Researchers tried to address this problem by extracting rules from trained Neural Networks. Most of the proposed rule extraction methods required specialized type of Neural Networks; some required binary inputs and some were computationally expensive. Craven proposed extracting MofN type Decision Trees from Neural Networks. We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimensional real world problems may be very complex. In this paper, we introduced a new method for extracting regular C4.5 like Decision Trees from trained Neural Networks. We showed that the new method (DecText) is effective in extracting high fidelity trees from trained networks. We also introduced a new discretization technique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity.
779D17C8	Knowledge Discovery and Data Mining	eamonn keogh + shruti kasetty	2002	On the need for time series data mining benchmarks: a survey and empirical demonstration	time series data + time series + data mining + indexation	time series + data mining + experimental evaluation	"
In the last decade there has been an explosion of interest in mining time series data. Literally hundreds of papers have introduced new algorithms to index, classify, cluster and segment time series. In this work we make the following claim. Much of this work has very little utility because the contribution made (speed in the case of indexing, accuracy in the case of classification and clustering, model accuracy in the case of segmentation) offer an amount of âimprovementâ that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets, or the variance that would have been observed by changing minor (unstated) implementation details. To illustrate our point, we have undertaken the most exhaustive set of time series experiments ever attempted, re-implementing the contribution of more than two dozen papers, and testing them on 50 real world, highly diverse datasets. Our empirical results strongly support our assertion, and suggest the need for a set of time series benchmarks and more careful empirical evaluation in the data mining community.
"
066AFD96	Knowledge Discovery and Data Mining	steven a eschrich + nitesh v chawla + lawrence o hall	2002	Generalization Methods in Bioinformatics	data mining + feature space + high throughput + drug discovery		"

"
58DCA9C0	Knowledge Discovery and Data Mining	robert j hilderman	2002	The Lorenz Dominance Order as a Measure of Interestingness in KDD	partial order + data mining + total order + directed graph + data structure		Ranking summaries generated from databases is useful within the context of descriptive data mining tasks where a single data set can be generalized in many different ways and to many levels of granularity. Our approach to generating summaries is based upon a data structure, associated with an attribute, called a domain generalization graph (DGG). A DGG for an attribute is a directed graph where each node represents a domain of values created by partitioning the original domain for the attribute, and each edge represents a generalization relation between these domains. Given a set of DGGs associated with a set of attributes, a generalization space can be defined as all possible combinations of domains, where one domain is selected from each DGG for each combination. This generalization space describes, then, all possible summaries consistent with the DGGs that can be generated from the selected attributes. When the number of attributes to be generalized is large or the DGGs associated with the attributes are complex, the generalization space can be very large, resulting in the generation of many summaries. The number of summaries can easily exceed the capabilities of a domain expert to identify interesting results. In this paper, we show that the Lorenz dominance order can be used to rank the summaries prior to presentation to the domain expert. The Lorenz dominance order defines a partial order on the summaries, in most cases, and in some cases, defines a total order. The rank order of the summaries represents an objective evaluation of their relative interestingness and provides the domain expert with a starting point for further subjective evaluation of the summaries.
7B676EC0	Knowledge Discovery and Data Mining	yulan liang + arpad kelemen	2002	Mining heterogeneous gene expression data with time lagged recurrent neural networks	 + dna microarray + Machine learning theory + Emerging technologies + Symbolic and algebraic manipulation + Probabilistic reasoning algorithms + Data mining + neural network + Information systems + Theory and algorithms for application domains + Theory of computation + nearest neighbor + factorial design + Machine learning approaches + Sequential Monte Carlo methods + Probability and statistics + Hardware + Mathematics of computing + Cellular neural networks + Markov-chain Monte Carlo methods + Probabilistic algorithms + recurrent neural network + back propagation + Information systems applications + Computing methodologies + time series + network architecture + Circuit substrates + support vector machine + Neural networks + backpropagation + Symbolic and algebraic algorithms + Machine learning + Design and analysis of algorithms + gene expression	+Heterogeneous+Time Lagged Neural Network+Trajectory learning+Backpropagation through time	Heterogeneous types of gene expressions may provide a better insight into the biological role of gene interaction with the environment, disease development and drug effect at the molecular level. In this paper for both exploring and prediction purposes a Time Lagged Recurrent Neural Network with trajectory learning is proposed for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments. The proposed procedures identify gene functional patterns from the dynamics of a state-trajectory learned in the heterogeneous time series and the gradient information over time. Also, the trajectory learning with Back-propagation through time algorithm can recognize gene expression patterns vary over time. This may reveal much more information about the regulatory network underlying gene expressions. The analyzed data were extracted from spotted DNA microarrays in the budding yeast expression measurements, produced by Eisen et al. The gene matrix contained 79 experiments over a variety of heterogeneous experiment conditions. The number of recognized gene patterns in our study ranged from two to ten and were divided into three cases. Optimal network architectures with different memory structures were selected based on Akaike and Bayesian information statistical criteria using two-way factorial design. The optimal model performance was compared to other popular gene classification algorithms such as Nearest Neighbor, Support Vector Machine, and Self-Organized Map. The reliability of the performance was verified with multiple iterated runs.
NE8739	Knowledge Discovery and Data Mining	V. ÄurÄin+M. Ghanem+Y. Guo+M. KÃ¶hler+A. Rowe+J. Syed+P. Wendel	2002	Discovery net: towards a grid of knowledge discovery.	 + Visual languages + Simulation types and techniques + Learning settings + Computing methodologies + Context specific languages + Information systems applications + Symbolic and algebraic manipulation + Software notations and tools + Data mining + Information systems + Theory of computation + Modeling and simulation + Visual analytics + Machine learning + Symbolic and algebraic algorithms + Design and analysis of algorithms + Software and its engineering	No keyword found	This paper provides a blueprint for constructing collaborative and distributed knowledge discovery systems within Grid-based computing environments. The need for such systems is driven by the quest for sharing knowledge, information and computing resources within the boundaries of single large distributed organisations or within complex Virtual Organisations (VO) created to tackle specific projects. The proposed architecture is built on top of a resource federation management layer and is composed of a set of different resources. We show how this architecture will behave during a typical KDD process design and deployment, how it enables the execution of complex and distributed data mining tasks with high performance and how it provides a community of e-scientists with means to collaborate, retrieve and reuse both KDD algorithms, discovery processes and knowledge in a visual analytical environment.
NE8733	Knowledge Discovery and Data Mining	Bhavani Raskutti+Herman FerrÃ¡+Adam Kowalczyk	2002	Combining clustering and co-training to enhance text classification using unlabelled data.	 + Cluster analysis + Retrieval tasks and goals + Learning paradigms + Computing methodologies + Information systems applications + Clustering and classification + Information retrieval + Clustering + Data mining + Unsupervised learning + Language resources + Information systems + Machine learning + Natural language processing + Artificial intelligence	No keyword found	In this paper, we present a new co-training strategy that makes use of unlabelled data. It trains two predictors in parallel, with each predictor labelling the unlabelled data for training the other predictor in the next round. Both predictors are support vector machines, one trained using data from the original feature space, the other trained with new features that are derived by clustering both the labelled and unlabelled data. Hence, unlike standard co-training methods, our method does not require a priori the existence of two redundant views either of which can be used for classification, nor is it dependent on the availability of two different supervised learning algorithms that complement each other.We evaluated our method with two classifiers and three text benchmarks: WebKB, Reuters newswire articles and 20 NewsGroups. Our evaluation shows that our co-training technique improves text classification accuracy especially when the number of labelled examples are very few.
NE8706	Knowledge Discovery and Data Mining	Ella Bingham+Heikki Mannila+Jouni K. SeppÃ¤nen	2002	Topics in 0--1 data.	 + Information systems applications + Probability and statistics + Mathematics of computing + Data mining + Information systems	No keyword found	Large 0--1 datasets arise in various applications, such as market basket analysis and information retrieval. We concentrate on the study of topic models, aiming at results which indicate why certain methods succeed or fail. We describe simple algorithms for finding topic models from 0--1 data. We give theoretical results showing that the algorithms can discover the epsilon-separable topic models of Papadimitriou et al. We present empirical results showing that the algorithms find natural topics in real-world data sets. We also briefly discuss the connections to matrix approaches, including nonnegative matrix factorization and independent component analysis.
5EB79B1C	Knowledge Discovery and Data Mining	jeffrey heer + adam rosien + ed h chi	2002	LumberJack: Intelligent Discovery and Analysis of Web User Traffic Composition	cluster analysis + web mining + web usage mining + world wide web + association rule		Web Usage Mining enables new understanding of user goals on the Web. This understanding has broad applications, and traditional mining techniques such as association rules have been used in business applications. We have developed an automated method to directly infer the major groupings of user traffic on a Web site [Heer01]. We do this by utilizing multiple data features of user sessions in a clustering analysis. We have performed an extensive, systematic evaluation of the proposed approach, and have discovered that certain clustering schemes can achieve categorization accuracies as high as 99% [Heer02b]. In this paper, we describe the further development of this work into a prototype service called LumberJack, a push-button analysis system that is both more automated and accurate than past systems.
8081123A	Knowledge Discovery and Data Mining	subramanyam mallela + inderjit s dhillon + rahul kumar	2002	Enhanced word clustering for hierarchical text classification	 + Cluster analysis + Learning paradigms + Data mining + Information systems + Complexity classes + Theory of computation + jensen shannon divergence + naive bayes + Natural language processing + feature selection + objective function + fractionation + Retrieval tasks and goals + Information systems applications + Computing methodologies + Clustering and classification + Information retrieval + Clustering + Unsupervised learning + Language resources + support vector machine + Machine learning + Complexity theory and logic + Computational complexity and cryptography + Artificial intelligence	No keyword found	"In this paper we propose a new information-theoretic divisive algorithm for word clustering applied to text classification. In previous work, such ""distributional clustering"" of features has been found to achieve improvements over feature selection in terms of classification accuracy, especially at lower number of features [2, 28]. However the existing clustering techniques are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost. In order to explicitly capture the optimality of word clusters in an information theoretic framework, we first derive a global criterion for feature clustering. We then present a fast, divisive algorithm that monotonically decreases this objective function value, thus converging to a local minimum. We show that our algorithm minimizes the ""within-cluster Jensen-Shannon divergence"" while simultaneously maximizing the ""between-cluster Jensen-Shannon divergence"". In comparison to the previously proposed agglomerative strategies our divisive algorithm achieves higher classification accuracy especially at lower number of features. We further show that feature clustering is an effective technique for building smaller class models in hierarchical classification. We present detailed experimental results using Naive Bayes and Support Vector Machines on the 20 Newsgroups data set and a 3-level hierarchy of HTML documents collected from Dmoz Open Directory."
5F658069	Knowledge Discovery and Data Mining	george forman	2002	Incremental Machine Learning to Reduce Biochemistry Lab Costs in the Search for Drug Discovery	machine learning + concept drift + drug discovery + reinforcement learning + supervised learning		"
This paper promotes the use of supervised machine learning in laboratory settings where chemists have a large number of samples to test for some property, and are interested in identifying as many positive instances for the least laboratory testing effort. Rather than traditional supervised learning where the chemists would first develop a large training set and then train a classifier, the paper promotes incrementally re-training from each lab test as it completes and then predicting the next best sample to test, as in the field of reinforcement learning. The method outperformed the 2001 KDD Cup thrombin competition winner, partly due to its reduced risk to concept drift from training set to test set.
"
7E6584A5	Knowledge Discovery and Data Mining	vijay s iyengar	2002	Transforming data to satisfy privacy constraints	 + Computer vision problems + Computing / technology policy + privacy + Data mining + Privacy policies + Information systems + genetic algorithm + Theory and algorithms for application domains + Theory of computation + regression model + Security and privacy + satisfiability + Mathematical analysis + Computer graphics + data transformation + Mathematics of computing + Social and professional topics + Database theory + Human and societal aspects of security and privacy + Reconstruction + Computer vision + prediction model + Information systems applications + Computing methodologies + data dissemination + Database and storage security + Numerical analysis + generalization + predictive modeling + Computation of transforms + Image manipulation + Theory of database privacy and security + Artificial intelligence	+predictive	Data on individuals and entities are being collected widely. These data can contain information that explicitly identifies the individual (e.g., social security number). Data can also contain other kinds of personal information (e.g., date of birth, zip code, gender) that are potentially identifying when linked with other available data sets. Data are often shared for business or legal reasons. This paper addresses the important issue of preserving the anonymity of the individuals or entities during the data dissemination process. We explore preserving the anonymity by the use of generalizations and suppressions on the potentially identifying portions of the data. We extend earlier works in this area along various dimensions. First, satisfying privacy constraints is considered in conjunction with the usage for the data being disseminated. This allows us to optimize the process of preserving privacy for the specified usage. In particular, we investigate the privacy transformation in the context of data mining applications like building classification and regression models. Second, our work improves on previous approaches by allowing more flexible generalizations for the data. Lastly, this is combined with a more thorough exploration of the solution space using the genetic algorithm framework. These extensions allow us to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraints.
7D5205D9	Knowledge Discovery and Data Mining	bin fang + mong li lee + wynne hsu	2002	Tumor cell identification using features rules	 + majority voting + Theory of computation + identification + Mathematical analysis + Computer graphics + Discrete mathematics + Approximation algorithms analysis + Mathematics of computing + quantitative analysis + Logic + image processing + Automated reasoning + Approximation + Computing methodologies + majority vote + Functional analysis + Graph theory + Computational geometry + Rendering + region of interest + Image manipulation + Visibility + Graph algorithms + Randomness, geometry and discrete structures + Design and analysis of algorithms	identification + features rules + local adaptive thresholding + dynamic water immersion + meta classifier + majority vote + weighted vote	Advances in imaging techniques have led to large repositories of images. There is an increasing demand for automated systems that can analyze complex medical images and extract meaningful information for mining patterns. Here, we describe a real-life image mining application to the problem of tumour cell counting. The quantitative analysis of tumour cells is fundamental to characterizing the activity of tumour cells. Existing approaches are mostly manual, time-consuming and subjective. Efforts to automate the process of cell counting have largely focused on using image processing techniques only. Our studies indicate that image processing alone is unable to give accurate results. In this paper, we examine the use of extracted features rules to aid in the process of tumor cell counting. We propose a robust local adaptive thresholding and dynamic water immersion algorithms to segment regions of interesting from background. Meaningful features are then extracted from the segmented regions. A number of base classifiers are built to generate features rules to help identify the tumor cell. Two voting strategies are implemented to combine the base classifiers into a meta-classifier. Experiment results indicate that this process of using extracted features rules to help identify tumor cell leads to better accuracy than pure image processing techniques alone.
NE8689	Knowledge Discovery and Data Mining	Aleksander KoÅcz+Xiaomei Sun+Jugal Kalita	2002	Efficient handling of high-dimensional feature spaces by randomized classifier ensembles.	 + Game tree search + Information systems applications + Symbolic and algebraic manipulation + Computing methodologies + Data mining + Information systems + Search methodologies + Computability + Models of computation + Theory of computation + Turing machines + Symbolic and algebraic algorithms + Discrete space search + Artificial intelligence + Design and analysis of algorithms	No keyword found	"Handling massive datasets is a difficult problem not only due to prohibitively large numbers of entries but in some cases also due to the very high dimensionality of the data. Often, severe feature selection is performed to limit the number of attributes to a manageable size, which unfortunately can lead to a loss of useful information. Feature space reduction may well be necessary for many stand-alone classifiers, but recent advances in the area of ensemble classifier techniques indicate that overall accurate classifier aggregates can be learned even if each individual classifier operates on incomplete ""feature view"" training data, i.e., such where certain input attributes are excluded. In fact, by using only small random subsets of features to build individual component classifiers, surprisingly accurate and robust models can be created. In this work we demonstrate how these types of architectures effectively reduce the feature space for submodels and groups of sub-models, which lends itself to efficient sequential and/or parallel implementations. Experiments with a randomized version of Adaboost are used to support our arguments, using the text classification task as an example."
783471A0	Knowledge Discovery and Data Mining	jouni k seppanen + ella bingham + heikki mannila	2002	Topics in 0--1 data	independent component analysis + nonnegative matrix factorization + information retrieval + clustering + market basket analysis		"
Large 0-1 datasets arise in various applications, such as market basket analysis and information retrieval. We concentrate on the study of topic models, aiming at results which indicate why certain methods succeed or fail. We describe simple algorithms for finding topic models from 0-1 data. We give theoretical results showing that the algorithms can discover the epsilon-separable topic models of Papadimitriou et al. We present empirical results showing that the algorithms find natural topics in real-world data sets. We also briefly discuss the connections to matrix approaches, including nonnegative matrix factorization and independent component analysis.
"
7753DAEA	Knowledge Discovery and Data Mining	bing liu + alexander tuzhilin	2002	Querying multiple sets of discovered rules	 + Triggers and rules + association rule + data mining + association rules + Information systems applications + query languages + Information retrieval + Information retrieval query processing + Data mining + Information systems + Database management system engines + rule based + first order logic + Data management systems + query language + Query languages	eol>Data mining queries + rulebases + association rules + query languages + query evaluation	Rule mining is an important data mining task that has been applied to numerous real-world applications. Often a rule mining system generates a large number of rules and only a small subset of them is really useful in applications. Although there exist some systems allowing the user to query the discovered rules, they are less suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rules. In this paper, we propose a new powerful rule query language Rule-QL for querying multiple rulebases that is modeled after SQL and has rigorous theoretical foundations of a rule-based calculus. In particular, we first propose a rule-based calculus RC based on the first-order logic, and then present the language Rule-QL that is at least as expressive as the safe fragment of RC. We also propose a number of efficient query evaluation techniques for Rule-QL and test them experimentally on some representative queries to demonstrate the feasibility of Rule-QL.
7F409EB8	Knowledge Discovery and Data Mining	masaru kitsuregawa + bowo prasetyo + iko pramudiono + katsumi takahashi	2002	Naviz: Website Navigational Behavior Visualizer	sequential pattern mining + web accessibility + data mining + directory service + web pages		Navigational behavior of website visitors can be extracted from web access log files with data mining techniques such as sequential pattern mining. Visualization of the discovered patterns is very helpful to understand how visitors navigate over the various pages on the site. Currently several web log visualization tools have been developed. However those tools are far from satisfactory. They do not provide global view of visitor access as well as individual traversal path effectively. Here we introduce Naviz, a system of interactive web log visualization that is designed to overcome those drawbacks. It combines two-dimensional graph of visitor access traversals that considers appropriate web traversal properties, i.e. hierarchization regarding traversal traffic and grouping of related pages, and facilities for filtering traversal paths by specifying visited pages and path attributes, such as number of hops, support and confidence. The tool also provides support for modern dynamic web pages. We apply the tool to visualize results of data mining study on web log data of Mobile Townpage, a directory service of phone numbers in Japan for i-Mode mobile internet users. The results indicate that our system can easily handle thousands of discovered patterns to discover interesting navigational behavior such as success paths, exit paths and lost paths.
5BBD1273	Knowledge Discovery and Data Mining	mukund deshpande + george karypis	2002	Evaluation of Techniques for Classifying Biological Sequences	 + support vector machine + k nearest neighbor + markov chains + markov model + svm + classification + machine learning + sequences + protein sequence	Classification + Sequences + Markov chains + bio-technology + SVM	"
In recent years we have witnessed an exponential increase in the amount of biological information, either DNA or protein sequences, that has become available in public databases. This has been followed by an increased interest in developing computational techniques to automatically classify these large volumes of sequence data into various categories corresponding to either their role in the chromosomes, their structure, and/or their function. In this paper we evaluate some of the widely-used sequence classification algorithms and develop a framework for modeling sequences in a fashion so that traditional machine learning algorithms, such as support vector machines, can be applied easily. Our detailed experimental evaluation shows that the SVM-based approaches are able to achieve higher classification accuracy compared to the more traditional sequence classification algorithms such as Markov model based techniques and K -nearest neighbor based approaches.
"
099941D7	Knowledge Discovery and Data Mining	abdelghani bellaachia + david b portnoy + yidong chen + abdel g elkahloun	2002	E-CAST: A Data Mining Algorithm for Gene Expression Data	data mining + graph theory + gene expression + data clustering + hierarchical clustering	Clustering + Data mining + Bio-informatics + Gene expression + Graph theory + Micro-array	"
Data clustering methods have been proven to be a successful data mining technique in the analysis of gene expression data. The Cluster affinity search technique (CAST) developed by Ben-Dor, et. al., 1999, which has been shown to cluster gene expression data well, has two drawbacks. First, the algorithm uses a fixed initial threshold value to start the clustering. As stated in the original paper, this parameter directly affects the size and number of clusters produced. Second, the algorithm requires a final cleaning step, which takes O(n2), to relocate n data points among the existing clusters. In this paper, we have developed and enhanced CAST algorithm, called E-CAST, that uses a dynamic threshold. The threshold value is computed at the beginning of each new cluster. We have implemented both CAST and E-CAST algorithms and tested their performance using three different data sets. The datasets are real gene expression data from melanoma, pheochromocytoma and brain cell tissue samples generated using micro-arrays technology. The results of both implementations were compared to the output from the hierarchical clustering program, written by Michael Eisen, with very comparable results. Not only did the final results compare favorably with the hierarchical approach, but they also indicate that the cleaning step of the original CAST algorithm may be unnecessary.
"
7724C991	Knowledge Discovery and Data Mining	c lee giles + secil ugurel + robert krovetz	2002	What's the code?: automatic classification of source code archives	 + Probabilistic computation + world wide web + Software notations and tools + Information systems + evaluation + Theory of computation + Functional languages + programming language + Digital libraries and archives + Logic + Software and its engineering + Automated reasoning + source code + Language types + Information systems applications + Computers in other domains + Information retrieval + machine learning + Document collection models + Models of computation + Document representation + Applied computing + support vector machine + General programming languages + clustering	No keyword found	There are various source code archives on the World Wide Web. These archives are usually organized by application categories and programming languages. However, manually organizing source code repositories is not a trivial task since they grow rapidly and are very large (on the order of terabytes). We demonstrate machine learning methods for automatic classification of archived source code into eleven application topics and ten programming languages. For topical classification, we concentrate on C and C++ programs from the Ibiblio and the Sourceforge archives. Support vector machine (SVM) classifiers are trained on examples of a given programming language or programs in a specified category. We show that source code can be accurately and automatically classified into topical categories and can be identified to be in a specific programming language class.
00A60FF6	Knowledge Discovery and Data Mining	jeanfrancois tomb + jason tsongli wang + li liao + sen zhang	2002	Clustering and Classifying Enzymes in Metabolic Pathways: Some Preliminary Results	metabolic pathway + enzyme		"

"
7AE864DF	Knowledge Discovery and Data Mining	christos faloutsos + leejay wu	2002	Making every bit count: fast nonlinear axis scaling	 + Statistical graphics + Symbolic and algebraic manipulation + Computing methodologies + euclidean distance + machine learning + evaluation + Statistical paradigms + Theory of computation + Numerical analysis + degree of freedom + Mathematical analysis + Computation of transforms + Symbolic and algebraic algorithms + probability distribution + clustering + Probability and statistics + Mathematics of computing + Design and analysis of algorithms	No keyword found	Existing axis scaling and dimensionality methods focus on preserving structure, usually determined via the Euclidean distance. In other words, they inherently assume that the Euclidean distance is already correct. We instead propose a novel nonlinear approach driven by an information-theoretic viewpoint, which we show is also strongly linked to intrinsic dimensionality, or degrees of freedom; and uniformity. Nonlinear transformations based on common probability distributions, combined with information-driven selection, simultaneously reduce the number of dimensions required and increase the value of those we retain. Experiments on real data confirm that this approach reveals correlations, finds novel attributes, and scales well.
7B17F6A1	Knowledge Discovery and Data Mining	hanspeter kriegel + matthias schubert + martin ester	2002	Web site mining: a new way to spot competitors, customers and suppliers in the world wide web	 + web pages + web content mining + feature space + world wide web	eol>web content mining + web site mining + web site classification + Markov classifiers	When automatically extracting information from the world wide web, most established methods focus on spotting single HTML-documents. However, the problem of spotting complete web sites is not handled adequately yet, in spite of its importance for various applications. Therefore, this paper discusses the classification of complete web sites. First, we point out the main differences to page classification by discussing a very intuitive approach and its weaknesses. This approach treats a web site as one large HTML-document and applies the well-known methods for page classification. Next, we show how accuracy can be improved by employing a preprocessing step which assigns an occurring web page to its most likely topic. The determined topics now represent the information the web site contains and can be used to classify it more accurately. We accomplish this by following two directions. First, we apply well established classification algorithms to a feature space of occurring topics. The second direction treats a site as a tree of occurring topics and uses a Markov tree model for further classification. To improve the efficiency of this approach, we additionally introduce a powerful pruning method reducing the number of considered web pages. Our experiments show the superiority of the Markov tree approach regarding classification accuracy. In particular, we demonstrate that the use of our pruning method not only reduces the processing time, but also improves the classification accuracy.
7E3B8B8C	Knowledge Discovery and Data Mining	glen jeh + jennifer widom	2002	SimRank: a measure of structural-context similarity	 + Game tree search + Computing methodologies + Information retrieval + Graph theory + Information retrieval query processing + Search methodologies + Information systems + computer science + Discrete mathematics + Graph algorithms + Mathematics of computing + Discrete space search + Artificial intelligence + bayesian networks	No keyword found	"The problem of measuring ""similarity"" of objects arises in many applications, and many domain-specific measures have been developed, e.g., matching text across documents or computing overlap among item-sets. We propose a complementary approach, applicable in any domain with object-to-object relationships, that measures similarity of the structural context in which objects occur, based on their relationships with other objects. Effectively, we compute a measure that says ""two objects are similar if they are related to similar objects:"" This general similarity measure, called SimRank, is based on a simple and intuitive graph-theoretic model. For a given domain, SimRank can be combined with other domain-specific similarity measures. We suggest techniques for efficient computation of SimRank scores, and provide experimental results on two application domains showing the computational feasibility and effectiveness of our approach."
78FEE5DA	Knowledge Discovery and Data Mining	kazumi saito + naonori ueda	2002	Single-shot detection of multiple categories of text using parametric mixture models	 + parameter space + Symbolic and algebraic manipulation + world wide web + Probabilistic reasoning algorithms + mixture model + evaluation + Theory of computation + Mathematical analysis + Sequential Monte Carlo methods + Natural language processing + Probability and statistics + Mathematics of computing + binary classification + Markov-chain Monte Carlo methods + Computations in finite fields + Probabilistic algorithms + multinomial distribution + Computing methodologies + Learning settings + machine learning + Language resources + Numerical analysis + Symbolic and algebraic algorithms + Machine learning + clustering + Design and analysis of algorithms + Artificial intelligence	No keyword found	"In this paper, we address the problem of detecting multiple topics or categories of text where each text is not assumed to belong to one of a number of mutually exclusive categories. Conventionally, the binary classification approach has been employed, in which whether or not text belongs to a category is judged by the binary classifier for every category. In this paper, we propose a more sophisticated approach to simultaneously detect multiple categories of text using parametric mixture models (PMMs), newly presented in this paper. PMMs are probabilistic generative models for text that has multiple categories. Our PMMs are essentially different from the conventional mixture of multinomial distributions in the sense that in the former several basis multinomial parameters are mixed in the parameter space, while in the latter several multinomial components are mixed. We derive efficient learning algorithms for PMMs within the framework of the maximum a posteriori estimate. We also empirically show that our method can outperform the conventional binary approach when applied to multitopic detection of World Wide Web pages, focusing on those from the ""yahoo.com"" domain."
7C188101	Knowledge Discovery and Data Mining	xiaowei xu + florian w beil + martin ester	2002	Frequent term-based text clustering	 + Cluster analysis + text clustering + Retrieval tasks and goals + Learning paradigms + Information systems applications + Computing methodologies + Clustering and classification + Information retrieval + Clustering + Data mining + Unsupervised learning + association rule mining + Information systems + Database management system engines + Machine learning + Data management systems + hierarchical clustering + clustering	eol>Clustering + Frequent Item Sets + Text Documents	Text clustering methods can be used to structure large sets of text or hypertext documents. The well-known methods of text clustering, however, do not really address the special problems of text clustering: very high dimensionality of the data, very large size of the databases and understandability of the cluster description. In this paper, we introduce a novel approach which uses frequent item (term) sets for text clustering. Such frequent sets can be efficiently discovered using algorithms for association rule mining. To cluster based on frequent term sets, we measure the mutual overlap of frequent sets with respect to the sets of supporting documents. We present two algorithms for frequent term-based text clustering, FTC which creates flat clusterings and HFTC for hierarchical clustering. An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the- art text clustering algorithms. Furthermore, our methods provide an understandable description of the discovered clusters by their frequent term sets.
7B3C8F37	Knowledge Discovery and Data Mining	anuradha bhamidipaty + sunita sarawagi	2002	Interactive deduplication using active learning	 + Cluster analysis + Hypertext / hypermedia + Learning paradigms + Human-centered computing + Learning settings + Information systems applications + Computing methodologies + Human computer interaction (HCI) + Data mining + Unsupervised learning + Information systems + Applied computing + active learning + web content mining + Multi / mixed media creation + Document management and text processing + Machine learning + Interaction paradigms + Document preparation	No keyword found	Deduplication is a key operation in integrating data from multiple sources. The main challenge in this task is designing a function that can resolve when a pair of records refer to the same entity in spite of various data inconsistencies. Most existing systems use hand-coded functions. One way to overcome the tedium of hand-coding is to train a classifier to distinguish between duplicates and non-duplicates. The success of this method critically hinges on being able to provide a covering and challenging set of training pairs that bring out the subtlety of deduplication function. This is non-trivial because it requires manually searching for various data inconsistencies between any two records spread apart in large lists.We present our design of a learning-based deduplication system that uses a novel method of interactively discovering challenging training pairs using active learning. Our experiments on real-life datasets show that active learning significantly reduces the number of instances needed to achieve high accuracy. We investigate various design issues that arise in building a system to provide interactive response, fast convergence, and interpretable output.
76E9551E	Knowledge Discovery and Data Mining	bill yuanchi chiu + stefano lonardi + eamonn keogh	2002	Finding surprising patterns in a time series database in linear time and space	 + feature extraction + markov model + Information systems applications + time series + anomaly detection + Data mining + linear time + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applica- t ions-Data Mining Keywords Time series+Suffix Tree+Novelty Detection+Anomaly Detection+Markov Model+Feature Extraction	"The problem of finding a specified pattern in a time series database (i.e. query by content) has received much attention and is now a relatively mature field. In contrast, the important problem of enumerating all surprising or interesting patterns has received far less attention. This problem requires a meaningful definition of ""surprise"", and an efficient search technique. All previous attempts at finding surprising patterns in time series use a very limited notion of surprise, and/or do not scale to massive datasets. To overcome these limitations we introduce a novel technique that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance, given some previously seen data."
752AACEC	Knowledge Discovery and Data Mining	walker white + cristian bucila + johannes gehrke + daniel kifer	2002	DualMiner: a dual-pruning algorithm for itemsets with constraints	contingency tables +  + Approximation + Information systems applications + Symbolic and algebraic manipulation + Computing methodologies + Functional analysis + Data mining + Information systems + Theory of computation + Algebraic algorithms + Mathematical analysis + Symbolic and algebraic algorithms + Approximation algorithms analysis + search space + Mathematics of computing + Mathematical optimization + Design and analysis of algorithms	No keyword found	"Constraint-based mining of itemsets for questions such as ""find all frequent itemsets where the total price is at least $50"" has received much attention recently. Two classes of constraints, monotone and antimonotone, have been identified as very useful. There are algorithms that efficiently take advantage of either one of these two classes, but no previous algorithms can efficiently handle both types of constraints simultaneously. In this paper, we present the first algorithm (called DualMiner) that uses both monotone and antimonotone constraints to prune its search space. We complement a theoretical analysis and proof of correctness of DualMiner with an experimental study that shows the efficacy of DualMiner compared to previous work."
7B8BCAAC	Knowledge Discovery and Data Mining	edwin p d pednault + naoki abe + bianca zadrozny	2002	Sequential cost-sensitive decision making with reinforcement learning	 + algorithms + web content mining + data mining + Machine learning + Computing methodologies + decision rule + reinforcement learning	No keyword found	Recently, there has been increasing interest in the issues of cost-sensitive learning and decision making in a variety of applications of data mining. A number of approaches have been developed that are effective at optimizing cost-sensitive decisions when each decision is considered in isolation. However, the issue of sequential decision making, with the goal of maximizing total benefits accrued over a period of time instead of immediate benefits, has rarely been addressed. In the present paper, we propose a novel approach to sequential decision making based on the reinforcement learning framework. Our approach attempts to learn decision rules that optimize a sequence of cost-sensitive decisions so as to maximize the total benefits accrued over time. We use the domain of targeted' marketing as a testbed for empirical evaluation of the proposed method. We conducted experiments using approximately two years of monthly promotion data derived from the well-known KDD Cup 1998 donation data set. The experimental results show that the proposed method for optimizing total accrued benefits out performs the usual targeted-marketing methodology of optimizing each promotion in isolation. We also analyze the behavior of the targeting rules that were obtained and discuss their appropriateness to the application domain.
0749DAB4	Knowledge Discovery and Data Mining	aidong zhang + li zhang + murali ramanathan	2002	Visualized Classification of Multiple Sample Types	gene expression + cluster analysis + parallel coordinates + dna microarray		"
The goal of the knowledge discovery and data mining is to extract the useful knowledge from the given data. Visualization enables us to find structures, features, patterns, and relationships in a dataset by presenting the data in various graphical forms with possible interactions. Recently, DNA microarray technology provides a board snapshot of the state of the cell by measuring the expression levels of thousands of genes simultaneously. Such information can thus be used to analyze different samples by the gene expression profiles. Last few years saw many cluster analysis and classsification methods extensively be applied to capture the similarity pattern of gene expressions. A novel interactive visualization approach, VizCluster, was presented and applied to classify samples of two types. It combines the merits of both high dimensional projection scatter plot and parallel coordinate plot, taking advantage of graphical visualization methods to reveal the underlining data patterns. In this paper, we expand VizCluster to classify multiple types of samples. First, we identify genes which are differentially expressed across the sample groups. Then we apply VizCluster to build classifiers based on those genes. Finally, classifiers were evaluated by either hold out or cross validation. Five gene expression data sets were used to illustrate the approach. Experimental performance demonstrated the feasibility and usefulness of this approach.
"
7C8D23EB	Knowledge Discovery and Data Mining	geoff hulten + pedro domingos	2002	Mining complex models from arbitrarily large databases in constant time	 + bayesian network + Learning paradigms + Information systems applications + Computing methodologies + Data mining + Information systems + Logical and relational learning + Inductive logic learning + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + Supervised learning by classification + bayesian networks	+General Terms Scalable learning algorithms+ubsampling+Hoeffding bounds+discrete search+Bayesian etworks	"In this paper we propose a scaling-up method that is applicable to essentially any induction algorithm based on discrete search. The result of applying the method to an algorithm is that its running time becomes independent of the size of the database, while the decisions made are essentially identical to those that would be made given infinite data. The method works within pre-specified memory limits and, as long as the data is iid, only requires accessing it sequentially. It gives anytime results, and can be used to produce batch, stream, time-changing and active-learning versions of an algorithm. We apply the method to learning Bayesian networks, developing an algorithm that is faster than previous ones by orders of magnitude, while achieving essentially the same predictive performance. We observe these gains on a series of large databases ""generated from benchmark networks, on the KDD Cup 2000 e-commerce data, and on a Web log containing 100 million requests."
785B8729	Knowledge Discovery and Data Mining	janming ho + shianhua lin	2002	Discovering informative content blocks from Web documents	 + Cluster analysis + Retrieval models and ranking + web pages + indexation + information retrieval + Learning paradigms + Computing methodologies + Information retrieval + Unsupervised learning + Information systems + Document representation + information extraction + Search engine architectures and scalability + entropy + information content + Search engine indexing + Machine learning	eol>Informative content discovery + Entropy + Information retrieval + Information extraction	In this paper, we propose a new approach to discover informative contents from a set of tabular documents (or Web pages) of a Web site. Our system, InfoDiscoverer, first partitions a page into several content blocks according to HTML tag <TABLE> in a Web page. Based on the occurrence of the features (terms) in the set of pages, it calculates entropy value of each feature. According to the entropy value of each feature in a content block, the entropy value of the block is defined. By analyzing the information measure, we propose a method to dynamically select the entropy-threshold that partitions blocks into either informative or redundant. Informative content blocks are distinguished parts of the page, whereas redundant content blocks are common parts. Based on the answer set generated from 13 manually tagged news Web sites with a total of 26,518 Web pages, experiments show that both recall and precision rates are greater than 0.956. That is, using the approach, informative blocks (news articles) of these sites can be automatically separated from semantically redundant contents such as advertisements, banners, navigation panels, news categories, etc. By adopting InfoDiscoverer as the preprocessor of information retrieval and extraction applications, the retrieval and extracting precision will be increased, and the indexing size and extracting complexity will also be reduced.
7FDFD684	Knowledge Discovery and Data Mining	marialuiza antonie + osmar r zaiane + alexandru coman	2002	Mammography Classification by an Association Rule-based Classifier	association rule mining + data mining + data cleaning + association rule + image classification + association rules	eol>Mammography Mining + Image Classification + Document Categorization + Association Rules + Medical Images	"
This paper proposes a new classification method based on association rule mining. This association rule-based classifier is experimented on a real dataset; a database of medical images. The system we propose consists of: a preprocessing phase, a phase for mining the resulted transactional database, and a final phase to organize the resulted association rules in a classification model. The experimental results show that the method performs well reaching over 80% in accuracy. Moreover, this paper illustrates, by comparison to other published research, how important the data cleaning phase is in building an accurate data mining architecture for image classification.
"
7A3B64F4	Knowledge Discovery and Data Mining	dekang lin + patrick pantel	2002	Discovering word senses from text	 + Retrieval tasks and goals + Information systems applications + Clustering and classification + Information retrieval + machine learning + Clustering + Data mining + feature vector + Information systems + evaluation + clustering	eol>Word sense discovery + clustering + evaluation + machine learning	Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses.
13E8C9D2	Knowledge Discovery and Data Mining	shuching chen + xin huang + chengcui zhang + meiling shyu	2002	User Concept Pattern Discovery Using Relevance Feedback And Multiple Instance Learning For Content-Based Image Retrieval	image retrieval		"
Understanding and learning the subjective aspect of humans in Content-Based Image Retrieval has been an active research field during the past few years. However, how to effectively discover users' concept patterns when there are multiple visual features existing in the retrieval system still remains a big issue. In this paper, we propose a multimedia data mining framework that incorporates Multiple Instance Learning into the user relevance feedback in a seamless way to discover the concept patterns of users, especially where the user's most interested region and how to map the local feature vector of that region to the high-level concept pattern of users. This underlying mapping can be progressively discovered through the feedback and learning procedure. The role user plays in the retrieval system is to guide the system mining process to his/her own focus of attention. The retrieval performance is tested under a couple of conditions.
"
80F176EE	Knowledge Discovery and Data Mining	ghaleb m abdulla + terence critchlow + tina eliassirad	2002	Statistical modeling of large-scale simulation data	mining + simulation + range query + scientific data + management + accuracy + shear + data storage + topology + exploration + distribution + statistical model + statistical modeling + statistical models + goodness of fit + root mean square error	eol>statistical modeling + large-scale scientific data sets + approximate ad-hoc queries	"
With the advent of fast computer systems, scientists are now able to generate terabytes of simulation data. Unfortunately, the sheer size of these data sets has made efficient exploration of them impossible. To aid scientists in gleaning insight from their simulation data, we have developed an ad-hoc query infrastructure. Our system, called AQSim (short for Ad-hoc Queries for Simulation) reduces the data storage requirements and query access times in two stages. First, it creates and stores mathematical and statistical models of the data at multiple resolutions. Second, it evaluates queries on the models of the data instead of on the entire data set. In this paper, we present two simple but effective statistical modeling techniques for simulation data. Our first modeling technique computes the âtrueâ (unbiased) mean of systematic partitions of the data. It makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model. Our second statistical modeling technique uses the Andersen-Darling goodness-of-fit method on systematic partitions of the data. This method evaluates a model by how well it passes the normality test on the data. Both of our statistical models effectively answer range queries. At each resolution of the data, we compute the precision of our answer to the user's query by scaling the onesided Chebyshev Inequalities with the original mesh's topology. We combine precisions at different resolutions by calculating their weighted average. Our experimental evaluations on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on multiple resolutions of large simulation data sets.
"
00C81A6B	Knowledge Discovery and Data Mining	xiaolan shen + christopher bystroff + jingjing hu + yu shao + mohammed j zaki	2002	Mining Protein Contact Maps	secondary structure +  + data mining + protein folding + protein sequence		"
The 3D conformation of a protein may be compactly represented in a symmetrical, square, boolean matrix of pairwise, inter-residue contacts, or âcontact mapâ. The contact map provides a host of useful information about the protein's structure. In this paper we describe how data mining can be used to extract valuable information from contact maps. For example, clusters of contacts represent certain secondary structures, and also capture non-local interactions, giving clues to the tertiary structure. In this paper we focus on two main tasks: 1) Given the database of protein sequences, discover an extensive set of non-local (frequent) dense patterns in their contact maps, and compile a library of such non-local interactions. 2) Cluster these patterns based on their similarities and evaluate the clustering quality. We show via experiments that our techniques are effective in characterizing contact patterns across different proteins, and can be used to improve contact map prediction for unknown proteins as well as to learn protein folding pathways.
"
7905C524	Knowledge Discovery and Data Mining	pat hanrahan + chris stolte + diane tang	2002	Query, analysis, and visualization of hierarchically structured data using Polaris	 + data warehouse + data mining + Symbolic and algebraic manipulation + Data mining + Database management system engines + Information systems + Database query processing + Theory and algorithms for application domains + Theory of computation + Algebraic algorithms + Database query processing and optimization (theory) + Data management systems + multidimensional database + interactive visualization + Database theory + Graphical user interfaces + scientific computing + Parallel and distributed DBMSs + Information systems applications + Computing methodologies + Human-centered computing + time series + Human computer interaction (HCI) + user interface + Symbolic and algebraic algorithms + data cube + Interaction paradigms	No keyword found	In the last several years, large OLAP databases have become common in a variety of applications such as corporate data warehouses and scientific computing. To support interactive analysis, many of these databases are augmented with hierarchical structures that provide meaningful levels of abstraction that can be leveraged by both the computer and analyst. This hierarchical structure generates many challenges and opportunities in the design of systems for the query, analysis, and visualization of these databases.In this paper, we present an interactive visual exploration tool that facilitates exploratory analysis of data warehouses with rich hierarchical structure, such as might be stored in data cubes. We base this tool on Polaris, a system for rapidly constructing table-based graphical displays of multidimensional databases. Polaris builds visualizations using an algebraic formalism derived from the interface and interpreted as a set of queries to a database. We extend the user interface, algebraic formalism, and generation of data queries in Polaris to expose and take advantage of hierarchical structure. In the resulting system, analysts can navigate through the hierarchical projections of a database, rapidly and incrementally generating visualizations for each projection.
77FB0FB8	Knowledge Discovery and Data Mining	charles elkan + bianca zadrozny	2002	Transforming classifier scores into accurate multiclass probability estimates	Probabilistic computation +  + domain knowledge + Probabilistic algorithms + data mining + direct marketing + Symbolic and algebraic manipulation + Information systems applications + Computing methodologies + Probabilistic reasoning algorithms + Data mining + Information systems + Models of computation + Theory of computation + support vector machine + naive bayes + Symbolic and algebraic algorithms + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + Design and analysis of algorithms + feature selection + Markov-chain Monte Carlo methods	No keyword found	Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making, such as example-dependent misclassification costs, the outputs of other classifiers, or domain knowledge. Previous calibration methods apply only to two-class problems. Here, we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates. We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples. Using naive Bayes and support vector machine classifiers, we give experimental results from a variety of two-class and multiclass domains, including direct marketing, text categorization and digit recognition.
7698A46E	Knowledge Discovery and Data Mining	david madigan + greg ridgeway	2002	Bayesian analysis of massive datasets via particle filters	bayesian analysis +  + Probabilistic algorithms + markov chain monte carlo + data mining + Information systems applications + Computing methodologies + Probabilistic reasoning algorithms + importance sampling + proof of concept + particle filter + dynamic system + Data mining + Information systems + data access + posterior distribution + Machine learning + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + Markov-chain Monte Carlo methods	No keyword found	"Markov Chain Monte Carlo (MCMC) techniques revolutionized statistical practice in the 1990s by providing an essential toolkit for making the rigor and flexibility of Bayesian analysis computationally practical. At the same time the increasing prevalence of massive datasets and the expansion of the field of data mining has created the need to produce statistically sound methods that scale to these large problems. Except for the most trivial examples, current MCMC methods require a complete scan of the dataset for each iteration eliminating their candidacy as feasible data mining techniques.In this article we present a method for making Bayesian analysis of massive datasets computationally feasible. The algorithm simulates from a posterior distribution that conditions on a smaller, more manageable portion of the dataset. The remainder of the dataset may be incorporated by reweighting the initial draws using importance sampling. Computation of the importance weights requires a single scan of the remaining observations. While importance sampling increases efficiency in data access, it comes at the expense of estimation efficiency. A simple modification, based on the ""rejuvenation"" step used in particle filters for dynamic systems models, sidesteps the loss of efficiency with only a slight increase in the number of data accesses.To show proof-of-concept, we demonstrate the method on a mixture of transition models that has been used to model web traffic and robotics. For this example we show that estimation efficiency is not affected while offering a 95% reduction in data accesses."
79A15317	Knowledge Discovery and Data Mining	phillip b gibbons + christos faloutsos + christopher r palmer	2002	ANF: a fast and scalable tool for data mining in massive graphs	 + social networks + data mining + Game tree search + direct marketing + Information systems applications + Computing methodologies + Graph theory + Data mining + linear models + Information systems + Search methodologies + social network + viral marketing + Discrete mathematics + Graph algorithms + Mathematics of computing + Discrete space search + Artificial intelligence	No keyword found	"Graphs are an increasingly important data source, with such important graphs as the Internet and the Web. Other familiar graphs include CAD circuits, phone records, gene sequences, city streets, social networks and academic citations. Any kind of relationship, such as actors appearing in movies, can be represented as a graph. This work presents a data mining tool, called ANF, that can quickly answer a number of interesting questions on graph-represented data, such as the following. How robust is the Internet to failures? What are the most influential database papers? Are there gender differences in movie appearance patterns? At its core, ANF is based on a fast and memory-efficient approach for approximating the complete ""neighbourhood function"" for a graph. For the Internet graph (268K nodes), ANF's highly-accurate approximation is more than 700 times faster than the exact computation. This reduces the running time from nearly a day to a matter of a minute or two, allowing users to perform ad hoc drill-down tasks and to repeatedly answer questions about changing data sources. To enable this drill-down, ANF employs new techniques for approximating neighbourhood-type functions for graphs with distinguished nodes and/or edges. When compared to the best existing approximation, ANF's approach is both faster and more accurate, given the same resources. Additionally, unlike previous approaches, ANF scales gracefully to handle disk resident graphs. Finally, we present some of our results from mining large graphs using ANF."
7B2B6A78	Knowledge Discovery and Data Mining	alin dobra + johannes gehrke	2002	SECRET: a scalable linear regression tree algorithm	 + Cluster analysis + regression tree + data mining + Game tree search + Learning paradigms + linear regression + Symbolic and algebraic manipulation + classification tree + Theory of computation + regression model + satisfiability + Discrete mathematics + linear model + text mining + Mathematics of computing + Discrete space search + Trees + em algorithm + Computing methodologies + Graph theory + Unsupervised learning + Search methodologies + Symbolic and algebraic algorithms + Machine learning + clustering + Artificial intelligence + Design and analysis of algorithms	+Even though regression trees were introduced in the CART book early in the development of decision trees by Breiman	Developing regression models for large datasets that are both accurate and easy to interpret is a very important data mining problem. Regression trees with linear models in the leaves satisfy both these requirements, but thus far, no truly scalable regression tree algorithm is known. This paper proposes a novel regression tree construction algorithm (SECRET) that produces trees of high quality and scales to very large datasets. At every node, SECRET uses the EM algorithm for Gaussian mixtures to find two clusters in the data and to locally transform the regression problem into a classification problem based on closeness to these clusters. Goodness of split measures, like the gini gain, can then be used to determine the split variable and the split point much like in classification tree construction. Scalability of the algorithm can be achieved by employing scalable versions of the EM and classification tree construction algorithms. An experimental evaluation on real and artificial data shows that SECRET has accuracy comparable to other linear regression tree algorithms but takes orders of magnitude less computation time for large datasets.
7B4906A8	Knowledge Discovery and Data Mining	jay ayres + jason flannick + johannes gehrke + tomi yiu	2002	Sequential PAttern mining using a bitmap representation	 + Information systems applications + Computing methodologies + Data mining + Information systems + Theory of computation + Data structures design and analysis + Computer graphics + Machine learning + sequential pattern mining + Image manipulation + search space + Design and analysis of algorithms + gene expression + Pattern matching + depth first search	No keyword found	We introduce a new algorithm for mining sequential patterns. Our algorithm is especially efficient when the sequential patterns in the database are very long. We introduce a novel depth-first search strategy that integrates a depth-first traversal of the search space with effective pruning mechanisms.Our implementation of the search strategy combines a vertical bitmap representation of the database with efficient support counting. A salient feature of our algorithm is that it incrementally outputs new frequent itemsets in an online fashion.In a thorough experimental evaluation of our algorithm on standard benchmark data from the literature, our algorithm outperforms previous work up to an order of magnitude.
76F0F507	Knowledge Discovery and Data Mining	xudong guan + yiling yang + jinyuan you	2002	CLOPE: a fast and effective clustering algorithm for transactional data	Cluster analysis +  + Retrieval tasks and goals + data mining + Learning paradigms + Computing methodologies + Information systems applications + Clustering and classification + Information retrieval + Clustering + Data mining + scalability + Unsupervised learning + Information systems + transaction data + Machine learning + categorical data + clustering	data mining + clustering + categorical data + scalability	This paper studies the problem of categorical data clustering, especially for transactional data characterized by high dimensionality and large volume. Starting from a heuristic method of increasing the height-to-width ratio of the cluster histogram, we develop a novel algorithm -- CLOPE, which is very fast and scalable, while being quite effective. We demonstrate the performance of our algorithm on two real world datasets, and compare CLOPE with the state-of-art algorithms.
7AEBDAB7	Knowledge Discovery and Data Mining	bin chen + peter j haas + peter scheuermann	2002	A new two-phase sampling based algorithm for discovering association rules	 + Cluster analysis + association rule + Learning paradigms + Symbolic and algebraic manipulation + random sampling + Database management system engines + Information systems + Theory of computation + Mathematical analysis + count data + Data management systems + Approximation algorithms analysis + Mathematics of computing + Triggers and rules + Approximation + Computing methodologies + empirical study + Functional analysis + Unsupervised learning + Numerical analysis + Computation of transforms + Symbolic and algebraic algorithms + Machine learning + sampling technique + clustering + Design and analysis of algorithms	No keyword found	"This paper introduces FAST, a novel two-phase sampling-based algorithm for discovering association rules in large databases. In Phase I a large initial sample of transactions is collected and used to quickly and accurately estimate the support of each individual item in the database. In Phase II these estimated supports are used to either trim ""outlier"" transactions or select ""representative"" transactions from the initial sample, thereby forming a small final sample that more accurately reflects the statistical characteristics (i.e., itemset supports) of the entire database. The expensive operation of discovering association rules is then performed on the final sample. In an empirical study, FAST was able to achieve 90--95% accuracy using a final sample having a size of only 15--33% of that of a comparable random sample. This efficiency gain resulted in a speedup by roughly a factor of 10 over previous algorithms that require expensive processing of the entire database --- even efficient algorithms that exploit sampling. Our new sampling technique can be used in conjunction with almost any standard association-rule algorithm, and can potentially render scalable other algorithms that mine ""count"" data."
7EE1BD49	Knowledge Discovery and Data Mining	junqiang liu + jiawei han + yunhe pan + ke wang	2002	Mining frequent item sets by opportunistic projection	 + association rule + association rules + Information systems applications + very large database + Data mining + breadth first search + depth first search + Information systems	eol>Association Rules + Frequent Patterns	In this paper, we present a novel algorithm Opportune Project for mining complete set of frequent item sets by projecting databases to grow a frequent item set tree. Our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structures, array-based or tree-based, to represent projected transaction subsets, and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to features of the subsets. More importantly, we propose novel methods to build tree-based pseudo projections and array-based unfiltered projections for projected transaction subsets, which makes our algorithm both CPU time efficient and memory saving. Basically, the algorithm grows the frequent item set tree by depth first search, whereas breadth first search is used to build the upper portion of the tree if necessary. We test our algorithm versus several other algorithms on real world datasets, such as BMS-POS, and on IBM artificial datasets. The empirical results show that our algorithm is not only the most efficient on both sparse and dense databases at all levels of support threshold, but also highly scalable to very large databases.
760286A6	Knowledge Discovery and Data Mining	karlton sequeira + mohammed j zaki	2002	ADMIT: anomaly-based data mining for intrusions	 + Cluster analysis + data model + conceptual clustering + false positive rate + data mining + Hypertext / hypermedia + Learning paradigms + intrusion detection + Data mining + system security + Information systems + Theory and algorithms for application domains + Theory of computation + Security and privacy + Multi / mixed media creation + Document management and text processing + Database theory + real time + Retrieval tasks and goals + Information systems applications + Computing methodologies + Human-centered computing + Clustering and classification + Information retrieval + Human computer interaction (HCI) + Clustering + data collection + Unsupervised learning + Database and storage security + Applied computing + intrusion detection system + utility computing + concept drift + Machine learning + Interaction paradigms + Theory of database privacy and security + Document preparation	No keyword found	Security of computer systems is essential to their acceptance and utility. Computer security analysts use intrusion detection systems to assist them in maintaining computer system security. This paper deals with the problem of differentiating between masqueraders and the true user of a computer terminal. Prior efficient solutions are less suited to real time application, often requiring all training data to be labeled, and do not inherently provide an intuitive idea of what the data model means. Our system, called ADMIT, relaxes these constraints, by creating user profiles using semi-incremental techniques. It is a real-time intrusion detection system with host-based data collection and processing. Our method also suggests ideas for dealing with concept drift and affords a detection rate as high as 80.3% and a false positive rate as low as 15.3%.
77C498E3	Knowledge Discovery and Data Mining	charu c aggarwal	2002	Collaborative crawling: mining user experiences for topical resource discovery	 + Probabilistic computation + Retrieval models and ranking + web pages + Sorting and searching + Information systems applications + world wide web + Information retrieval + user experience + Data mining + Information systems + Models of computation + Theory of computation + Information storage systems + satisfiability + public domain + Data structures design and analysis + Design and analysis of algorithms + gene expression	No keyword found	The rapid growth of the world wide web had made the problem of topic specific resource discovery an important one in recent years. In this problem, it is desired to find web pages which satisfy a predicate specified by the user. Such a predicate could be a keyword query, a topical query, or some arbitrary contraint. Several techniques such as focussed crawling and intelligent crawling have recently been proposed for topic specific resource discovery. All these crawlers are linkage based, since they use the hyperlink behavior in order to perform resource discovery. Recent studies have shown that the topical correlations in hyperlinks are quite noisy and may not always show the consistency necessary for a reliable resource discovery process. In this paper, we will approach the problem of resource discovery from an entirely different perspective; we will mine the significant browsing patterns of world wide web users in order to model the likelihood of web pages belonging to a specified predicate. This user behavior can be mined from the freely available traces of large public domain proxies on the world wide web. We refer to this technique as collaborative crawling because it mines the collective user experiences in order to find topical resources. Such a strategy is extremely effective because the topical consistency in world wide web browsing patterns turns out to very reliable. In addition, the user-centered crawling system can be combined with linkage based systems to create an overall system which works more effectively than a system based purely on either user behavior or hyperlinks.
7F6A6745	Knowledge Discovery and Data Mining	randy goebel + yonghe niu + tong zheng	2002	WebFrame: In Pursuit of Computationally and Cognitively Efficient Web Mining	web mining + visualization + data mining + value of information	data mining + web mining + navigation compression + visualization	The goal of web mining is relatively simple: provide both computationally and cognitively efficient methods for improving the value of information to users of the WWW. The need for computational efficiency is well-recognized by the data mining community, which sprung from the database community concern for efficient manipulation of large datasets. The motivation for cognitive efficiency is more elusive but at least as important. In as much as cognitive efficiency can be informally construed as ease of understanding, then what is important is any tool or technique that presents cognitively manageable abstractions of large datasets. We present our initial development of a framework for gathering, analyzing, and redeploying web data. Not dissimilar to conventional data mining, the general idea is that good use of web data first requires the careful selection of data (both usage and content data), the deployment of appropriate learning methods, and the evaluation of the results of applying the results of learning in a web application. Our framework includes tools for building, using, and visualizing web abstractions. We present an example of the deployment of our framework to navigation improvement. The abstractions we develop are called Navigation Compression Models (NCMs), and we show a method for creating them, using them, and visualizing them to aid in their understanding.
7856695D	Knowledge Discovery and Data Mining	nurit vatnik + yizhak idan + einat neumann + uri eick + saharon rosset	2002	Customer lifetime value modeling and its use for customer retention planning	 + Statistical graphics + Information systems applications + customer retention + Data mining + Information systems + Statistical paradigms + Numerical analysis + Mathematical analysis + Number-theoretic computations + Probability and statistics + Mathematics of computing + lifetime value + customer lifetime value	eol>Lifetime Value + Length of Service + Churn Modeling + Retention Campaign + Incentive Allocation	We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry. We discuss the components of this problem, in particular customer value and length of service (or tenure) modeling, and present a novel segment-based approach, motivated by the segment-level view marketing analysts usually employ. We then describe how we build on this approach to estimate the effects of retention on Lifetime Value. Our solution has been successfully implemented in Amdocs' Business Insight (BI) platform, and we illustrate its usefulness in real-world scenarios.
77B78357	Knowledge Discovery and Data Mining	n r patel + mahesh kumar + jonathan woo	2002	Clustering seasonality patterns in the presence of errors	 + Cluster analysis + Software creation and management + scale invariance + Learning paradigms + gaussian distribution + Data mining + Information systems + Software and its engineering + distance function + Retrieval tasks and goals + forecasting + Information systems applications + Computing methodologies + time series + Clustering and classification + Information retrieval + euclidean distance + Clustering + Software development techniques + Unsupervised learning + measurement error + Machine learning + product life cycle + hierarchical clustering + Error handling and recovery + clustering + seasonality	Presence of Errors	Clustering is a very well studied problem that attempts to group similar data points. Most traditional clustering algorithms assume that the data is provided without measurement error. Often, however, real world data sets have such errors and one can obtain estimates of these errors. We present a clustering method that incorporates information contained in these error estimates. We present a new distance function that is based on the distribution of errors in data. Using a Gaussian model for errors, the distance function follows a Chi-Square distribution and is easy to compute. This distance function is used in hierarchical clustering to discover meaningful clusters. The distance function is scale-invariant so that clustering results are independent of units of measuring data. In the special case when the error distribution is the same for each attribute of data points, the rank order of pair-wise distances is the same for our distance function and the Euclidean distance function. The clustering method is applied to the seasonality estimation problem and experimental results are presented for the retail industry data as well as for simulated data, where it outperforms classical clustering methods.
5AD240C0	Knowledge Discovery and Data Mining	william w cohen + jacob richman + whizbang labs	2002	Learning to match and cluster entity names	record linkage		"
Information retrieval is, in large part, the study of methods for assessing the similarity of pairs of documents. Document similarity metrics have been used for many tasks including ad hoc document retrieval, text classification [YC1994], and summarization [GC1998,SSMB1997]. Another problem area in which similarity metrics are central is record linkage (e.g., [KA1985]), where one wishes to determine if two database records taken from different source databases refer to the same entity. For instance, one might wish to determine if two database records from two different hospitals, each containing a patient's name, address, and insurance information, refer to the same person; as another example, one might wish to determine if two bibliography records, each containing a paper title, list of authors, and journal name, refer to the same publication. In both of these examples (and in many other practical cases) most of the record fields contain text---person names, paper titles and so on. It is natural to ask if document similarity metrics developed in the IR community can be useful for matching and clustering entity names. Recently there has been a substantial amount of work in addressing this question. Somewhat surprisingly, traditional IR document similarity metrics have experimentally been useful in comparing the very short âdocumentsâ that are used as informal names for entities. TF-IDF similarity metrics have been shown to be competitive with or superior to hand-coded similarity metrics and string editdistance based metrics in a variety of domains, ranging from bibliographic references for scientific papers [LGB1999] to names of animal species and popular movies [C2000]. Even when TF-IDF similarity is not the most accurate similarity metric, it is often the fastest reasonable metric, and can be usefully applied in combination with other, more computationally expensive similarity tests [MNRS2000]. In this paper, we propose and describe several techniques for adaptively modifying document similarity metrics. We will evaluate these techniques but on entity-matching and entity-clustering tasks---in particular, we evaluate using learned similarity metrics in combination with simple TFIDF based distance schemes. Our hope is that learned distance metrics can improve on generalpurpose, task-independent distance metrics like TF-IDF, and ultimately provide performance comparable to engineered, domain-specific distance metrics and/or entity matching schemes (e.g. [HS1998, GFS2000]). Although our evaluation is solely in the context of entity-name matching and clustering, we conjecture that similar learning methods will be useful in other tasks involving document similarity metrics, such as document retrieval, summarization, classification, and clustering. Below we more precisely define the problem, present an algorithm, present our experimental results. We then discuss related work and present some concluding remarks.
"
762DE30B	Knowledge Discovery and Data Mining	soumen chakrabarti + sunita sarawagi + shantanu godbole	2002	Scaling multi-class support vector machines using inter-class confusion	confusion matrix +  + Statistical graphics + discrimination learning + gaussian mixture models + Graph theory + Statistical paradigms + support vector machine + Numerical analysis + Mathematical analysis + Computations on matrices + Discrete mathematics + clustering + Probability and statistics + Graph algorithms + Mathematics of computing	No keyword found	Support vector machines (SVMs) excel at two-class discriminative learning problems. They often outperform generative classifiers, especially those that use inaccurate generative models, such as the naÃ¯ve Bayes (NB) classifier. On the other hand, generative classifiers have no trouble in handling an arbitrary number of classes efficiently, and NB classifiers train much faster than SVMs owing to their extreme simplicity. In contrast, SVMs handle multi-class problems by learning redundant yes/no (one-vs-others) classifiers for each class, further worsening the performance gap. We propose a new technique for multi-way classification which exploits the accuracy of SVMs and the speed of NB classifiers. We first use a NB classifier to quickly compute a confusion matrix, which is used to reduce the number and complexity of the two-class SVMs that are built in the second stage. During testing, we first get the prediction of a NB classifier and use that to selectively apply only a subset of the two-class SVMs. On standard benchmarks, our algorithm is 3 to 6 times faster than SVMs and yet matches or even exceeds their accuracy.
58ABADB8	Knowledge Discovery and Data Mining	myra spiliopoulou + bettina berendt + miki nakagawa + bamshad mobasher	2002	The Impact of Site Structure and User Environment on Session Reconstruction in Web Usage Analysis	 + web personalization + web usage mining + data collection	Data preparation + sessionization heuristics + Web usage mining	"
The analysis of user behavior on the Web presupposes a reliable reconstruction of the users' navigational activities. Cookies and server-generated session identifiers have been designed to allow a faithful session reconstruction. However, in the absence of reliable methods, analysts must rely on heuristics methods (a) to identify unique visitors to a site, and (b) to distinguish among the activities of such users during independent sessions. The characteristics of the site, such as the site structure, as well as the methods used for data collection (e.g., the existence of cookies and reliable synchronization across multiple servers) may necessitate the use of different types of heuristics. In this study, we extend our work on the reliability of sessionizing mechanisms, by investigating the impact of site structure on the quality of constructed sessions. Specifically, we juxtapose sessionizing on a frame-based and a frame-free version of a site. We investigate the behavior of cookies, server-generated session identification, and heuristics that exploit session duration, page stay time and page linkage. Different measures of session reconstruction quality, as well as experiments on the impact on the prediction of frequent entry and exit pages, show that different reconstruction heuristics can be recommended depending on the characteristics of the site. We also present first results on the impact of session reconstruction heuristics on predictive applications such as Web personalization.
"
7B74256D	Knowledge Discovery and Data Mining	johannes gehrke + reba schuller + shai bendavid	2002	A theoretical framework for learning from a pool of disparate data sources	 + Probabilistic algorithms + Middleware for databases + Information integration + Object-relational mapping facilities + Symbolic and algebraic manipulation + Computing methodologies + computational learning theory + Probabilistic reasoning algorithms + mathematical analysis + machine learning + Information systems + relational data + Theory of computation + data gathering + Symbolic and algebraic algorithms + Data management systems + clustering + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + Design and analysis of algorithms + Markov-chain Monte Carlo methods	No keyword found	Many enterprises incorporate information gathered from a variety of data sources into an integrated input for some learning task. For example, aiming towards the design of an automated diagnostic tool for some disease, one may wish to integrate data gathered in many different hospitals. A major obstacle to such endeavors is that different data sources may vary considerably in the way they choose to represent related data. In practice, the problem is usually solved by a manual construction of semantic mappings and translations between the different sources. Recently there have been attempts to introduce automated algorithms based on machine learning tools for the construction of such translations.In this work we propose a theoretical framework for making classification predictions from a collection of different data sources, without creating explicit translations between them. Our framework allows a precise mathematical analysis of the complexity of such tasks, and it provides a tool for the development and comparison of different learning algorithms. Our main objective, at this stage, is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of questions related to this framework.
5B9B1C55	Knowledge Discovery and Data Mining	maleq khan + william perrizo + qin ding	2002	k-nearest Neighbor Classification on Spatial Data Streams Using P-trees	spatial data + nearest neighbor + k nearest neighbor + data mining		"
Classification of spatial data streams is crucial, since the training dataset changes often. Building a new classifier each time can be very costly with most techniques. In this situation, k-nearest neighbor (KNN) classification is a very good choice, since no residual classifier needs to be built ahead of time. KNN is extremely simple to implement and lends itself to a wide variety of variations. We propose a new method of KNN classification for spatial data using a new, rich, data-mining-ready structure, the Peano-count-tree (P-tree). We merely perform some AND/OR operations on P-trees to find the nearest neighbors of a new sample and assign the class label. We have fast and efficient algorithms for the AND/OR operations, which reduce the classification time significantly. Instead of taking exactly the k nearest neighbors we form a closed-KNN set. Our experimental results show closed-KNN yields higher classification accuracy as well as significantly higher speed.
"
7DAEB614	Knowledge Discovery and Data Mining	philip k chan + matthew v mahoney	2002	Learning nonstationary models of normal network traffic for detecting novel attacks	 + conceptual clustering + data mining + Cryptanalysis and other attacks + Computing / technology policy + intrusion detection + Computer crime + Theory and algorithms for application domains + Theory of computation + Database and storage security + intrusion detection system + Security and privacy + Intrusion/anomaly detection and malware mitigation + Cryptography + Social and professional topics + Theory of database privacy and security + Database theory	No keyword found	Traditional intrusion detection systems (IDS) detect attacks by comparing current behavior to signatures of known attacks. One main drawback is the inability of detecting new attacks which do not have known signatures. In this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic. Behavior that deviates from the learned normal model signals possible novel attacks. Our IDS is unique in two respects. First, it is nonstationary, modeling probabilities based on the time since the last event rather than on average rate. This prevents alarm floods. Second, the IDS learns protocol vocabularies (at the data link through application layers) in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software. On the 1999 DARPA IDS evaluation data set [9], we detect 70 of 180 attacks (with 100 false alarms), about evenly divided between user behavioral anomalies (IP addresses and ports, as modeled by most other systems) and protocol anomalies. Because our methods are unconventional there is a significant non-overlap of our IDS with the original DARPA participants, which implies that they could be combined to increase coverage.
7DDCDA1D	Knowledge Discovery and Data Mining	sheila tejada + craig a knoblock + steven minton	2002	Learning domain-independent string transformation weights for high accuracy object identification	 + Retrieval models and ranking + Computer vision + Hierarchical representations + Computer vision representations + Computing methodologies + Information retrieval + Computations on polynomials + Information systems + Numerical analysis + Mathematical analysis + Computer graphics + Shape modeling + Mathematics of computing + lifetime value + Artificial intelligence	No keyword found	The task of object identification occurs when integrating information from multiple websites. The same data objects can exist in inconsistent text formats across sites, making it difficult to identify matching objects using exact text match. Previous methods of object identification have required manual construction of domain-specific string transformations or manual setting of general transformation parameter weights for recognizing format inconsistencies. This manual process can be time consuming and error-prone. We have developed an object identification system called Active Atlas [18], which applies a set of domain-independent string transformations to compare the objects' shared attributes in order to identify matching objects. In this paper, we discuss extensions to the Active Atlas system, which allow it to learn to tailor the weights of a set of general transformations to a specific application domain through limited user input. The experimental results demonstrate that this approach achieves higher accuracy and requires less user involvement than previous methods across various application domains.
761984EC	Knowledge Discovery and Data Mining	jayanth nayak + markus hofmann + srinivasan jagannathan + kevin c almeroth	2002	A model for discovering customer value for E-content	profitability +  + Probabilistic computation + Probabilistic algorithms + Human-centered computing + Probabilistic reasoning algorithms + Human computer interaction (HCI) + internet marketing + probabilistic model + Models of computation + Theory of computation + Web-based interaction + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + Interaction paradigms + real time + customer behavior + Markov-chain Monte Carlo methods + bayesian networks	No keyword found	There exists a huge demand for multimedia goods and services in the Internet. Currently available bandwidth speeds can support sale of downloadable content like CDs, e-books, etc. as well as services like video-on-demand. In the future, such services will be prevalent in the Internet. Since costs are typically fixed, maximizing revenue can maximize profits. A primary determinant of revenue in such e-content markets is how much value the customers associate with the content. Though marketing surveys are useful, they cannot adapt to the dynamic nature of the Internet market. In this work, we examine how to learn customer valuations in close to real-time. Our contributions in this paper are threefold: (1) we develop a probabilistic model to describe customer behavior, (2) we develop a framework for pricing e-content based on basic economic principles, and (3) we propose a price discovering algorithm that learns customer behavior parameters and suggests prices to an e-content provider. We validate our algorithm using simulations. Our simulations indicate that our algorithm generates revenue close to the maximum expectation. Further, they also indicate that the algorithm is robust to transient customer behavior.
5D2BF762	Knowledge Discovery and Data Mining	wenke lee + kui w mok + salvatore j stolfo	2002	Algorithms for Mining System Audit Data	low frequency + intrusion detection + data gathering + feature selection + data mining + association rule		We describe our research in applying data mining techniques to construct intrusion detection models. The key ideas are to mine system audit data for consistent and useful patterns of program and user behavior, and use the set of relevant system features presented in the patterns to compute classifiers that can recognize anomalies and known intrusions. Our past experiments showed that classification rules can be used to detect intrusions, provided that sufficient audit data is available for training and the right set of system features are selected. We use the association rules and frequent episodes computed from audit data as the basis for guiding the audit data gathering and feature selection processes. In order to compute only the relevant patterns, we consider the âorder of importanceâ and âreferenceâ relations among the attributes of data, and modify these two basic algorithms accordingly to use axis attribute(s) and reference attribute(s) as forms of item constraints in the data mining process. We also use an iterative level-wise approximate mining procedure for uncovering the low frequency but important patterns. We report our experiments in using these algorithms on real-world audit data.
7A061E69	Knowledge Discovery and Data Mining	thorsten joachims	2002	Optimizing search engines using clickthrough data	 + Statistical graphics + search engine + Symbolic and algebraic manipulation + Computing methodologies + Information retrieval + meta search engine + Information retrieval query processing + infoviz + Information systems + Statistical paradigms + Theory of computation + support vector machine + Symbolic and algebraic algorithms + text mining + Probability and statistics + Mathematics of computing + Design and analysis of algorithms + information retrieval system	No keyword found	This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.
79C9DEE4	Knowledge Discovery and Data Mining	tubao ho + trongdung nguyen + dungduc nguyen	2002	Visualization support for a user-centered KDD process	 + data preprocessing + Interaction design process and methods + Philosophical/theoretical foundations of artificial intelligence + Rule learning + User centered design + model selection + Human-centered computing + Computing methodologies + Software notations and tools + knowledge discovery + Human computer interaction (HCI) + Knowledge representation and reasoning + Development frameworks and environments + Integrated and visual development environments + Interaction design + Machine learning + Machine learning approaches + Cognitive science + Artificial intelligence + Software and its engineering	No keyword found	Viewing knowledge discovery as a user-centered process that requires an effective collaboration between the user and the discovery system, our work aims to support an active role of the user in that process by developing synergistic visualization tools integrated in our discovery system D2MS. These tools provide an ability of visualizing the entire process of knowledge discovery in order to help the user with data preprocessing, selecting mining algorithms and parameters, evaluating and comparing discovered models, and taking control of the whole discover process. Our case-studies with two medical datasets on meningitis and stomach cancer show that, with visualization tools in D2MS, the user gains better insight in each step of the knowledge discovery process as well the relationship between data and discovered knowledge.
7717D118	Knowledge Discovery and Data Mining	chunyi shi + yuchang lu + xiaoming jin	2002	Similarity measure based on partial information of time series	 + Cluster analysis + Learning paradigms + Machine learning + Information systems applications + Computing methodologies + time series + time series data + Data mining + data collection + Unsupervised learning + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- data mining+1.5.3 [Pattern Recognition+Clustering- similarity measures Keywords Time series+Similarity measure+Partial information	Similarity measure of time series is an important subroutine in many KDD applications. Previous similarity models mainly focus on the prominent series behaviors by considering the whole information of time series. In this paper, we address the problem: which portion of information is more suitable for similarity measure for the data collected from a certain field. We propose a model for the retrieval and representation of the partial information in time series data, and a methodology for evaluating the similarity measurements based on partial information. The methodology is to retrieve various portions of information from the raw data and represent it in a concise form, then cluster the time series using the partial information and evaluate the similarity measurements through comparing the results with a standard classification. Experiments on data set from stock market give some interesting observations and justify the usefulness of our approach.
8054CC13	Knowledge Discovery and Data Mining	werner stuetzle + alejandro murua + jeremy tantrum	2003	Assessment and pruning of hierarchical model based clustering	Cluster analysis +  + feature space + bayesian information criterion + Learning paradigms + Computing methodologies + mixture model + elliptical distribution + Unsupervised learning + mixture of gaussians + unimodality + Machine learning + hierarchical model + Probability and statistics + Mathematics of computing + density estimation	+I.5.3 [Pattern Recognition+Clustering+I.5.1 [Pattern	"The goal of clustering is to identify distinct groups in a dataset. The basic idea of model-based clustering is to approximate the data density by a mixture model, typically a mixture of Gaussians, and to estimate the parameters of the component densities, the mixing fractions, and the number of components from the data. The number of distinct groups in the data is then taken to be the number of mixture components, and the observations are partitioned into clusters (estimates of the groups) using Bayes' rule. If the groups are well separated and look Gaussian, then the resulting clusters will indeed tend to be ""distinct"" in the most common sense of the word - contiguous, densely populated areas of feature space, separated by contiguous, relatively empty regions. If the groups are not Gaussian, however, this correspondence may break down; an isolated group with a non-elliptical distribution, for example, may be modeled by not one, but several mixture components, and the corresponding clusters will no longer be well separated. We present methods for assessing the degree of separation between the components of a mixture model and between the corresponding clusters. We also propose a new clustering method that can be regarded as a hybrid between model-based and nonparametric clustering. The hybrid clustering algorithm prunes the cluster tree generated by hierarchical model-based clustering. Starting with the tree corresponding to the mixture model chosen by the Bayesian Information Criterion, it progressively merges clusters that do not appear to correspond to different modes of the data density."
780756D1	Knowledge Discovery and Data Mining	junshui ma + simon j perkins	2003	Online novelty detection on temporal sequences	support vector regression +  + online algorithm + confidence + Machine learning + Computing methodologies + anomaly detection	+Novelty detection+Anomaly Detection+Online algorithm+Support	In this paper, we present a new framework for online novelty detection on temporal sequences. This framework include a mechanism for associating each detection result with a confidence value. Based on this framework, we develop a concrete online detection algorithm, by modeling the temporal sequence using an online support vector regression algorithm. Experiments on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm.
7B37BA14	Knowledge Discovery and Data Mining	gregg t vesonder + jon r wright + tamraparni dasu	2003	Data quality through knowledge engineering	 + expert system + knowledge engineering + data quality + domain knowledge + data mining + data processing + Machine learning + Computing methodologies + business rules + subject matter expert	+Categories and Subject Descriptors I.2.6 [KDD Framework and Process+Keywords Data quality+Business Operations Databases+Static and Dynamic Constraints	Traditionally, data quality programs have acted as a preprocessing stage to make data suitable for a data mining or analysis operation. Recently, data quality concepts have been applied to databases that support business operations such as provisioning and billing. Incorporating business rules that drive operations and their associated data processes is critically important to the success of such projects. However, there are many practical complications. For example, documentation on business rules is often meager. Rules change frequently. Domain knowledge is often fragmented across experts, and those experts do not always agree. Typically, rules have to be gathered from subject matter experts iteratively, and are discovered out of logical or procedural sequence, like a jigsaw puzzle. Our approach is to impement business rules as constraints on data in a classical expert system formalism sometimes called production rules. Our system works by allowing good data to pass through a system of constraints unchecked. Bad data violate constraints and are flagged, and then fed back after correction. Constraints are added incrementally as better understanding of the business rules is gained. We include a real-life case study.
7F1441CF	Knowledge Discovery and Data Mining	soon tee teoh + kwanliu ma	2003	PaintingClass: interactive construction, visualization and exploration of decision trees	 + Philosophical/theoretical foundations of artificial intelligence + Computing methodologies + Human-centered computing + Information systems applications + knowledge discovery + information visualization + Human computer interaction (HCI) + classification + Data mining + Information systems + decision tree + decision trees + Computer graphics + interactive visualization + Cognitive science + Artificial intelligence	+alization+interactive	Decision trees are commonly used for classification. We propose to use decision trees not just for classification but also for the wider purpose of knowledge discovery, because visualizing the decision tree can reveal much valuable information in the data. We introduce PaintingClass, a system for interactive construction, visualization and exploration of decision trees. PaintingClass provides an intuitive layout and convenient navigation of the decision tree. PaintingClass also provides the user the means to interactively construct the decision tree. Each node in the decision tree is displayed as a visual projection of the data. Through actual examples and comparison with other classification methods, we show that the user can effectively use PaintingClass to construct a decision tree and explore the decision tree to gain additional knowledge.
7F59B3CD	Knowledge Discovery and Data Mining	yongdai kim	2003	Averaged boosting: a noise-robust ensemble method	empirical study + hilbert space		A new noise robust ensemble method called âAveraged Boosting (A-Boostingâ is proposed. Using the hypothetical ensemble algorithm in Hilbert space, we explain that A-Boosting can be understood as a method of constructing a sequence of hypotheses and coefficients such that the average of the product of the base hypotheses and coefficients converges to the desirable function. Empirical studies showed that A-Boosting outperforms Bagging for low noise cases and is more robust than AdaBoost to label noise.
5BC344A3	Knowledge Discovery and Data Mining	shashi shekhar + vipin kumar + yan huang + pusheng zhang	2003	Correlation analysis of spatial time series datasets: a filter-and-refine approach	 + spatial autocorrelation + time series		"
A spatial time series dataset is a collection of time series, each referencing a location in a common spatial framework. Correlation analysis is often used to identify pairs of potentially interacting elements from the cross product of two spatial time series datasets. However, the computational cost of correlation analysis is very high when the dimension of the time series and the number of locations in the spatial frameworks are large. The key contribution of this paper is the use of spatial autocorrelation among spatial neighboring time series to reduce computational cost. A filter-and-refine algorithm based on coning, i.e. grouping of locations, is proposed to reduce the cost of correlation analysis over a pair of spatial time series datasets. Cone-level correlation computation can be used to eliminate (filter out) a large number of element pairs whose correlation is clearly below (or above) a given threshold. Element pair correlation needs to be computed for remaining pairs. Using experimental studies with Earth science datasets, we show that the filter-and-refine approach can save a large fraction of the computational cost, particularly when the minimal correlation threshold is high.
"
0B1BCD75	Knowledge Discovery and Data Mining	aidong zhang + li zhang + murali ramanathan	2003	Enhanced visualization of time series through higher fourier harmonics	time series		"

"
5B6D9FD6	Knowledge Discovery and Data Mining	boonserm kijsirikul + ratthachat chatpatanasiri	2003	Upgrading ILP rules to first-order Bayesian networks	feature extraction + first order + conditional probability table + bayesian network		Inductive Logic Programming (ILP) is an efficient technique for relational data mining, but when ILP is applied in imperfect domains, the rules induced by ILP often struggle with the overfitting problem. This paper proposes a method to learn first-order Bayesian network (FOBN) which can handle imperfect data powerfully. Due to a high computation cost for directly learning FOBN, we adapt an ILP and a Bayesian network learner to construct FOBN. We propose a feature extraction algorithm to generate features from ILP rules, and use these features as the main structure of the FOBN. We also propose a propositionalisation algorithm for translating the original data into the single table format to learn the remaining parts of the FOBN structure and its conditional probability tables by a standard Bayesian network learner.
77A5226E	Knowledge Discovery and Data Mining	clare giardina + john chiochetti + william peter	2003	New unsupervised clustering algorithm for large datasets	 + rule based + initial condition + Information systems applications + geospatial data + clustering + k means + Information systems	+geospatial data+large datasets+data streaming	"A fast and accurate unsupervised clustering algorithm has been developed for clustering very large datasets. Though designed for very large volumes of geospatial data, the algorithm is general enough to be used in a wide variety of domain applications. The number of computations the algorithm requires is ~ O(N), and thus faster than hierarchical algorithms. Unlike the popular K-means heuristic, this algorithm does not require a series of iterations to converge to a solution. In addition, this method does not depend on initialization of a given number of cluster representatives, and so is insensitive to initial conditions. Being unsupervised, the algorithm can also ""rank"" each cluster based on density. The method relies on weighting a dataset to grid points on a mesh, and using a small number of rule-based agents to find the high density clusters. This method effectively reduces large datasets to the size of the grid, which is usually many orders of magnitude smaller. Numerical experiments are shown that demonstrate the advantages of this algorithm over other techniques."
0BA584D9	Knowledge Discovery and Data Mining	wai lam + yiqiu han	2003	Exploiting hierarchical domain values for Bayesian learning	bayesian learning		This paper proposes a framework for exploiting hierarchical structures of feature domain values in order to improve classification performance under Bayesian learning framework. Inspired by the statistical technique called shrinkage, we investigate the variances in the estimation of parameters for Bayesian learning. We develop two algorithms by maintaining a balance between precision and robustness to improve the estimation. We have evaluated our methods using two real-world data sets, namely, a weather data set and a yeast gene data set. The results demonstrate that our models benefit from exploring the hierarchical structures.
7D93209E	Knowledge Discovery and Data Mining	david fram + william dumouchel + june s almenoff	2003	Empirical Bayesian data mining for discovering patterns in post-marketing drug safety	 + association rule + Visualization + data mining + adverse reaction + drug safety + association rules + Information systems applications + Human-centered computing + adverse event + pharmacovigilance + Data mining + Visualization application domains + post marketing surveillance + Information systems + Scientific visualization	eol>Data mining + empirical Bayes methods + association rules + postmarketing surveillance + pharmacovigilance	"Because of practical limits in characterizing the safety profiles of therapeutic products prior to marketing, manufacturers and regulatory agencies perform post-marketing surveillance based on the collection of adverse reaction reports (""pharmacovigilance"").The resulting databases, while rich in real-world information, are notoriously difficult to analyze using traditional techniques. Each report may involve multiple medicines, symptoms, and demographic factors, and there is no easily linked information on drug exposure in the reporting population. KDD techniques, such as association finding, are well-matched to the problem, but are difficult for medical staff to apply and interpret.To deploy KDD effectively for pharmacovigilance, Lincoln Technologies and GlaxoSmithKline collaborated to create a webbased safety data mining web environment. The analytical core is a high-performance implementation of the MGPS (Multi-Item Gamma Poisson Shrinker) algorithm described previously by DuMouchel and Pregibon, with several significant extensions and enhancements. The environment offers an interface for specifying data mining runs, a batch execution facility, tabular and graphical methods for exploring associations, and drilldown to case details. Substantial work was involved in preparing the raw adverse event data for mining, including harmonization of drug names and removal of duplicate reports.The environment can be used to explore both drug-event and multi-way associations (interactions, syndromes). It has been used to study age/gender effects, to predict the safety profiles of proposed combination drugs, and to separate contributions of individual drugs to safety problems in polytherapy situations."
7521E71A	Knowledge Discovery and Data Mining	gleb tschapek + kevin b pratt	2003	Visualizing concept drift	 + visualization + concept drift + parallel coordinates + Information systems applications + Information systems	+Categories and Subject Descriptors H.2.8[Database Management+Database Applications- Data Mining Keywords Visualization+Concept Drift+Parallel Histogram+Parallel Coordinate Graph+Brushing	We describe a visualization technique that uses brushed, parallel histograms to aid in understanding concept drift in multidimensional problem spaces. This technique illustrates the relationship between changes in distributions of multiple antecedent feature values and the outcome distribution. We can also observe effects on the relative utilization of predictive rules. Our parallel histogram technique solves the over-plotting difficulty of parallel coordinate graphs and the difficulty of comparing distributions of brushed and original data. We demonstrate our technique's usefulness in understanding concept drifts in power demand and stock investment returns.
7B2E1D8A	Knowledge Discovery and Data Mining	neeraj agrawal + sachindra joshi + sumit negi + raghu krishnapuram	2003	A bag of paths model for measuring structural similarity in Web documents	 + query optimization + xml document + structural similarity + Information retrieval + Information systems	+fZU U H  8 flt	Structural information (such as layout and look-and-feel) has been extensively used in the literatuce for extraction of interesting or relevant data, efficient storage, and query optimization. Traditionally, tree models (such as DOM trees) have been used to represent structural information, especially in the case of HTML and XML documents. However, computation of structural similarity between documents based on the tree model is computationally expensive. In this paper, we propose an alternative scheme for representing the structural information of documents based on the paths contained in the corresponding tree model. Since the model includes partial information about parents, children and siblings, it allows us to define a new family of meaningful (and at the same time computationally simple) structural similarity measures. Our experimental results based on the SIGMOD XML data set as well as HTML document collections from ibm.com, dell.com, and amazon.com show that the representation is powerful enough to produce good clusters of structurally similar pages.
7BDB5A85	Knowledge Discovery and Data Mining	daphne koller	2003	Statistical learning from relational data	relational data + 	No keyword found	Distance function computation is a key subtask in many data mining algorithms and applications. The most effective form of the distance function can only be expressed in the context of a particular data domain. It is also often a challenging and non-trivial ...
7D003BCA	Knowledge Discovery and Data Mining	d a newlands + geoffrey i webb + shane m butler	2003	On detecting differences between groups	 + data analysis + data mining + Machine learning + Information systems applications + Computing methodologies + Information retrieval + retailing + Data mining + Information systems	Contrast-set discovery + Rule discovery + Retailing	Understanding the differences between contrasting groups is a fundamental task in data analysis. This realization has led to the development of a new special purpose data mining technique, contrast-set mining. We undertook a study with a retail collaborator to compare contrast-set mining with existing rule-discovery techniques. To our surprise we observed that straightforward application of an existing commercial rule-discovery system, Magnum Opus, could successfully perform the contrast-set-mining task. This led to the realization that contrast-set mining is a special case of the more general rule-discovery task. We present the results of our study together with a proof of this conclusion.
80132E88	Knowledge Discovery and Data Mining	john j salerno + zhongfei zhang + philip s yu	2003	Applying data mining in investigating money laundering crimes	 + data mining + Learning paradigms + data clustering + Computing methodologies + histogram + Supervised learning + coral + Classification and regression trees + Machine learning + Machine learning approaches + clustering + Supervised learning by classification	+Clustering+Histogram+Timeline Analysis	In this paper, we study the problem of applying data mining to facilitate the investigation of money laundering crimes (MLCs). We have identified a new paradigm of problems --- that of automatic community generation based on uni-party data, the data in which there is no direct or explicit link information available. Consequently, we have proposed a new methodology for Link Discovery based on Correlation Analysis (LDCA). We have used MLC group model generation as an exemplary application of this problem paradigm, and have focused on this application to develop a specific method of automatic MLC group model generation based on timeline analysis using the LDCA methodology, called CORAL. A prototype of CORAL method has been implemented, and preliminary testing and evaluations based on a real MLC case data are reported. The contributions of this work are: (1) identification of the uni-party data community generation problem paradigm, (2) proposal of a new methodology LDCA to solve for problems in this paradigm, (3) formulation of the MLC group model generation problem as an example of this paradigm, (4) application of the LDCA methodology in developing a specific solution (CORAL) to the MLC group model generation problem, and (5) development, evaluation, and testing of the CORAL prototype in a real MLC case data.
5CD8A090	Knowledge Discovery and Data Mining	lixiu yao + chenzhou ye + jie yang + nianyi chen	2003	Improving performance of decision tree algorithms with multi-edited nearest neighbor rule	decision tree + nearest neighbor		The paper proposed a new method based on the multi-edited nearest neighbor rule to prevent decision tree algorithms from growing a tree of unnecessary large size and hence partially alleviate the problem of âover-trainingâ. For this purpose, two useful prosperities of the multi-edited nearest neighbor rule are investigated. Experiments show that the method proposed could drastically reduce the size of resulting trees, significantly enhance their understandability, and meanwhile improve the test accuracy when the control parameter takes an appropriate value.
7A98D7BE	Knowledge Discovery and Data Mining	raj bhatnagar + wen niu + goutham kurra	2003	Mining high dimensional data for classifier knowledge	 + high dimensional data + Machine learning + Computing methodologies + statistical inference + gene expression + pattern recognition	No keyword found	We present in this paper the problem of discovering sets of attribute-value pairs in high dimensional data sets that are of interest not because of co-occurrence alone, but due to their value in serving as cores for potential classifiers of clusters. We present our algorithm in the context of a gene-expression dataset. Gene expression data, in most situations, is insufficient for clustering algorithms and any statistical inference because for 6000+ genes, typically only 10s and at most 100s of data points become available. It is difficult to use statistical techniques to design a classifier for such immensely under-specified data. The observed data, though statistically, insufficient contains some information about the domain. Our goal is to discover as much information about all potential classifiers as possible from the data and then summarize this knowledge. This summarization provides insights into the composition of potential classifiers. We present here algorithms and methods for mining a high dimensional data set, exemplified by a gene expression data set, for mining such information.
79513A0F	Knowledge Discovery and Data Mining	nick koudas + sudipto guha + dimitrios gunopulos	2003	Correlating synchronous and asynchronous data streams	 + data mining + data streams + Database query languages (principles) + Information systems + Theory and algorithms for application domains + Theory of computation + data stream mining + data reduction + synthetic data + Data management systems + Database theory + singular value decomposition + Query languages	No keyword found	In a variety of modern mining applications, data are commonly viewed as infinite time ordered data streams rather as finite data sets stored on disk. This view challenges fundamental assumptions commonly made in the context of several data mining algorithms.In this paper, we study the problem of identifying correlations between multiple data streams. In particular, we propose algorithms capable of capturing correlations between multiple continuous data streams in a highly efficient and accurate manner. Our algorithms and techniques are applicable in the case of both synchronous and asynchronous data streaming environments. We capture correlations between multiple streams using the well known technique of Singular Value Decomposition (SVD). Correlations between data items, and the SVD technique in particular, have been repeatedly utilized in an off-line (non stream) data mining problems, for example forecasting, approximate query answering, and data reduction.We propose a methodology based on a combination of dimensionality reduction and sampling to make the SVD technique suitable for a data stream context. Our techniques are approximate, trading accuracy with performance, and we analytically quantify this tradeoff. We present a through experimental evaluation, using both real and synthetic data sets, from a prototype implementation of our technique, investigating the impact of various parameters in the accuracy of the overall computation. Our results indicate, that correlations between multiple data streams can be identified very efficiently and accurately. The algorithms proposed herein, are presented as generic tools, with a multitude of applications on data stream mining problems.
7E4AFBAD	Knowledge Discovery and Data Mining	jiong yang + anthony k h tung + gao cong + feng pan + mohammed j zaki	2003	Carpenter: finding closed patterns in long biological datasets	 + Information systems applications + gene expression + Information systems	+H.2.8 [Database Management+Database Applications- Data Mining Keywords frequent pattern	The growth of bioinformatics has resulted in datasets with new characteristics. These datasets typically contain a large number of columns and a small number of rows. For example, many gene expression datasets may contain 10,000-100,000 columns but only 100-1000 rows.Such datasets pose a great challenge for existing (closed) frequent pattern discovery algorithms, since they have an exponential dependence on the average row length. In this paper, we describe a new algorithm called CARPENTER that is specially designed to handle datasets having a large number of attributes and relatively small number of rows. Several experiments on real bioinformatics datasets show that CARPENTER is orders of magnitude better than previous closed pattern mining algorithms like CLOSET and CHARM.
7DC9BAA0	Knowledge Discovery and Data Mining	andreas s weigend	2003	Analyzing customer behavior at Amazon.com	 + customer behavior	No keyword found	Distance function computation is a key subtask in many data mining algorithms and applications. The most effective form of the distance function can only be expressed in the context of a particular data domain. It is also often a challenging and non-trivial ...
7CA09520	Knowledge Discovery and Data Mining	david madigan + dmitriy fradkin	2003	Experiments with random projections for machine learning	 + dimensionality reduction + supervised learning + Machine learning + Computing methodologies + Probability and statistics + Mathematics of computing + machine learning	+Categories and Subject Descriptors G.3 [Probability and Statistics+Probabilistic algorithms+I.5.m [Pattern Recognition+Miscellaneous General Terms Experimentation+Performance Keywords random	Dimensionality reduction via Random Projections has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning. In particular, we compare Random Projections and PCA on a number of different datasets and using different machine learning methods. While we find that the random projection approach predictively underperforms PCA, its computational advantages may make it attractive for certain applications.
7FC355E2	Knowledge Discovery and Data Mining	jennifer l gardy + fei chen + martin ester + rong she + fiona s l brinkman + ke wang	2003	Frequent-subsequence-based prediction of outer membrane proteins	outer membrane +  + association rule + support vector machine + drug design + data mining + Information systems applications + genome sequence + classification + protein localization + Information systems	eol>Classification	"A number of medically important disease-causing bacteria (collectively called Gram-negative bacteria) are noted for the extra ""outer"" membrane that surrounds their cell. Proteins resident in this membrane (outer membrane proteins, or OMPs) are of primary research interest for antibiotic and vaccine drug design as they are on the surface of the bacteria and so are the most accessible targets to develop new drugs against. With the development of genome sequencing technology and bioinformatics, biologists can now deduce all the proteins that are likely produced in a given bacteria and have attempted to classify where proteins are located in a bacterial cell. However such protein localization programs are currently least accurate when predicting OMPs, and so there is a current need for the development of a better OMP classifier. Data mining research suggests that the use of frequent patterns has good performance in aiding the development of accurate and efficient classification algorithms. In this paper, we present two methods to identify OMPs based on frequent subsequences and test them on all Gram-negative bacterial proteins whose localizations have been determined by biological experiments. One classifier follows an association rule approach, while the other is based on support vector machines (SVMs). We compare the proposed methods with the state-of-the-art methods in the biological domain. The results demonstrate that our methods are better both in terms of accurately identifying OMPs and providing biological insights that increase our understanding of the structures and functions of these important proteins."
78E2EC66	Knowledge Discovery and Data Mining	l friedland + david jensen + michael hay + jennifer neville	2003	Learning relational probability trees	relational data + estimation +  + data mining + classification tree + statistical model + machine learning + relational learning + random testing	No keyword found	Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.
77CA01A7	Knowledge Discovery and Data Mining	wei fan + philip s yu + haixun wang + jiawei han	2003	Mining concept-drifting data streams using ensemble classifiers	 + data streams + Learning paradigms + Information systems applications + Computing methodologies + empirical study + knowledge discovery + Data mining + Information systems + concept drift + Supervised learning + Classification and regression trees + Machine learning + classifier + Machine learning approaches + Supervised learning by classification	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâdata min- ing+I.2.6 [Artificial Intelligence+Learningâconcept learning+I.5.2 [Pattern Recognition+Design Methodologyâclassifier de- sign and evaluation Keywords classifier	Recently, mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection, target marketing, network intrusion detection, etc. Conventional knowledge discovery tools are facing two challenges, the overwhelming volume of the streaming data, and the concept drifts. In this paper, we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers. We train an ensemble of classification models, such as C4.5, RIPPER, naive Beyesian, etc., from sequential chunks of the data stream. The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment. Thus, the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification. Our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy, and the ensemble framework is effective for a variety of classification models.
7C1FACBB	Knowledge Discovery and Data Mining	philip barry + jianping zhang + mary mcdonald	2003	Architecting a knowledge discovery engine for military commanders utilizing massive runs of simulations	 + bayesian network + emergent behavior + system architecture + data mining + Information systems applications + knowledge discovery + decision support + bayesian networks + Information systems	No keyword found	The Marine Corps' Project Albert seeks to model complex phenomenon by observing the behavior of relatively simple simulations over thousands of runs. A rich data base is developed by running the simulations thousands of times, varying the agent and scenario input parameters as well as the random seeds. Exploring this result space may provide significant insight into nonlinear, surprising, and emergent behaviors. Capturing these results can provide a path for making the results usable for decision support to a military commander. This paper presents two data mining approaches, rule discovery and Bayesian networks, for analyzing the Albert simulation data. The first approach generates rules from the data and then uses them to create descriptive model. The second generates Bayesian Networks which provide a quantitative belief model for decision support. Both of these approaches as well as the Project Albert simulations are framed in the context of a system architecture for decision support.
5B4FE6E2	Knowledge Discovery and Data Mining	howard j hamilton + xin wang	2003	DBRS: a density-based spatial clustering method with random sampling	very large database + spatial database + random sampling + k means		In this paper, we propose a novel density-based spatial clustering method called DBRS. The algorithm can identify clusters of widely varying shapes, clusters of varying densities, clusters which depend on non-spatial attributes, and approximate clusters in very large databases. DBRS achieves these results by repeatedly picking an unclassified point at random and examining its neighborhood. A theoretical comparison of DBRS and DBSCAN, a well-known density-based algorithm, is also given in the paper.
04B0983A	Knowledge Discovery and Data Mining	song junde + zhao yanchang	2003	AGRID: an efficient algorithm for clustering large high-dimensional datasets	grid + clustering + dimensionality + data mining		The clustering algorithm GDILC relies on density-based clustering with grid and is designed to discover clusters of arbitrary shapes and eliminate noises. However, it is not scalable to large high-dimensional datasets. In this paper, we improved this algorithm in five important directions. Through these improvements, AGRID is of high scalability and can process large high-dimensional datasets. It can discover clusters of various shapes and eliminate noises effectively. Besides, it is insensitive to the order of input and is a nonparametric algorithm. The high speed and accuracy of the AGRID clustering algorithm was shown in our experiments.
5D4D37AF	Knowledge Discovery and Data Mining	hui xiong + vipin kumar + jinmei xu + sam yuan sung	2003	A new clustering algorithm for transaction data via caucus	synthetic data + transaction data + point of sale		The fast-growing large point of sale databases in stores and companies sets a pressing need for extracting high-level knowledge. Transaction clustering arises to receive attentions in recent years. However, traditional clustering techniques are not useful to solve this problem. Transaction data sets are different from the traditional data sets in their high dimensionality, sparsity and a large number of outliers. In this paper we present and experimentally evaluate a new efficient transaction clustering technique based on cluster of buyers called caucus that can be effectively used for identification of center of cluster. Experiments on real and synthetic data sets indicate that compare to prior work, caucus-based method can derive clusters of better quality as well as reduce the execution time considerably.
7BEDEDA2	Knowledge Discovery and Data Mining	niall m adams + richard j bolton	2003	An iterative hypothesis-testing strategy for pattern discovery	 + data storage + hypothesis test + data mining + statistical models + statistical model + uncertainty + outlier detection + residual analysis	No keyword found	Pattern discovery has emerged as a direct result of increased data storage and analytic capabilities available to the data analyst. Without a massive amount of data, we do not have the evidence to support the discovery of the local deterministic structures that we call patterns. As such, pattern discovery is one of the few areas of data mining that cannot be considered simply as a 'scaling-up' of current statistical methodology to analyze large data sets. However, the philosophies of hypothesis testing and modeling in traditional statistics do lend themselves to forming a framework for pattern discovery, and we can also draw from ideas relating to outlier discovery and residual analysis to discover patterns. We illustrate an iterative strategy in a statistical framework by way of its application to one simulated and two real data sets.
599D9CCD	Knowledge Discovery and Data Mining	dekang lin + patrick pantel	2003	Discovery of inference rules from text	inference rule		"
In this paper, we propose an unsupervised method for discovering inference rules from text, such as âX is author of Y â X wrote Yâ, âX solved Y â X found a solution to Yâ, and âX caused Y â Y is triggered by Xâ. Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.
"
7A1DA510	Knowledge Discovery and Data Mining	charu c aggarwal	2003	Towards systematic design of distance functions for data mining applications	 + distance function + user requirements + satisfiability + data mining + Information systems applications + Data mining + data type + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data Mining Keywords Data mining+distance functions+user interaction	Distance function computation is a key subtask in many data mining algorithms and applications. The most effective form of the distance function can only be expressed in the context of a particular data domain. It is also often a challenging and non-trivial task to find the most effective form of the distance function. For example, in the text domain, distance function design has been considered such an important and complex issue that it has been the focus of intensive research over three decades. The final design of distance functions in this domain has been reached only by detailed empirical testing and consensus over the quality of results provided by the different variations. With the increasing ability to collect data in an automated way, the number of new kinds of data continues to increase rapidly. This makes it increasingly difficult to undertake such efforts for each and every new data type. The most important aspect of distance function design is that since a human is the end-user for any application, the design must satisfy the user requirements with regard to effectiveness. This creates the need for a systematic framework to design distance functions which are sensitive to the particular characteristics of the data domain. In this paper, we discuss such a framework. The goal is to create distance functions in an automated waywhile minimizing the work required from the user. We will show that this framework creates distance functions which are significantly more effective than popularly used functions such as the Euclidean metric.
7B008E05	Knowledge Discovery and Data Mining	yehuda koren + david harel	2003	A two-way visualization method for clustered data	 + Information systems applications + hierarchical clustering + information visualization + Data mining + dendrogram + Information systems	+H.2.8 [DATABASE MANAGEMENT+Database Applicationsâ Data mining Keywords Information visualization	We describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low-dimensional embedding, thereby gaining the benefits of both approaches. In a single image one can view all the clusters, examine the relations between them and study many of their properties. The method is based on an algorithm for low-dimensional embedding of clustered data, with the property that separation between all clusters is guaranteed, regardless of their nature. In particular, the algorithm was designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data, so that every two disjoint clusters in the hierarchy are drawn separately.
7A68D86D	Knowledge Discovery and Data Mining	s ma + jose e moreira + irina rish + adam j oliner + madan m gupta + r sahoo + ricardo vilalta + anand sivasubramaniam	2003	Critical event prediction for proactive management in large-scale computer clusters	 + bayesian network + system management + Information systems applications + Computing methodologies + time series model + time series + Information systems + rule based + event correlation + Machine learning + system performance + autonomic computing + control system	No keyword found	As the complexity of distributed computing systems increases, systems management tasks require significantly higher levels of automation; examples include diagnosis and prediction based on real-time streams of computer events, setting alarms, and performing continuous monitoring. The core of autonomic computing, a recently proposed initiative towards next-generation IT-systems capable of 'self-healing', is the ability to analyze data in real-time and to predict potential problems. The goal is to avoid catastrophic failures through prompt execution of remedial actions.This paper describes an attempt to build a proactive prediction and control system for large clusters. We collected event logs containing various system reliability, availability and serviceability (RAS) events, and system activity reports (SARs) from a 350-node cluster system for a period of one year. The 'raw' system health measurements contain a great deal of redundant event data, which is either repetitive in nature or misaligned with respect to time. We applied a filtering technique and modeled the data into a set of primary and derived variables. These variables used probabilistic networks for establishing event correlations through prediction algorithms. We also evaluated the role of time-series methods, rule-based classification algorithms and Bayesian network models in event prediction.Based on historical data, our results suggest that it is feasible to predict system performance parameters (SARs) with a high degree of accuracy using time-series models. Rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70% accuracy.
58F31E48	Knowledge Discovery and Data Mining	edmond h wu + michael k ng	2003	A graph-based optimization algorithm for website topology using interesting association rules	probability measure + association rule + web pages		The Web serves as a global information service center that contains vast amount of data. The Website structure should be designed effectively so that users can efficiently find their information. The main contribution of this paper is to propose a graph-based optimization algorithm to modify Website topology using interesting association rules. The interestingness of an association rule A â B is defined based on the probability measure between two sets of Web pages A and B in the Website. If the probability measure between A and B is low (high), then the association rule A â B has high (low) interest. The hyperlinks in the Website can be modified to adapt user access patterns according to association rules with high interest. We present experimental results and demonstrate that our method is effective.
5B4B1431	Knowledge Discovery and Data Mining	dennis mcleod + latifur khan	2003	Effective Retrieval of Audio Information from Annotated Text Using Ontologies	sql + precision + query expansion + information retrieval system + vector space model + ontology	eol>Metadata + Ontology + Audio + SQL + Precision + and Recall	"
To improve the accuracy in terms of precision and recall of an audio information retrieval system we have created a domainspecific ontology (a collection of key concepts and their interrelationships), as well as a novel, pruning algorithm. Taking into account the shortcomings of keyword-based techniques, we have opted to employ a concept-based technique utilizing this ontology. The key problem in the retrieval of audio information is to achieve high precision and high recall. Typically, in traditional approaches, high recall is achieved at the expense of low precision, and vice versa. Through the use of a domain-specific ontology appropriate concepts can be identified during metadata generation (description of audio) or query generation, thus improving precision. In case of the association of irrelevant concepts to queries or documents there is a loss of precision. On the other hand, if relevant concepts are discarded, a loss of recall will ensue. Therefore, in conjunction with the use of a domain specific ontology we have proposed a novel, automatic pruning algorithm which prunes as many irrelevant concepts as possible during any case of query generation. By associating concepts in the ontology through techniques of correlation, this algorithm presents a method for the selection of concepts in the query generation. To improve recall, controlled and correct query expansion mechanism is proposed. This guarantees that precision will not be lost. Moreover, we present a way for the query generation in which domain-specific ontology can be used to generate information selection requests in terms of database queries in SQL. In trial implementations we have demonstrated that our ontology-based model outperforms keyword-based technique (vector space model) in terms of precision and recall. 1 This research has been funded [or funded in part] by the Integrated Media Systems Center, a National Science Foundation Engineering Research Center, Cooperative Agreement No. EEC-9529152.
"
780C6F08	Knowledge Discovery and Data Mining	joseph l hellerstein + david thoenen + genady grabarnik + sheng ma + changshing perng	2003	Data-driven validation, completion and construction of event relationship networks	 + information infrastructure + design process + root cause analysis + service design	No keyword found	Event management is a focal point in building and maintaining high quality information infrastructures. We have witnessed the shift of the paradigm of event management in practice from root cause analysis (RCA) to action-oriented analysis (AOA). IBM has developed a pioneer event management methodology (EMD) based on the AOA paradigm and applied it to more than two hundred production sites with success. Foreseeably, more and more event management professionals will apply AOA in different incarnations in building proactive management facilities. By that, building correct and effective Event Relationship Networks (ERNs) becomes the dominating activity in AOA service design process. Currently, the quality of ERNs and the cost of building them largely depend on the knowledge of domain experts. We believe that we can utilize historical event logs in shortening the ERNs design process and perfecting the quality of ERNs. In this paper, we describe in detail how to apply this data-driven approach in ERN validation, completion and construction.
05F988EA	Knowledge Discovery and Data Mining	aleksandar icev + elizabeth f ryder + carolina ruiz	2003	Distance-enhanced association rules for gene expression	 + association rule + gene expression analysis + data mining + gene expression + association rule mining		"
We introduce a novel data mining technique for the analysis of gene expression. Gene expression is the effective production of the protein that a gene encodes. We focus on the characterization of the expression patterns of genes based on their promoter regions. The promoter region of a gene contains short sequences called motifs to which gene regulatory proteins may bind, thereby controlling when and in which cell types the gene is expressed. Our approach addresses two important aspects of gene expression analysis: (1) Binding of proteins at more than one motif is usually required, and several different types of proteins may need to bind several different types of motifs in order to confer transcriptional specificity. (2) Since proteins controlling transcription may need to interact physically, we know that the order and spacing in which motifs occur can affect expression. We use association rules to address the combinatorial aspect. The association rules we employ have the ability to involve multiple motifs and to predict expression in multiple cell types. To address the second aspect, we enhance association rules with information about the distances among the motifs, or items, that are present in the rule. Rules of interest are those whose set of motifs deviates properly, i.e. set of motifs whose pair-wise distances are highly conserved in the promoter regions where these motifs occur. We describe the design, implementation, and evaluation of our Distance-based Association Rule Mining algorithm (DARM) to mine those rules. We show that these distance-based rules achieve higher classification performance than standard association rules over two real datasets. gene expression analysis, distance-based association rule mining.
"
7C03F06E	Knowledge Discovery and Data Mining	karam gouda + mohammed j zaki	2003	Fast vertical mining using diffsets	 + data representation + Information systems applications + association rule mining + Information systems	No keyword found	A number of vertical mining algorithms have been proposed recently for association mining, which have shown to be very effective and usually outperform horizontal approaches. The main advantage of the vertical format is support for fast frequency counting via intersection operations on transaction ids (tids) and automatic pruning of irrelevant data. The main problem with these approaches is when intermediate results of vertical tid lists become too large for memory, thus affecting the algorithm scalability.In this paper we present a novel vertical data representation called Diffset, that only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns. We show that diffsets drastically cut down the size of memory required to store intermediate results. We show how diffsets, when incorporated into previous vertical mining methods, increase the performance significantly.
7C6B21F4	Knowledge Discovery and Data Mining	bin chen + peter j haas + herve bronnimann + manoranjan dash + peter scheuermann	2003	Efficient data reduction with EASE	random sampling + sampling + count data + data streams + olap + association rule mining + association rules + contingency table + association rule + simple random sampling + data reduction		"""A variety of mining and analysis problems - ranging from association-rule discovery to contingency table analysis to materialization of certain approximate datacubes - involve the extraction of knowledge from a set of categorical count data. Such data can be viewed as a collection of """"transactions,"""" where a transaction is a fixed-length vector of counts. Classical algorithms for solving count-data problems require one or more computationally intensive passes over the entire database and can be prohibitively slow. One effective method for dealing with this ever-worsening scalability problem is to run the algorithms on a small sample of the data. We present a new data-reduction algorithm, called EASE, for producing such a sample. Like the FAST algorithm introduced by Chen et al., EASE is especially designed for count data applications. Both EASE and FAST take a relatively large initial random sample and then deterministically produce a subsample whose """"distance"""" - appropriately defined - from the complete database is minimal. Unlike FAST, which obtains the final subsample by quasi-greedy descent, EASE uses epsilon-approximation methods to obtain the final subsample by a process of repeated halving. Experiments both in the context of association rule mining and classical X2 contingency-table analysis show that EASE outperforms both FAST and simple random sampling, sometimes dramatically. Copyright 2003 ACM."""
750B28A9	Knowledge Discovery and Data Mining	mong li lee + jing dai + wynne hsu	2003	Mining viewpoint patterns in image databases	 + Information systems applications + spatial relationships + spatial relationship + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- Data mining+Image database. Keywords Image mining+spatial relationship+image database	The increasing number of image repositories has made image mining an important task because of its potential in discovering useful image patterns from a large set of images. In this paper, we introduce the notion of viewpoint patterns for image databases. Viewpoint patterns refer to patterns that capture the invariant relationships of one object from the point of view of another object. These patterns are unique and significant in images because the absolute positional information of objects for most images is not important, but rather, it is the relative distance and orientation of the objects from each other that is meaningful. We design a scalable and efficient algorithm to discover such viewpoint patterns. Experiments results on various image sets demonstrate that viewpoint patterns are meaningful and interesting to human users.
7A7153E9	Knowledge Discovery and Data Mining	koichi furukawa + keiko shimazu + atsuhito momma	2003	Experimental study of discovering essential information from customer inquiry	 + association rule + Operations research + data mining + Information systems applications + noun + Business process management + Data mining + association rule mining + Information systems + Marketing + Enterprise computing + Applied computing + text mining	+text mining+association rule+prior confidence+posterior confidence+syntactic dependency	This paper reports the result of our experimental study on a new method of applying an association rule miner to discover useful information from customer inquiry database in a call center of a company. It has been claimed that association rule mining is not suited for text mining. To overcome this problem, we propose (1) to generate sequential data set of words with dependency structure from the Japanese text database, and (2) to employ a new method for extracting meaningful association rules by applying a new rule selection criterion. Each inquiry in the sequential data was represented as a list of word pairs, each of which consists of a verb and its dependent noun. The association rules were induced regarding each pair of words as an item. The rule selection criterion comes from our principle that we put heavier weights to co-occurrence of multiple items more than single item occurrence. We regarded a rule important if the existence of the items in the rule body significantly affects the occurrence of the item in the rule head. The selected rules were then categorized to form meaningful information classes. With this method, we succeeded in extracting useful information classes from the text database, which were not acquired by only simple keyword retrieval. Also, inquiries with multiple aspects were properly classified into corresponding multiple categories.
5CB4A30F	Knowledge Discovery and Data Mining	bruno bachimont + younes hafri + peter l stanchev + chabane djeraba	2003	A Markovian approach for web user profiling and clustering	markov model		The objective of this paper is to propose an approach that extracts automatically web user profiling based on user navigation paths. Web user profiling consists of the best representative behaviors, represented by Markov models (MM). To achieve this objective, our approach is articulated around three notions: (1) Applying probabilistic exploration using Markov models. (2) Avoiding the problem of Markov model high-dimensionality and sparsity by clustering web documents, based on their content, before applying the Markov analysis. (3) Clustering Markov models, and extraction of their gravity centers. On the basis of these three notions, the approach makes possible the prediction of future states to be visited in k steps and navigation sessions monitoring, based on both content and traversed paths. The original application of the approach concerns the exploitation of multimedia archives in the perspective of the Copyright Deposit that preserves Frenchâs WWW documents. The approach may be the exploitation tool for any web site.
5F2E65FA	Knowledge Discovery and Data Mining	william m pottenger + tianhao wu	2003	A semi-supervised algorithm for pattern discovery in information extraction from textual data	text mining + supervised learning + data mining + regular expression + information extraction + regular language + machine learning		In this article we present a semi-supervised algorithm for pattern discovery in information extraction from textual data. The patterns that are discovered take the form of regular expressions that generate regular languages. We term our approach âsemi-supervisedâ because it requires significantly less effort to develop a training set than other approaches. From the training data our algorithm automatically generates regular expressions that can be used on previously unseen data for information extraction. Our experiments show that the algorithm has good testing performance on many features that are important in the fight against terrorism.
NE8603	Knowledge Discovery and Data Mining	Mark Last+Menahem Friedman+Abraham Kandel	2003	The data mining approach to automated software testing.	Software defect analysis +  + Software creation and management + Software verification and validation + Information systems applications + Computing methodologies + Data mining + Information systems + Logical and relational learning + Inductive logic learning + Software testing and debugging + Machine learning + Machine learning approaches + Software and its engineering	+Regression Testing+Input-Output Analysis+Info-Fuzzy Networks+Finite Element Solver	"In today's industry, the design of software tests is mostly based on the testers' expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world's economy. The costs of the so-called ""catastrophic"" software failures (such as Mars Polar Lander shutdown in 1999) are even hard to measure. In this paper, we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data. The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications, designing a minimal set of regression tests, and evaluating the correctness of software outputs when testing new, potentially flawed releases of the system. To study the feasibility of the proposed approach, we have applied a novel data mining algorithm called Info-Fuzzy Network (IFN) to execution data of a general-purpose code for solving partial differential equations. After being trained on a relatively small number of randomly generated input-output examples, the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program."
NE8579	Knowledge Discovery and Data Mining	Mehmet KoyutÃ¼rk+Ananth Grama	2003	PROXIMUS: a framework for analyzing very high dimensional discrete-attributed datasets.	 + Cluster analysis + Learning paradigms + Machine learning + Information systems applications + Computing methodologies + Data mining + Unsupervised learning + Information systems	+%9 +ffi	This paper presents an efficient framework for error-bounded compression of high-dimensional discrete attributed datasets. Such datasets, which frequently arise in a wide variety of applications, pose some of the most significant challenges in data analysis. Subsampling and compression are two key technologies for analyzing these datasets. PROXIMUS provides a technique for reducing large datasets into a much smaller set of representative patterns, on which traditional (expensive) analysis algorithms can be applied with minimal loss of accuracy. We show desirable properties of PROXIMUS in terms of runtime, scalability to large datasets, and performance in terms of capability to represent data in a compact form. We also demonstrate applications of PROXIMUS in association rule mining. In doing so, we establish PROXIMUS as a tool for preprocessing data before applying computationally expensive algorithms or as a tool for directly extracting correlated patterns. Our experimental results show that use of the compressed data for association rule mining provides excellent precision and recall values (near 100%) across a range of support thresholds while reducing the time required for association rule mining drastically.
806B99CB	Knowledge Discovery and Data Mining	osmar r zaiane + mohammad elhajj	2003	Inverted matrix: efficient discovery of frequent items in large datasets in the context of interactive mining	association rule + association rules + transaction data + random access + association rule mining + data structure		Existing association rule mining algorithms suffer from many problems when mining massive transactional datasets. One major problem is the high memory dependency: either the gigantic data structure built is assumed to fit in main memory, or the recursive mining process is too voracious in memory resources. Another major impediment is the repetitive and interactive nature of any knowledge discovery process. To tune parameters, many runs of the same algorithms are necessary leading to the building of these huge data structures time and again. This paper proposes a new disk-based association rule mining algorithm called Inverted Matrix, which achieves its efficiency by applying three new ideas. First, transactional data is converted into a new database layout called Inverted Matrix that prevents multiple scanning of the database during the mining phase, in which finding frequent patterns could be achieved in less than a full scan with random access. Second, for each frequent item, a relatively small independent tree is built summarizing co-occurrences. Finally, a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed. Experimental studies reveal that our Inverted Matrix approach outperform FP-Tree especially in mining very large transactional databases with a very large number of unique items. Our random access disk-based approach is particularly advantageous in a repetitive and interactive setting. Copyright 2003 ACM.
8078B221	Knowledge Discovery and Data Mining	william dumouchel + deepak agarwal	2003	Applications of sampling and fractional factorial designs to model-free data squashing	 + curse of dimensionality + random sampling + statistical model + stratified sampling + principal component + fractional factorial design	+General Terms Algorithms+Performance+Design Keywords Summary of massive datasets+data squashing+fractional factorial	"The concept of ""data squashing"" was introduced by DuMouchel et al [4] as a method of summarizing massive data sets that preserves statistical relationships among variables. The idea is to create a smaller data set that allows statistical modeling to take place using in-memory algorithms, and to preserve the modeling results more accurately than would a same-size random sample from the massive data set. This research attempts to avoid several limitations of previous approaches to data squashing. Our method avoids the curse of dimensionality by a double use of principal components transformations that makes computing time linear in the number of cases and quadratic in the number of variables. Categorical and continuous variables are smoothly integrated. Because the binning is based on principal components, which are uncorrelated, we can use fractional factorial designs that sample less than one point per bin. We also investigate various weighting schemes for the squashed sample to see whether matching moments or matching subregion data counts is more effective. Finally, previous work required the specification of a statistical model, either to perform the squashing algorithm or to compare the worth of different squashing methods. Our approach to evaluation is model free and does not even require the specification of variables as responses or predictors. Instead, we develop a chi-squared like measure of accuracy to compare the closeness of various discrete densities (the squashed data sets) to the discrete massive data set."
758899F3	Knowledge Discovery and Data Mining	michael e houle	2003	Navigating massive data sets via local clustering	 + similarity search + Retrieval tasks and goals + confidence + Information systems applications + soft clustering + association + Clustering and classification + Information retrieval + Data mining + Clustering + Information systems + nearest neighbor + feature extraction	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data mining+H.3.3 [Information Storage and Retrieval+Information Search and RetrievalâClustering Keywords Soft clustering	This paper introduces a scalable method for feature extraction and navigation of large data sets by means of local clustering, where clusters are modeled as overlapping neighborhoods. Under the model, intra-cluster association and external differentiation are both assessed in terms of a natural confidence measure. Minor clusters can be identified even when they appear in the intersection of larger clusters. Scalability of local clustering derives from recent generic techniques for efficient approximate similarity search. The cluster overlap structure gives rise to a hierarchy that can be navigated and queried by users. Experimental results are provided for two large text databases.
5A897F4B	Knowledge Discovery and Data Mining	a hoffmann + johanna i westbrook + tatjana zrimec + ashesh mahidadia + siew siew ong + annie lau	2003	Mining patterns of dyspepsia symptoms across time points using constraint association rules	scenario analysis + knowledge discovery + human interaction + association rule mining + association rule + domain knowledge + temporal data		In this paper, we develop and implement a framework for constraint-based association rule mining across subgroups in order to help a domain expert find useful patterns in a medical data set that includes temporal data. This work is motivated by the difficulties experienced in the medical domain to identify and track dyspepsia symptom clusters within and across time. Our framework, Apriori with Subgroup and Constraint (ASC), is built on top of the existing Apriori framework. We have identified four different types of phase-wise constraints for subgroups: constraint across subgroups, constraint on subgroup, constraint on pattern content and constraint on rule. ASC has been evaluated in a real-world medical scenario; analysis was conducted with the interaction of a domain expert. Although the framework is evaluated using a data set from the medical domain, it should be general enough to be applicable in other domains.
80C157DC	Knowledge Discovery and Data Mining	werner goebl + elias pampalk + gerhard widmer	2003	Visualizing changes in the structure of data for exploratory feature selection	Cluster analysis +  + high dimensional data + feature extraction + data mining + Learning paradigms + Machine learning + Computing methodologies + structural change + feature selection + Unsupervised learning	+sures+algorithms Keywords High-Dimensional Data+Interactive Data Mining	Using visualization techniques to explore and understand high-dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays. Several visualization techniques have been developed to study the cluster structure of data, i.e., the existence of distinctive groups in the data and how these clusters are related to each other. However, only few of these techniques lend themselves to studying how this structure changes if the features describing the data are changed. Understanding this relationship between the features and the cluster structure means understanding the features themselves and is thus a useful tool in the feature extraction phase.In this paper we present a novel approach to visualizing how modification of the features with respect to weighting or normalization changes the cluster structure. We demonstrate the application of our approach in two music related data mining projects.
7822DE50	Knowledge Discovery and Data Mining	bing liu + robert grossman + yanhong zhai	2003	Mining data records in Web pages	 + web pages + information integration + web mining + Machine learning + Computing methodologies + Information systems applications + string matching + Information systems	eol>Web data records + Web mining	A large amount of information on the Web is contained in regularly structured objects, which we call data records. Such data records are important because they often present the essential information of their host pages, e.g., lists of products or services. It is useful to mine such data records in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about data records on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and non-contiguous data records. Our experimental results show that the proposed technique outperforms existing techniques substantially.
78476A96	Knowledge Discovery and Data Mining	jim gray	2003	On-line science: the world-wide telescope as a prototype for the new computational science		No keyword found	Distance function computation is a key subtask in many data mining algorithms and applications. The most effective form of the distance function can only be expressed in the context of a particular data domain. It is also often a challenging and non-trivial ...
7D00E891	Knowledge Discovery and Data Mining	se june hong + richard d lawrence + jacques cherrier	2003	Passenger-based predictive modeling of airline no-show rates	 + prediction model + data mining + linear regression + Information systems applications + classification + Data mining + probabilistic model + Information systems + decision tree + naive bayes + predictive modeling	No keyword found	Airlines routinely overbook flights based on the expectation that some fraction of booked passengers will not show for each flight. Accurate forecasts of the expected number of no-shows for each flight can increase airline revenue by reducing the number of spoiled seats (empty seats that might otherwise have been sold) and the number of involuntary denied boardings at the departure gate. Conventional no-show forecasting methods typically average the no-show rates of historically similar flights, without the use of passenger-specific information.We develop two classes of models to predict cabin-level no-show rates using specific information on the individual passengers booked on each flight. The first of these models computes the no-show probability for each passenger, using both the cabin-level historical forecast and the extracted passenger features as explanatory variables. This passenger-level model is implemented using three different predictive methods: a C4.5 decision-tree, a segmented Naive Bayes algorithm, and a new aggregation method for an ensemble of probabilistic models. The second cabin-level model is formulated using the desired cabin-level no-show rate as the response variable. Inputs to this model include the predicted cabin-level no-show rates derived from the various passenger-level models, as well as simple statistics of the features of the cabin passenger population. The cabin-level model is implemented using either linear regression, or as a direct probability model with explicit incorporation of the cabin-level no-show rates derived from the passenger-level model outputs.The new passenger-based models are compared to a conventional historical model, using train and evaluation data sets taken from over 1 million passenger name records. Standard metrics such as lift curves and mean-square cabin-level errors establish the improved accuracy of the passenger-based models over the historical model. All models are also evaluated using a simple revenue model, and it is shown that the cabin-level passenger-based model can produce between 0.4% and 3.2% revenue gain over the conventional model, depending on the revenue-model parameters.
NE8612	Knowledge Discovery and Data Mining	Shlomo Argamon+Marin Å ariÄ+Sterling S. Stein	2003	Style mining of electronic messages for multiple authorship discrimination: first results.	 + Machine learning + Computing methodologies + Information retrieval + Information systems	No keyword found	This paper considers the use of computational stylistics for performing authorship attribution of electronic messages, addressing categorization problems with as many as 20 different classes (authors). Effective stylistic characterization of text is potentially useful for a variety of tasks, as language style contains cues regarding the authorship, purpose, and mood of the text, all of which would be useful adjuncts to information retrieval or knowledge-management tasks. We focus here on the problem of determining the author of an anonymous message, based only on the message text. Several multiclass variants of the Winnow algorithm were applied to a vector representation of the message texts to learn models for discriminating different authors. We present results comparing the classification accuracy of the different approaches. The results show that stylistic models can be accurately learned to determine an author's identity.
NE8575	Knowledge Discovery and Data Mining	Mohammad El-Hajj+Osmar R. ZaÃ¯ane	2003	Inverted matrix: efficient discovery of frequent items in large datasets in the context of interactive mining.	 + Information systems applications + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Data Mining Keywords Association Rules+Frequent Patterns Mining+COFI-tree+Inverted Matrix	Existing association rule mining algorithms suffer from many problems when mining massive transactional datasets. One major problem is the high memory dependency: either the gigantic data structure built is assumed to fit in main memory, or the recursive mining process is too voracious in memory resources. Another major impediment is the repetitive and interactive nature of any knowledge discovery process. To tune parameters, many runs of the same algorithms are necessary leading to the building of these huge data structures time and again. This paper proposes a new disk-based association rule mining algorithm called Inverted Matrix, which achieves its efficiency by applying three new ideas. First, transactional data is converted into a new database layout called Inverted Matrix that prevents multiple scanning of the database during the mining phase, in which finding frequent patterns could be achieved in less than a full scan with random access. Second, for each frequent item, a relatively small independent tree is built summarizing co-occurrences. Finally, a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed. Experimental studies reveal that our Inverted Matrix approach outperform FP-Tree especially in mining very large transactional databases with a very large number of unique items. Our random access disk-based approach is particularly advantageous in a repetitive and interactive setting.
NE8622	Knowledge Discovery and Data Mining	Åule GÃ¼ndÃ¼z+M. Tamer Ãzsu	2003	A Web page prediction model based on click-stream tree representation of user behavior.	 + Machine learning + Computing methodologies	+Categories and Subject Descriptors I.5.2 [Pattern Recognition+Design Methodology- clas- sifier design and evaluation Keywords Web usage mining+two dimensional sequential model+graph based	Predicting the next request of a user as she visits Web pages has gained importance as Web-based activity increases. Markov models and their variations, or models based on sequence mining have been found well suited for this problem. However, higher order Markov models are extremely complicated due to their large number of states whereas lower order Markov models do not capture the entire behavior of a user in a session. The models that are based on sequential pattern mining only consider the frequent sequences in the data set, making it difficult to predict the next request following a page that is not in the sequential pattern. Furthermore, it is hard to find models for mining two different kinds of information of a user session. We propose a new model that considers both the order information of pages in a session and the time spent on them. We cluster user sessions based on their pair-wise similarity and represent the resulting clusters by a click-stream tree. The new user session is then assigned to a cluster based on a similarity measure. The click-stream tree of that cluster is used to generate the recommendation set. The model can be used as part of a cache prefetching system as well as a recommendation model.
NE8570	Knowledge Discovery and Data Mining	HervÃ© BrÃ¶nnimann+Bin Chen+Manoranjan Dash+Peter Haas+Peter Scheuermann	2003	Efficient data reduction with EASE.	 + Information systems applications + Information systems	No keyword found	"A variety of mining and analysis problems --- ranging from association-rule discovery to contingency table analysis to materialization of certain approximate datacubes --- involve the extraction of knowledge from a set of categorical count data. Such data can be viewed as a collection of ""transactions,"" where a transaction is a fixed-length vector of counts. Classical algorithms for solving count-data problems require one or more computationally intensive passes over the entire database and can be prohibitively slow. One effective method for dealing with this ever-worsening scalability problem is to run the algorithms on a small sample of the data. We present a new data-reduction algorithm, called EASE, for producing such a sample. Like the FAST algorithm introduced by Chen et al., EASE is especially designed for count data applications. Both EASE and FAST take a relatively large initial random sample and then deterministically produce a subsample whose ""distance"" --- appropriately defined --- from the complete database is minimal. Unlike FAST, which obtains the final subsample by quasi-greedy descent, EASE uses epsilon-approximation methods to obtain the final subsample by a process of repeated halving. Experiments both in the context of association rule mining and classical Ï2 contingency-table analysis show that EASE outperforms both FAST and simple random sampling, sometimes dramatically."
7E1A9A3B	Knowledge Discovery and Data Mining	daxin jiang + aidong zhang + jian pei	2003	Interactive exploration of coherent patterns in time-series gene expression data	Cluster analysis +  + tree structure + indexation + bioinformatics + Learning paradigms + Machine learning + Computing methodologies + time series + Unsupervised learning	coherent patterns + gene expression data + bioinformatics	Discovering coherent gene expression patterns in time-series gene expression data is an important task in bioinformatics research and biomedical applications. In this paper, we propose an interactive exploration framework for mining coherent expression patterns in time-series gene expression data. We develop a novel tool, coherent pattern index graph, to give users highly confident indications of the existences of coherent patterns. To derive a coherent pattern index graph, we devise an attraction tree structure to record the genes in the data set and summarize the information needed for the interactive exploration. We present fast and scalable algorithms to construct attraction trees and coherent pattern index graphs from gene expression data sets. We conduct an extensive performance study on some real data sets to verify our design. The experimental results strongly show that our approach is more effective than the state-of-the-art methods in mining real gene expression data, and is scalable in mining large data sets.
7AA4A686	Knowledge Discovery and Data Mining	edward y chang + huaxing you + yileh wu + kingshy goh + beitao li	2003	The anatomy of a multimodal information filter	 + empirical study + Information retrieval + Information systems	+Å&quot;4ffA+-8E <newline> GTE <newline> 	The proliferation of objectionable information on the Internet has reached a level of serious concern. To empower end-users with the choice of blocking undesirable and offensive websites, we propose a multimodal information filter, named MORF. In this paper, we present MORF's core components: its confidence-based classifier, a Cross-bagging ensemble scheme, and multimodal classification algorithm. Empirical studies and initial statistics collected from the MORF filters deployed at sites in the U.S. and Asia show that MORF is both efficient and effective, due to our classification methods.
7C794032	Knowledge Discovery and Data Mining	wenwu lou + guimei liu + hongjun lu + jeffrey xu yu	2003	On computing, storing and querying frequent patterns	 + data mining + data structure + Information systems applications + data warehousing + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- Data Mining Keywords data mining and	"Extensive efforts have been devoted to developing efficient algorithms for mining frequent patterns. However, frequent pattern mining remains a time-consuming process, especially for very large datasets. It is therefore desirable to adopt a ""mining once and using many times"" strategy. Unfortunately, there has been little work reported on managing and organizing a large set of patterns for future use. In this paper, we propose a disk-based data structure, CFP-tree (Condensed Frequent Pattern Tree), for organizing frequent patterns discovered from transactional databases. In addition to an efficient algorithm for CFP-tree construction, we also developed algorithms to efficiently support two important types of queries, namely queries with minimum support constraints and queries with item constraints, against the stored patterns, as these two types of queries are basic building blocks for complex frequent pattern related mining tasks. Comprehensive experimental study has been conducted to demonstrate the effectiveness of CFP-tree and efficiency of related algorithms."
0306CFC3	Knowledge Discovery and Data Mining	mark w craven + marios skounakis	2003	Evidence combination in biomedical natural-language processing	natural language processing		"

"
75EE664A	Knowledge Discovery and Data Mining	padhraic smyth + scott r white	2003	Algorithms for estimating relative importance in networks	 + data analysis + graph theory + markov chains + social networks + Information retrieval + markov chain + Information systems + social network + biological network + graphs + markov model	+Markov chains+PageRank+social networks+relative	"Large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis---examples include social networks, Web graphs, telecommunication networks, and biological networks. In interactive analysis of such data a natural query is ""which entities are most important in the network relative to a particular individual or set of individuals?"" We investigate the problem of answering such queries in this paper, focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes. We define a general framework and a number of different algorithms, building on ideas from social networks, graph theory, Markov models, and Web graph analysis. We experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among September 11th terrorists, a network of collaborative research in biotechnology among companies and universities, and a network of co-authorship relationships among computer science researchers."
7B7A3ACA	Knowledge Discovery and Data Mining	alain casali + lotfi lakhal + rosine cicchetti	2003	Extracting semantics from data cubes using cube transversals and closures	closure operator +  + closures + Information systems applications + lattices + data cube + expressive power + algorithm + algorithm design + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Data Mining Keywords Algorithm+Closures+Datacubes+Hypergraph Transversals+Lattices+Version Spaces	In this paper we propose a lattice-based approach intended for extracting semantics from datacubes: borders of version spaces for supervised classification, closed cube lattice to summarize the semantics of datacubes w.r.t. COUNT, SUM, and covering graph of the quotient cube as a visualization tool of minimal multidimensional associations. With this intention, we introduce two novel concepts: the cube transversals and the cube closures over the cube lattice of a categorical database relation. We propose a levelwise merging algorithm for mining minimal cube transversals with a single database scan. We introduce the cube connection, show that it is a Galois connection and derive a closure operator over the cube lattice. Using cube transversals and closures, we define a new characterization of boundary sets which provide a condensed representation of version spaces used to enhance supervised classification. The algorithm designed for computing such borders improves the complexity of previous proposals. We also introduce the concept of closed cube lattice and show that it is isomorph to on one hand the Galois lattice and on the other hand the quotient cube w.r.t. COUNT, SUM. Proposed in [16], the quotient cube is a succinct summary of a datacube preserving the Rollup/Drilldown semantics. We show that the quotient cube w.r.t. COUNT, SUM and the closed cube lattice have a similar expression power but the latter has the smallest possible size. Finally we focus on the multidimensional association issue and introduce the covering graph of the quotient cube which provides the user with a visualization tool of minimal multidimensional associations.
NE8651	Knowledge Discovery and Data Mining	Gloria T. Lau+Kincho H. Law+Gio Wiederhold	2003	Similarity analysis on government regulations.	 + Document representation + Enterprise computing + Applied computing + Computing methodologies + Information systems applications + Information retrieval + Natural language processing + Data mining + Content analysis and feature selection + Artificial intelligence + Information systems	+Categories and Subject Descriptors H.3.1 [Information Storage and Retrieval+Content Analysis and Indexing â linguistic processing+H.2.8 [Database Management+Database Applications â data mining+J.1 [Administrative Data Processing+Law. Keywords Regulations+Similarity Analysis+Legal Informatics+Text Mining	Government regulations are semi-structured text documents that are often voluminous, heavily cross-referenced between provisions and even ambiguous. Multiple sources of regulations lead to difficulties in both understanding and complying with all applicable codes. In this work, we propose a framework for regulation management and similarity analysis. An online repository for legal documents is created with the help of text mining tool, and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy generated by knowledge engineers based on concepts. Our similarity analysis core identifies relevant provisions and brings them to the user's attention, and this is performed by utilizing both the hierarchical and referential structures of regulations to provide a better comparison between provisions. Preliminary results show that our system reveals hidden similarities that are not apparent between provisions based on node content comparisons.
5E1939D2	Knowledge Discovery and Data Mining	alex kovarsky + nan niu + xuejie qin + zhiyong lu + jorg sander	2003	Automatic extraction of clusters from hierarchical clustering representations	 + hierarchical clustering + optics + dendrogram	Hierarchical clustering + OPTICS + Single-Link method + dendrogram + reachability-plot	"
Hierarchical clustering algorithms are typically more effective in detecting the true clustering structure of a data set than partitioning algorithms. However, hierarchical clustering algorithms do not actually create clusters, but compute only a hierarchical representation of the data set. This makes them unsuitable as an automatic pre-processing step for other algorithms that operate on detected clusters. This is true for both dendrograms and reachability plots, which have been proposed as hierarchical clustering representations, and which have different advantages and disadvantages. In this paper we first investigate the relation between dendrograms and reachability plots and introduce methods to convert them into each other showing that they essentially contain the same information. Based on reachability plots, we then introduce a technique that automatically determines the significant clusters in a hierarchical cluster representation. This makes it for the first time possible to use hierarchical clustering as an automatic pre-processing step that requires no user interaction to select clusters from a hierarchical cluster representation.
"
7F73B602	Knowledge Discovery and Data Mining	junichi takeuchi + satoshi morinaga + kenji yamanishi	2003	Distributed cooperative mining for information consortia	estimation + information gain +  + probability distribution + Information systems applications + normal distribution + statistical model + financial risk management + Information systems	No keyword found	We consider the situation where a number of agents are distributed and each of them collects a data sequence generated according to an unknown probability distribution. Here each of the distributions is specified by common parameters and individual parameters e.g., a normal distribution with an identical mean and a different variance. Here we introduce a notion of an information consortium, which is a framework where the agents cannot show raw data to one another, but they like to enjoy significant information gain for estimating the respective distributions. Such an information consortium has recently received much interest in a broad range of areas including financial risk management, ubiquitous network mining, etc. In this paper we are concerned with the following three issues: 1) how to design a collaborative strategy for agents to estimate the respective distributions in the information consortium, 2) characterizing when each agent has a benefit in terms of information gain for estimating its distribution or information loss for predicting future data, and 3) charracterizing how much benefit each agent obtains. In this paper we yield a statistical formulation of information consortia and solve all of the above three problems for a general form of probability distributions. Specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit.
NE8592	Knowledge Discovery and Data Mining	Xintao Wu+Daniel BarbarÃ¡+Yong Ye	2003	Screening and interpreting multi-item associations based on log-linear modeling.	 + Information systems applications + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- data mining+statistical database Keywords Association Rule+Log-linear Model+Graphical Model	Association rules have received a lot of attention in the data mining community since their introduction. The classical approach to find rules whose items enjoy high support (appear in a lot of the transactions in the data set) is, however, filled with shortcomings. It has been shown that support can be misleading as an indicator of how interesting the rule is. Alternative measures, such as lift, have been proposed. More recently, a paper by DuMouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that cannot be explained by pairwise associations between the items involved. This approach, however, has its limitations, since it stops short of considering higher order interactions (other than pairwise) among the items. In this paper, we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items. Since fitting loglinear models for large data sets can be computationally prohibitive, we apply graph-theoretical results to divide the original set of items into components (sets of items) that are statistically independent from each other. We then apply loglinear modeling to each of the components and find the interesting associations among items in them. The technique is experimentally evaluated with a real data set (insurance data) and a series of synthetic data sets. The results show that the technique is effective in finding interesting associations among the items involved.
5AF1D7BD	Knowledge Discovery and Data Mining	mariana ciucu + patrick heas + mihai datcu + james c tilton	2003	Scale Space Exploration for Mining Image Information Content	digital elevation model + tree structure + data structure + free energy + information extraction + curse of dimensionality + image classification + synthetic aperture radar + information content + thermodynamics + multispectral images + feature space + data clustering + scale space		"
Images are highly complex multidimensional signals, with rich and complicated information content. For this reason they are difficult to analyze through a unique automated approach. However, a hierarchical representation is helpful for the understanding of image content. In this paper, we describe an application of a scale-space clustering algorithm (melting) for exploration of image information content. Clustering by melting considers the feature space as a thermodynamical ensemble and groups the data by minimizing the free energy, having the temperature as a scale parameter. We develop clustering by melting for multidimensional data, and propose and demonstrate a solution for the initialization of the algorithm. Due to the curse of dimensionality, for initialization of clusters we choose the initial clusters centers with an algorithm that performs a fast cluster centers estimation with low computation cost. We further analyze the information extracted by melting and propose a structure for information representation that enables exploration of image content. This structure is a tree in the scale space showing how the clusters merge. Implementation of the algorithm is through a multi-tree structure. With this structure, we can explore the image content as an information mining function, we obtain a more compact data structure, we have maximum of information in scale space because we memorize the bifurcation points and the trajectories of the centers points in the scale space. The information encoded in the tree structure enables the fast reconstruction and exploration of the data cluster structure and the investigation of hierarchical sequences of image classifications. We demonstrate the effectiveness of the approach with examples using satellite multispectral image (SPOT 4) and Synthetic Aperture Radar - SAR and Digital Elevation Models - DEM derived from SAR interferometry (SRTM).
"
7AD842F5	Knowledge Discovery and Data Mining	menahem friedman + abraham kandel	2003	The data mining approach to automated software testing	functional requirement + regression testing + input output + software testing + data mining + input output analysis + finite element + partial differential equation		"""In todays industry, the design of software tests is mostly based on the testers expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the worlds economy. The costs of the so-called """"catastrophic"""" software failures (such as Mars Polar Lander shutdown in 1999) are even hard to measure. In this paper, we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data. The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications, designing a minimal set of regression tests, and evaluating the correctness of software outputs when testing new, potentially flawed releases of the system. To study the feasibility of the proposed approach, we have applied a novel data mining algorithm called Info-Fuzzy Network (IFN) to execution data of a general-purpose code for solving partial differential equations. After being trained on a relatively small number of randomly generated input-output examples, the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program. Copyright 2003 ACM."""
7F83AF98	Knowledge Discovery and Data Mining	jaideep vaidya + chris clifton	2003	Privacy-preserving k -means clustering over vertically partitioned data	 + k means clustering + Parallel and distributed DBMSs + data mining + Information systems applications + privacy + knowledge discovery + Data mining + Information systems + Database management system engines + Theory and algorithms for application domains + Theory of computation + Database and storage security + Security and privacy + Data management systems + Theory of database privacy and security + Database theory	+Data	Privacy and security concerns can prevent sharing of data, derailing data mining projects. Distributed knowledge discovery, if done correctly, can alleviate this problem. The key is to obtain valid results, while providing guarantees on the (non)disclosure of data. We present a method for k-means clustering when different sites contain different attributes for a common set of entities. Each site learns the cluster of each entity, but learns nothing about the attributes at other sites.
791A7FB5	Knowledge Discovery and Data Mining	constantin f aliferis + alexander statnikov + ioannis tsamardinos	2003	Time and sample efficient discovery of Markov blankets and direct causal relations	 + bayesian network + graph connectivity + data mining + Machine learning + Computing methodologies + variable selection + bayesian networks	+Categories and Subject Descriptors I.2.6 [Artificial Intelligence+Learning Keywords Novel data mining algorithms+robust and scalable statistical methods+Bayesian networks	Data Mining with Bayesian Network learning has two important characteristics: under conditions learned edges between variables correspond to casual influences, and second, for every variable T in the network a special subset (Markov Blanket) identifiable by the network is the minimal variable set required to predict T. However, all known algorithms learning a complete BN do not scale up beyond a few hundred variables. On the other hand, all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region.The contribution of this paper is two-fold. We introduce a novel local algorithm that returns all variables with direct edges to and from a target variable T as well as a local algorithm that returns the Markov Blanket of T. Both algorithms (i) are sound, (ii) can be run efficiently in datasets with thousands of variables, and (iii) significantly outperform in terms of approximating the true neighborhood previous state-of-the-art algorithms using only a fraction of the training size required by the existing methods. A fundamental difference between our approach and existing ones is that the required sample depends on the generating graph connectivity and not the size of the local region; this yields up to exponential savings in sample relative to previously known algorithms. The results presented here are promising not only for discovery of local causal structure, and variable selection for classification, but also for the induction of complete BNs.
792222DC	Knowledge Discovery and Data Mining	caleb c noble + diane j cook	2003	Graph-based anomaly detection	 + data mining + Information systems applications + anomaly detection + Data mining + Information systems	eol>Data mining + anomaly detection + graph regularity	Anomaly detection is an area that has received much attention in recent years. It has a wide variety of applications, including fraud detection and network intrusion detection. A good deal of research has been performed in this area, often using strings or attribute-value data as the medium from which anomalies are to be extracted. Little work, however, has focused on anomaly detection in graph-based data. In this paper, we introduce two techniques for graph-based anomaly detection. In addition, we introduce a new method for calculating the regularity of a graph, with applications to anomaly detection. We hypothesize that these methods will prove useful both for finding anomalies, and for determining the likelihood of successful anomaly detection within graph-based data. We provide experimental results using both real-world network intrusion data and artificially-created data.
7FEB0DA7	Knowledge Discovery and Data Mining	laks v s lakshmanan + yuelong jiang + ke wang	2003	Mining unexpected rules by pushing user dynamics	 + association rule + Information systems applications + Data mining + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data Mining Keywords Association rule	Unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest. In this paper, we study three important issues that have been previously ignored in mining unexpected rules. First, the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario, in addition to the knowledge itself. Second, the prior knowledge should be considered right from the start to focus the search on unexpected rules. Third, the unexpectedness of a rule depends on what other rules the user has seen so far. Thus, only rules that remain unexpected given what the user has seen should be considered interesting. We develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules.
5E8A50CA	Knowledge Discovery and Data Mining	jian chih ou + mingsyan chen + changhung lee	2003	Progressive Weighted Miner: An Efficient Method for Time-Constraint Mining	life cycle + data mining + cumulant + decision analysis + association rule + indexing terms + time variant	Data mining + time-constraint + time-variant + weighted association rules	"
The discovery of association relationship among the data in a huge database has been known to be useful in selective marketing, decision analysis, and business management. A significant amount of research effort has been elaborated upon the development of efficient algorithms for data mining. However, without fully considering the timevariant characteristics of items and transactions, it is noted that some discovered rules may be expired from users' interest. In other words, some discovered knowledge may be obsolete and of little use, especially when we perform the mining schemes on a transaction database of short life cycle products. This aspect is, however, rarely addressed in prior studies. To remedy this, we broaden in this paper the horizon of frequent pattern mining by introducing a weighted model of transaction-weighted association rules in a time-variant database. Specifically, we propose an efficient Progressive Weighted Miner (abbreviatedly as PWM ) algorithm to perform the mining for this problem as well as conduct the corresponding performance studies. In algorithm PWM, the importance of each transaction period is first reflected by a proper weight assigned by the user. Then, PWM partitions the time-variant database in light of weighted periods of transactions and performs weighted mining. Algorithm PWM is designed to progressively accumulate the itemset counts based on the intrinsic partitioning characteristics and employ a filtering threshold in each partition to early prune out those cumulatively infrequent 2-itemsets. With this design, algorithm PWM is able to efficiently produce weighted association rules for applications where different time periods are assigned with different weights and lead to results of more interest.
"
80E4C1EC	Knowledge Discovery and Data Mining	d stott parker + kelvin t leung	2003	Empirical comparisons of various voting methods in bagging	 + data mining + Machine learning + Computing methodologies + voting + generalization error	Figure 1 + Process of Model Reconciliation	Finding effective methods for developing an ensemble of models has been an active research area of large-scale data mining in recent years. Models learned from data are often subject to some degree of uncertainty, for a variety of resoans. In classification, ensembles of models provide a useful means of averaging out error introduced by individual classifiers, hence reducing the generalization error of prediction.The plurality voting method is often chosen for bagging, because of its simplicity of implementation. However, the plurality approach to model reconciliation is ad-hoc. There are many other voting methods to choose from, including the anti-plurality method, the plurality method with elimination, the Borda count method, and Condorcet's method of pairwise comparisons. Any of these could lead to a better method for reconciliation.In this paper, we analyze the use of these voting methods in model reconciliation. We present empirical results comparing performance of these voting methods when applied in bagging. These results include some surprises, and among other things suggest that (1) plurality is not always the best voting method; (2) the number of classes can affect the performance of voting methods; and (3) the degree of dataset noise can affect the performance of voting methods. While it is premature to make final judgments about specific voting methods, the results of this work raise interesting questions, and they open the door to the application of voting theory in classification theory.
7F4374B1	Knowledge Discovery and Data Mining	william w cohen + zhenzhen kou + robert f murphy	2003	Extracting information from text and images for location proteomics	 + optical character recognition + information extraction + search engine + biological data + text mining + fluorescence microscopy + protein localization		"
There is extensive interest in automating the collection, organization and summarization of biological data. Data in the form of figures and accompanying captions in literature present special challenges for such efforts. Based on our previously developed search engines to find fluorescence microscope images depicting protein subcellular patterns, we introduced text mining and Optical Character Recognition (OCR) techniques for caption understanding and figure-text matching, so as to build a robust, comprehensive toolset for extracting information about protein subcellular localization from the text and images found in online journals. Our current system can generate assertions such as âFigure N depicts a localization of type L for protein P in cell type Câ.
"
790B0E80	Knowledge Discovery and Data Mining	gio wiederhold + gloria t lau	2003	Similarity analysis on government regulations	text mining + regulations + knowledge engineering + government regulation	eol>Regulations + Similarity Analysis + Legal Informatics + Text Mining	"
Government regulations are semi-structured text documents that are often voluminous, heavily cross-referenced between provisions and even ambiguous. Multiple sources of regulations, like those from federal, state, and local offices, lead to difficulties in both understanding and complying with all applicable codes. In this work, we propose a framework for regulation management and similarity analysis. An online repository for legal documents is created with the help of text mining tool, and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy based on concepts generated by knowledge engineers. Our similarity analysis core identifies relevant provisions and brings them to the user's attention, and this is performed by utilizing both the structure and referencing of regulations to provide a better comparison between provisions. Preliminary results show that our system reveals hidden similarities between provisions that are not identified using traditional comparison techniques.
"
755CDA12	Knowledge Discovery and Data Mining	zhijun zhan + wenliang du	2003	Using randomized response techniques for privacy-preserving data mining	 + decision tree + security + data mining + decision tree classifier + Information systems applications + privacy + Information systems	Privacy + security + decision tree + data mining	Privacy is an important issue in data mining and knowledge discovery. In this paper, we propose to use the randomized response techniques to conduct the data mining computation. Specially, we present a method to build decision tree classifiers from the disguised data. We conduct experiments to compare the accuracy of our decision tree with the one built from the original undisguised data. Our results show that although the data are disguised, our method can still achieve fairly high accuracy. We also show how the parameter used in the randomized response techniques affects the accuracy of the results.
7B1B13A0	Knowledge Discovery and Data Mining	tom khabaza + gregory piatetskyshapiro + sridhar ramaswamy	2003	Capturing best practice for microarray gene expression data analysis	 + gene selection + microarray data + data mining + Information systems applications + Computing methodologies + microarrays + Information systems + multi class classification + Machine learning + gene expression + best practice + random testing	eol>microarrays + gene expression + data mining process + application template + Clementine	"Analyzing gene expression data from microarray devices has many important application in medicine and biology, but presents significant challenges to data mining. Microarray data typically has many attributes (genes) and few examples (samples), making the process of correctly analyzing such data difficult to formulate and prone to common mistakes. For this reason it is unusually important to capture and record good practices for this form of data mining. This paper presents a process for analyzing microarray data, including pre-processing, gene selection, randomization testing, classification and clustering; this process is captured with ""Clementine Application Templates"". The paper describes the process in detail and includes three case studies, showing how the process is applied to 2-class classification, multi-class classification and clustering analyses for publicly available microarray datasets."
7E9815DE	Knowledge Discovery and Data Mining	daniel barbara + xintao wu + yong ye	2003	Screening and interpreting multi-item associations based on log-linear modeling	data mining + association rule + log linear model + graphical model + higher order + synthetic data + statistical independence		"
Association rules have received a lot of attention in the data mining community since their introduction. The classical approach to nd rules whose items enjoy high support (appear in a lot of the transactions in the data set) is, however, lled with shortcomings. It has been shown that support can be misleading as an indicator of how interesting the rule is. Alternative measures, such as lift, have been proposed. More recently, a paper by DuMouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that cannot be explained by pairwise associations between the items involved. This approach, however, has its limitations, since it stops short of considering higher order interactions (other than pairwise) among the items. In this paper, we propose a method that examines the parameters of the tted loglinear models to nd all the signi cant association patterns among the items. Since tting loglinear models for large data sets can be computationally prohibitive, we apply graph-theoretical results to divide the original set of items into components (sets of items) that are statistically independent from each other. We then apply loglinear modeling to each of the components and nd the interesting associations among items in them. The technique is experimentally evaluated with a real data set (insurance data) and a series of synthetic data sets. The results show that the technique is e ective in nding interesting associations among the items involved.
"
7FEBB071	Knowledge Discovery and Data Mining	tom heskes + alexander ypma	2003	Automatic categorization of web pages and user clustering with mixtures of hidden Markov models	 + web usage mining + em algorithm + web pages + transition probability + data mining + hidden markov model + mixture model	web usage mining + data mining + navigation patterns + automatic categorization + clustering + hidden Markov models + mixture models + user pro ling	"
We propose mixtures of hidden Markov models for modelling clickstreams of web surfers. Hence, the page categorization is learned from the data without the need for a (possibly cumbersome) manual categorization. We provide an EM algorithm for training a mixture of HMMs and show that additional static user data can be incorporated easily to possibly enhance the labelling of users. Furthermore, we use prior knowledge to enhance generalization and avoid numerical problems. We use parameter tying to decrease the danger of over tting and to reduce computational overhead. We put a at prior on the parameters to deal with the problem that certain transitions between page categories occur very seldom or not at all, in order to ensure that a nonzero transition probability between these categories nonetheless remains. In applications to arti cial data and real-world web logs we demonstrate the usefulness of our approach. We train a mixture of HMMs on arti cial navigation patterns, and show that the correct model is being learned. Moreover, we show that the use of static 'satellite data' may enhance the labeling of shorter navigation patterns. When applying a mixture of HMMs to realworld web logs from a large Dutch commercial web site, we demonstrate that sensible page categorizations are being learned.
"
80799A66	Knowledge Discovery and Data Mining	sunita sarawagi + soumen chakrabarti + shantanu godbole	2003	Cross-training: learning probabilistic mappings between topics	semi supervised learning +  + difference set + support vector machine + multi task learning + support vector machines + Machine learning + em + Computing methodologies + text mining	Semi-supervised multi-task learning + Document classi cation + EM + Support Vector Machines	Classification is a well-established operation in text mining. Given a set of labels A and a set DA of training documents tagged with these labels, a classifier learns to assign labels to unlabeled test documents. Suppose we also had available a different set of labels B, together with a set of documents DB marked with labels from B. If A and B have some semantic overlap, can the availability of DB help us build a better classifier for A, and vice versa? We answer this question in the affirmative by proposing cross-training: a new approach to semi-supervised learning in presence of multiple label sets. We give distributional and discriminative algorithms for cross-training and show, through extensive experiments, that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification.
7993B302	Knowledge Discovery and Data Mining	Åule gunduz + m tamer ozsu	2003	A Web page prediction model based on click-stream tree representation of user behavior	prediction model + sequence mining + higher order + sequential pattern mining + markov model + web usage mining + web pages		Predicting the next request of a user as she visits Web pages has gained importance as Web-based activity increases. Markov models and their variations, or models based on sequence mining have been found well suited for this problem. However, higher order Markov models are extremely complicated due to their large number of states whereas lower order Markov models do not capture the entire behavior of a user in a session. The models that are based on sequential pattern mining only consider the frequent sequences in the data set, making it difficult to predict the next request following a page that is not in the sequential pattern. Furthermore, it is hard to find models for mining two different kinds of information of a user session. We propose a new model that considers both the order information of pages in a session and the time spent on them. We cluster user sessions based on their pair-wise similarity and represent the resulting clusters by a click-stream tree. The new user session is then assigned to a cluster based on a similarity measure. The click-stream tree of that cluster is used to generate the recommendation set. The model can be used as part of a cache prefetching system as well as a recommendation model. Copyright 2003 ACM.
80DE0766	Knowledge Discovery and Data Mining	jian pei + jiawei han + jianyong wang	2003	CLOSET+: searching for the best strategies for mining frequent closed itemsets	 + association rule + top down + tree structure + data structure + association rules + bottom up + Information systems applications + Data mining + depth first search + Information systems	+association rules	"Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis. Extensive studies have proposed various strategies for efficient frequent closed itemset mining, such as depth-first search vs. breadthfirst search, vertical formats vs. horizontal formats, tree-structure vs. other data structures, top-down vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask ""what are the pros and cons of the strategies?"" and ""what and how can we pick and integrate the best strategies to achieve higher performance in general cases?""In this study, we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability."
5935279C	Knowledge Discovery and Data Mining	osmar r zaiane + marialuiza antonie + alexandru coman	2003	Associative Classifiers for Medical Images	data mining + association rule mining + association rule + data cleaning + classification system		"
This paper presents two classi cation systems for medical images based on association rule mining. The system we propose consists of: a pre-processing phase, a phase for mining the resulted transactional database, and a nal phase to organize the resulted association rules in a classi cation model. The experimental results show that the method performs well, reaching over 80% in accuracy. Moreover, this paper illustrates how important the data cleaning phase is in building an accurate data mining architecture for image classi cation.
"
80A8C780	Knowledge Discovery and Data Mining	aristides gionis + teija kujala + heikki mannila	2003	Fragments of order	 + Theory of computation + satisfiability + Information systems applications + Data mining + partial order + data collection + Design and analysis of algorithms + Information systems	No keyword found	High-dimensional collections of 0--1 data occur in many applications. The attributes in such data sets are typically considered to be unordered. However, in many cases there is a natural total or partial order âº underlying the variables of the data set. Examples of variables for which such orders exist include terms in documents, courses in enrollment data, and paleontological sites in fossil data collections. The observations in such applications are flat, unordered sets; however, the data sets respect the underlying ordering of the variables. By this we mean that if A âº B âº C are three variables respecting the underlying ordering âº, and both of variables A and C appear in an observation, then, up to noise levels, variable B also appears in this observation. Similarly, if A1 âº A2 âº â¦ âº Al-1 âº Ai is a longer sequence of variables, we do not expect to see many observations for which there are indices i < j < k such that Ai and Ak occur in the observation but Aj does not.In this paper we study the problem of discovering fragments of orders of variables implicit in collections of unordered observations. We define measures that capture how well a given order agrees with the observed data. We describe a simple and efficient algorithm for finding all the fragments that satisfy certain conditions. We also discuss the sometimes necessary postprocessing for selecting only the best fragments of order. Also, we relate our method with a sequencing approach that uses a spectral algorithm, and with the consecutive ones problem. We present experimental results on some real data sets (author lists of database papers, exam results data, and paleontological data).
7E3B444C	Knowledge Discovery and Data Mining	yong ye + kalpathi r subramanian + xintao wu	2003	Interactive Analysis of Gene Interactions Using Graphical gaussian model	dna microarray + association rule mining + microarray data		"
DNA microarray provides a powerful basis for analysis of gene expression. Data mining methods such as clustering have been widely applied to microarray data to link genes that show similar expression patterns. However, this approach usually fails to unveil gene-gene interactions in the same cluster. Association rule mining and loglinear models have been used for this purpose, but their inherent limitations as well as information loss due to discretization limit the applicability of the results. Here we propose the use of a Graphical Gaussian Model to discover pairwise gene interactions. We have constructed a prototype system that permits rapid interactive exploration of gene relationships; results can be validated by experts or known information, or suggest new experiments. We have tested our methodology using the yeast microarray data. Our results reveal some previously unknown interactions that have solid biological explanations.
"
06A87C34	Knowledge Discovery and Data Mining	david w cheung + kevin y yip + michael k ng	2003	A highly-usable projected clustering algorithm for gene expression profiles	 + high dimensional data + gene expression		"
Projected clustering has become a hot research topic due to its ability to cluster high-dimensional data. However, most existing projected clustering algorithms depend on some critical user parameters in determining the relevant attributes of each cluster. In case wrong parameter values are used, the clustering performance will be seriously degraded. Unfortunately, correct parameter values are rarely known in real datasets. In this paper, we propose a projected clustering algorithm that does not depend on user inputs in determining relevant attributes. It responds to the clustering status and adjusts the internal thresholds dynamically. From experimental results, our algorithm shows a much higher usability than the other projected clustering algorithms used in our comparison study. It also works well with a gene expression dataset for studying lymphoma. The high usability of the algorithm and the encouraging results suggest that projected clustering can be a practical tool for analyzing gene expression pro les.
"
7BDC34F8	Knowledge Discovery and Data Mining	foster provost + claudia perlich	2003	Aggregation-based feature invention and relational concept classes	relational data +  + Machine learning + Computing methodologies + aggregation + relational learning + feature vector	No keyword found	Model induction from relational data requires aggregation of the values of attributes of related entities. This paper makes three contributions to the study of relational learning. (1) It presents a hierarchy of relational concepts of increasing complexity, using relational schema characteristics such as cardinality, and derives classes of aggregation operators that are needed to learn these concepts. (2) Expanding one level of the hierarchy, it introduces new aggregation operators that model the distributions of the values to be aggregated and (for classification problems) the differences in these distributions by class. (3) It demonstrates empirically on a noisy business domain that more-complex aggregation methods can increase generalization performance. Constructing features using target-dependent aggregations can transform relational prediction tasks so that well-understood feature-vector-based modeling algorithms can be applied successfully.
7C5B6718	Knowledge Discovery and Data Mining	joao gama + pedro medas + ricardo rocha	2003	Accurate decision trees for mining high-speed data streams	 + data streams + Learning paradigms + Information systems applications + Computing methodologies + Data mining + Information systems + decision tree + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + time constant + Supervised learning by classification	+Categories and Subject Descriptors H.2.8 [Database Management+Database applicationsâ data mining+I.2.6 [Artificial Intelligence+Learningâ classifiers design and evaluation Keywords Data Streams+Incremental Decision Trees+Functional Leaves	In this paper we study the problem of constructing accurate decision tree models from data streams. Data streams are incremental tasks that require incremental, online, and any-time learning algorithms. One of the most successful algorithms for mining data streams is VFDT. In this paper we extend the VFDT system in two directions: the ability to deal with continuous data and the use of more powerful classification techniques at tree leaves. The proposed system, VFDTc, can incorporate and classify new information online, with a single scan of the data, in time constant per example. The most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets. This is relevant due to the any-time property. We study the behaviour of VFDTc in different problems and demonstrate its utility in large and medium data sets. Under a bias-variance analysis we observe that VFDTc in comparison to C4.5 is able to reduce the variance component.
80EDD50B	Knowledge Discovery and Data Mining	huan liu + lei yu	2003	Efficiently handling feature redundancy in high-dimensional data	 + Feature selection + Machine learning algorithms + data mining + Information systems applications + Computing methodologies + empirical study + Data mining + Information systems + high dimensional data + Machine learning + redundancy + feature selection	Feature selection + redundancy + high-dimensional data	High-dimensional data poses a severe challenge for data mining. Feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining. Traditionally, feature selection is focused on removing irrelevant features. However, for high-dimensional data, removing redundant features is equally critical. In this paper, we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model. The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features.
7D513D28	Knowledge Discovery and Data Mining	won suk lee + joong hyuk chang	2003	Finding recent frequent itemsets adaptively over online data streams	 + data mining + Information systems applications + Information systems	eol>Recent frequent itemsets + Data stream + Decay mechanism + Delayed-insertion + Pruning of itemsets	A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate. Consequently, the knowledge embedded in a data stream is more likely to be changed as time goes by. Identifying the recent change of a data stream, specially for an online data stream, can provide valuable information for the analysis of the data stream. In addition, monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge. However, most of mining algorithms over a data stream do not differentiate the information of recently generated transactions from the obsolete information of old transactions which may be no longer useful or possibly invalid at present. This paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream. The effect of old transactions on the mining result of the data steam is diminished by decaying the old occurrences of each itemset as time goes by. Furthermore, several optimization techniques are devised to minimize processing time as well as main memory usage. Finally, the proposed method is analyzed by a series of experiments.
76C59885	Knowledge Discovery and Data Mining	uwe f mayer + armand sarkissian	2003	Experimental design for solicitation campaigns	 + data mining + experimental design + statistical model + Probability and statistics + Mathematics of computing + data collection	eol>Experimental Design + solicitation campaign + data collection	Data mining techniques are routinely used by fundraisers to select those prospects from a large pool of candidates who are most likely to make a financial contribution. These techniques often rely on statistical models based on trial performance data. This trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool. Collecting this trial data involves a cost; therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospects.We describe an experimental design approach to optimally choose the trial prospects from an existing large pool of prospects. Prospects are clustered to render the problem practically tractable. We modify the standard D-optimality algorithm to prevent repeated selection of the same prospect cluster, since each prospect can only be solicited at most once.We assess the benefits of this approach on the KDD-98 data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size.
7CDBA7E0	Knowledge Discovery and Data Mining	feng tao + mohsen m farid + fionn murtagh	2003	Weighted Association Rule Mining using weighted support and significance framework	 + Information systems applications + Data mining + association rule mining + Information systems	eol>Weighted Association Rule Mining + Weighted Support + Significant relationship + weighted downward closure property + WARM algorithm	"We address the issues of discovering significant binary relationships in transaction datasets in a weighted setting. Traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight. The goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatornal explosion of insignificant relationships. We identify the challenge of using weights in the iterative process of generating large itemsets. The problem of invalidation of the ""downward closure property"" in the weighted setting is solved by using an improved model of weighted support measurements and exploiting a ""weighted downward closure property"". A new algorithm called WARM (Weighted Association Rule Mining) is developed based on the improved model. The algorithm is both scalable and efficient in discovering significant relationships in weighted settings as illustrated by experiments performed on simulated datasets."
7EA1DCA7	Knowledge Discovery and Data Mining	mark schwabacher + stephen d bay	2003	Mining distance-based outliers in near linear time with randomization and a simple pruning rule	 + algorithms + data mining + computer networks + random processes + Information systems applications + linear systems + anomaly detection + outliers + Data mining + outliers statistics + Information systems + nested loops + high dimensional data + hilbert space + data bases + linear time	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ data mining Keywords Outliers+distance-based operations+anomaly detection+disk- based	Defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set. Recently, much work has been conducted with the goal of finding fast algorithms for this task. We show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used. We test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude. Our average case analysis suggests that much of the efficiency is because the time to process non-outliers, which are the majority of examples, does not depend on the size of the data set.
7C860987	Knowledge Discovery and Data Mining	marin saric + sterling stuart stein + shlomo argamon	2003	Style mining of electronic messages for multiple authorship discrimination: first results	computer and information science + knowledge management + text mining + machine learning + information retrieval	Text mining + Text categorization + Authorship attribution + Computational stylistics + Electronic communication	"
This paper considers the use of computational stylistics for performing authorship attribution of electronic messages, addressing categorization problems with as many as 20 different classes (authors). E ective stylistic characterization of text is potentially useful for a variety of tasks, as language style contains cues regarding the authorship, purpose, and mood of the text, all of which would be useful adjuncts to information retrieval or knowledge-management tasks. We focus here on the problem of determining the author of an anonymous message, based only on the message text. Several multiclass variants of the Winnow algorithm were applied to a vector representation of the message texts to learn models for discriminating di erent authors. We present results comparing the classi cation accuracy of the di erent approaches. The results show that stylistic models can be accurately learned to determine an author's identity.
"
7DA01A16	Knowledge Discovery and Data Mining	michalis vazirgiannis + magdalini eirinaki + iraklis varlamis	2003	SEWeP: using site semantics and a taxonomy to enhance the Web personalization process	 + web usage mining + web personalization + Collaborative and social computing systems and tools + Collaborative and social computing + web mining + Information systems applications + Human-centered computing + World Wide Web + Information systems	eol>Web personalization + Web Mining + Semantic Annotation of Web Content	Web personalization is the process of customizing a Web site to the needs of each specific user or set of users, taking advantage of the knowledge acquired through the analysis of the user's navigational behavior. Integrating usage data with content, structure or user profile data enhances the results of the personalization process. In this paper, we present SEWeP, a system that makes use of both the usage logs and the semantics of a Web site's content in order to personalize it. Web content is semantically annotated using a conceptual hierarchy (taxonomy). We introduce C-logs, an extended form of Web usage logs that encapsulates knowledge derived from the link semantics. C-logs are used as input to the Web usage mining process, resulting in a broader yet semantically focused set of recommendations.
8102FB26	Knowledge Discovery and Data Mining	raymond j mooney + mikhail bilenko	2003	Adaptive duplicate detection using learnable string similarity measures	 + distance metric + distance function + record linkage + support vector machine + data cleaning + Machine learning + Information systems applications + Computing methodologies + data integrity + vector space + Information systems	+ilarity measures+string edit distance	The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. In this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. We propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain. We present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector-space based measure that employs a Support Vector Machine (SVM) for training. Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques.
7AB2899D	Knowledge Discovery and Data Mining	steven p ketchpel + kamal ali	2003	Golden Path Analyzer: using divide-and-conquer to cluster Web clickstreams	 + work in process + web pages + web mining + Machine learning + Computing methodologies + divide and conquer + clustering	+Categories and Subject Descriptors I.2.6 [Artifical Intelligence+Learning General Terms Algorithms. Keywords Web-mining+clustering+divide-and-conquer	This paper describes a novel algorithm and deployed system Golden Path Analyzer (GPA) that analyzes clickstreams of people trying to complete the same task on a website. It finds the shortest, successful paths taken by users - 'golden paths' - and uses these as seeds for clickstream clusters. Other users are assigned to a cluster if their clickstream is a supersequence of the golden path. The advantages of this approach are that the resulting clusters are easily comprehended, they are few in number, correspond to semantically different strategies used by the users, and jointly partition all the clickstreams. GPA's key contribution over prior work in process funnels is that by not excluding users that make diversions from the golden path, GPA is able to assign more users to fewer clusters. Another key contribution is to use actual full clickstreams as cluster seeds to which supersequences of other users are added. Golden paths correspond to complete clickstreams that are based on actual user page transitions. GPA is particularly useful for site designers to improve processes such as shopping, returns and registration. Its analyses identify which web pages cause many users to deviate from a golden path, which links distract users and the percentage of users taking each golden path. GPA has demonstrated value on more than twenty client projects in diverse industries.
794470A6	Knowledge Discovery and Data Mining	dimitrios gunopulos + marios hadjieleftheriou + eamonn keogh + michail vlachos	2003	Indexing multi-dimensional time-series with support for multiple distance measures	 + dynamic time warping + distance function + longest common subsequence + trajectories + indexation + Information systems applications + time series + euclidean distance + Information systems	No keyword found	Although most time-series data mining research has concentrated on providing solutions for a single distance function, in this work we motivate the need for a single index structure that can support multiple distance measures. Our specific area of interest is the efficient retrieval and analysis of trajectory similarities. Trajectory datasets are very common in environmental applications, mobility experiments, video surveillance and are especially important for the discovery of certain biological patterns. Our primary similarity measure is based on the Longest Common Subsequence (LCSS) model, that offers enhanced robustness, particularly for noisy data, which are encountered very often in real world applications. However, our index is able to accommodate other distance measures as well, including the ubiquitous Euclidean distance, and the increasingly popular Dynamic Time Warping (DTW). While other researchers have advocated one or other of these similarity measures, a major contribution of our work is the ability to support all these measures without the need to restructure the index. Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall. The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW.
59614199	Knowledge Discovery and Data Mining	junghwan oh + sanjaykumar kote + jeongkyu lee	2003	Real time video data mining for surveillance video streams	real time + data mining		"
We extend our previous work [1] of the general framework for video data mining to further address the issue such as how to mine video data. To extract motions, we use an accumulation of quantized pixel differences among all frames in a video segment. As a result, the accumulated motions of segment are represented as a two dimensional matrix. Further, we develop how to capture the location of motions occurring in a segment using the same matrix generated for the calculation of the amount. We study how to cluster those segmented pieces using the features (the amount and the location of motions) we extract by the matrix above. We investigate an algorithm to find whether a segment has normal or abnormal events by clustering and modeling normal events, which occur mostly. In addition to deciding normal or abnormal, the algorithm computes Degree of Abnormality of a segment, which represents to what extent a segment is distant to the existing segments in relation with normal events. Our experimental studies indicate that the proposed techniques are promising.
"
7A8700D1	Knowledge Discovery and Data Mining	brian kulis + bart selman + john e hopcroft + omar khan	2003	Natural communities in large linked networks	 + community structure + Information retrieval + stability + Information systems	natural communities + large linked networks + hierarchical agglomerative clustering + stability	We are interested in finding natural communities in large-scale linked networks. Our ultimate goal is to track changes over time in such communities. For such temporal tracking, we require a clustering algorithm that is relatively stable under small perturbations of the input data. We have developed an efficient, scalable agglomerative strategy and applied it to the citation graph of the NEC CiteSeer database (250,000 papers; 4.5 million citations). Agglomerative clustering techniques are known to be unstable on data in which the community structure is not strong. We find that some communities are essentially random and thus unstable while others are natural and will appear in most clusterings. These natural communities will enable us to track the evolution of communities over time.
8151588E	Knowledge Discovery and Data Mining	robert f murphy + richard wang + william w cohen	2003	Understanding captions in biomedical publications	 + information extraction + scientific knowledge + boosting + bioinformatics + Machine learning + Computing methodologies + Information retrieval + Information systems	No keyword found	"From the standpoint of the automated extraction of scientific knowledge, an important but little-studied part of scientific publications are the figures and accompanying captions. Captions are dense in information, but also contain many extra-grammatical constructs, making them awkward to process with standard information extraction methods. We propose a scheme for ""understanding"" captions in biomedical publications by extracting and classifying ""image pointers"" (references to the accompanying image). We evaluate a number of automated methods for this task, including hand-coded methods, methods based on existing learning techniques, and methods based on novel learning techniques. The best of these methods leads to a usefully accurate tool for caption-understanding, with both recall and precision in excess of 94% on the most important single class in a combined extraction/classification task."
75970BB5	Knowledge Discovery and Data Mining	yunyue zhu + dennis shasha	2003	Efficient elastic burst detection in data streams	 + data structure + Information systems applications + gamma ray burst + sliding window + Data mining + real time + linear time + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data Mining Keywords data	Burst detection is the activity of finding abnormal aggregates in data streams. Such aggregates are based on sliding windows over data streams. In some applications, we want to monitor many sliding window sizes simultaneously and to report those windows with aggregates significantly different from other periods. We will present a general data structure for detecting interesting aggregates over such elastic windows in near linear time. We present applications of the algorithm for detecting Gamma Ray Bursts in large-scale astrophysical data. Detection of periods with high volumes of trading activities and high stock price volatility is also demonstrated using real time Trade and Quote (TAQ) data from the New York Stock Exchange (NYSE). Our algorithm beats the direct computation approach by several orders of magnitude.
7E921AC3	Knowledge Discovery and Data Mining	joydeep ghosh + inderjit s dhillon + suvrit sra + arindam banerjee	2003	Generative model-based clustering of directional data	 + Cluster analysis + information retrieval + Learning paradigms + em + Computing methodologies + gaussian distribution + Information retrieval + mixtures + Unsupervised learning + Information systems + Machine learning + expectation maximization + clustering	+directional data+mixtures+von Mises-Fisher+EM	High dimensional directional data is becoming increasingly important in contemporary applications such as analysis of text and gene-expression data. A natural model for multi-variate directional data is provided by the von Mises-Fisher (vMF) distribution on the unit hypersphere that is analogous to the multi-variate Gaussian distribution in Rd. In this paper, we propose modeling complex directional data as a mixture of vMF distributions. We derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the parameters of this mixture. We also propose two clustering algorithms corresponding to these variants. An interesting aspect of our methodology is that the spherical kmeans algorithm (kmeans with cosine similarity) can be shown to be a special case of both our algorithms. Thus, modeling text data by vMF distributions lends theoretical validity to the use of cosine similarity which has been widely used by the information retrieval community. As part of experimental validation, we present results on modeling high-dimensional text and gene-expression data as a mixture of vMF distributions. The results indicate that our approach yields superior clusterings especially for difficult clustering tasks in high-dimensional spaces.
7E8A6B45	Knowledge Discovery and Data Mining	dharmendra s modha + subramanyam mallela + inderjit s dhillon	2003	Information-theoretic co-clustering	 + Cluster analysis + optimization problem + data analysis + document clustering + co clustering + Learning paradigms + random variable + Computing methodologies + Information retrieval + Mathematical foundations of cryptography + Unsupervised learning + Information systems + Coding theory + Security and privacy + Machine learning + probability distribution + Probability and statistics + mutual information + information theory + Mathematics of computing + Cryptography + contingency table + Information theory	+Categories and Subject Descriptors E.4 [Coding and Information Theory+Data compaction and compression+G.3 [Probability and Statistics+Con- tingency table analysis+H.3.3 [Information Search and Retrieval+Clustering+I.5.3 [Pattern Recognition+Clus- tering Keywords	Two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text, web-log and market-basket data analysis. A basic problem in contingency table analysis is co-clustering: simultaneous clustering of the rows and columns. A novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in information theory---the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters. We present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages. Using the practical example of simultaneous word-document clustering, we demonstrate that our algorithm works well in practice, especially in the presence of sparsity and high-dimensionality.
7B34A435	Knowledge Discovery and Data Mining	darya chudova + eric mjolsness + scott gaffney + padhraic smyth	2003	Translation-invariant mixture models for curve clustering	discrete time +  + em algorithm + Machine learning + em + Computing methodologies + expectation maximization + mixture model	+Categories and Subject Descriptors I.2.6 [Artificial Intelligence+Learning Keywords curve clustering+alignment+transformation invariance+EM+mixture	In this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves defined on a discrete time grid. Our approach uses the Expectation-Maximization (EM) algorithm to recover both the mean curve shapes for each cluster, and the most likely shifts, offsets, and cluster memberships for each curve. We demonstrate how Bayesian estimation methods can improve the results for small sample sizes by enforcing smoothness in the cluster mean curves. We evaluate the methodology on two real-world data sets, time-course gene expression data and storm trajectory data. Experimental results show that models that incorporate curve alignment systematically provide improvements in predictive power and within-cluster variance on test data sets. The proposed approach provides a non-parametric, computationally efficient, and robust methodology for clustering broad classes of curve data.
58510E87	Knowledge Discovery and Data Mining	jeffrey xu yu + li wei + weining qian + aoying zhou + wen jin	2003	HOT: hypergraph-based outlier test for categorical data	 + distance metric + database system + high dimensional data + curse of dimensionality + data mining + categorical data + very large database + outlier detection + missing values	eol>Outlier + Hypergraph + High-dimensional data	"
As a widely used data mining technique, outlier detection is a process which aims to find anomalies while providing good explanations. Most existing detection methods are basically designed for numeric data, however, real-life data such as web pages, business transactions and bioinformatics records always contain categorical data. So it causes difficulty to find reasonable exceptions in the real world applications. In this paper, we introduce a novel outlier mining method based on hypergraph model for categorical data. Since hypergraphs precisely capture the distribution characteristics in data subspaces, this method is effective in identifying anomalies in dense subspaces and presents good interpretations for the local outlierness. By selecting the most relevant subspaces, the problem of âcurse of dimensionalityâ in very large databases can also be ameliorated. Furthermore, the connectivity property is used to replace the distance metrics, so that the distance-based computation is not needed anymore, which enhances the robustness for handling missing-value data. The fact that connectivity computation facilitates the aggregation operations supported by most SQL-compatible database systems, makes the mining process much efficient. Finally, we give experiments and analysis which show that our method can find outliers in categorical data with good performance and quality. ÂThe work is partially supported by the National Grand Fundamental Research 973 Program of China under Grant No. G1998030414 and the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 99038 yThe author is partially supported by Microsoft Research Fellowship.
"
7692E5BA	Knowledge Discovery and Data Mining	bing liu + lan yi + xiaoli li	2003	Eliminating noisy information in Web pages for data mining	 + Retrieval models and ranking + web pages + tree structure + web mining + Retrieval tasks and goals + data mining + Information systems applications + Clustering and classification + Information retrieval + Information extraction + Clustering + Data mining + Information systems + Document filtering	eol>Noise detection + noise elimination + Web mining	A commercial Web page typically contains many information blocks. Apart from the main content blocks, it usually has such blocks as navigation panels, copyright and privacy notices, and advertisements (for business purposes and for easy user access). We call these blocks that are not the main content blocks of the page the noisy blocks. We show that the information contained in these noisy blocks can seriously harm Web data mining. Eliminating these noises is thus of great importance. In this paper, we propose a noise elimination technique based on the following observation: In a given Web site, noisy blocks usually share some common contents and presentation styles, while the main content blocks of the pages are often diverse in their actual contents and/or presentation styles. Based on this observation, we propose a tree structure, called Style Tree, to capture the common presentation styles and the actual contents of the pages in a given Web site. By sampling the pages of the site, a Style Tree can be built for the site, which we call the Site Style Tree (SST). We then introduce an information based measure to determine which parts of the SST represent noises and which parts represent the main contents of the site. The SST is employed to detect and eliminate noises in any Web page of the site by mapping this page to the SST. The proposed technique is evaluated with two data mining tasks, Web page clustering and classification. Experimental results show that our noise elimination technique is able to improve the mining results significantly.
7FDD4D2E	Knowledge Discovery and Data Mining	xiaofang zhou + xingzhi sun + maria e orlowska	2003	Finding event-oriented patterns in long temporal sequences	 + data mining + sequence mining		"
A major task of traditional temporal event sequence mining is to nd all frequent event patterns from a long temporal sequence. In many real applications, however, events are often grouped into di erent types, and not all types are of equal importance. In this paper, we consider the problem of e cient mining of temporal event sequences which lead to an instance of a speci c type of event. Temporal constraints are used to ensure sensibility of the mining results. We will rst generalise and formalise the problem of event-oriented temporal sequence data mining. After discussing some unique issues in this new problem, we give a set of criteria, which are adapted from traditional data mining techniques, to measure the quality of patterns to be discovered. Finally we present an algorithm to discover potentially interesting patterns.
"
7645192B	Knowledge Discovery and Data Mining	hua li + rafael alonso + chumki basu + jeffrey a bloom	2003	An adaptive nearest neighbor search for a parts acquisition ePortal	 + nearest neighbor + feature space + nearest neighbor search + Machine learning + query by example + Information systems applications + Computing methodologies + Information retrieval + Information systems	+General Terms Algorithms+Management+Measurement+Performance+Design+Experimentation. Keywords Adaptive Search+k-Nearest Neighbor classification+User Profiling+Query by Example	"One of the major hurdles in maintaining long-lived electronic systems is that electronic parts become obsolete, no longer available from the original suppliers. When this occurs, an engineer is tasked with resolving the problem by finding a replacement that is ""as similar as possible"" to the original part. The current approach involves a laborious manual search through several electronic portals and data books. The search is difficult because potential replacements may differ from the original and from each other by one or more parameters. Worse still, the cumbersome nature of this process may cause the engineers to miss appropriate solutions amid the many thousands of parts listed in industry catalogs.In this paper, we address this problem by introducing the notion of a parametric ""distance"" between electronic components. We use this distance to search a large parts data set and recommend likely replacements. Recommendations are based on an adaptive nearest-neighbor search through the parametric data set. For each user, we learn how to scale the axes of the feature space in which the nearest neighbors are sought. This allows the system to learn each user's judgment of the phrase ""as similar as possible."""
7B320EC3	Knowledge Discovery and Data Mining	hideto yokoi + katsuhiko takabayashi + tu bao ho + dung duc nguyen + trong dung nguyen + saori kawasaki + si quang le	2003	Mining hepatitis data with temporal abstraction	 + data analysis + temporal database + Information systems applications + machine learning + Information systems	eol>Hepatitis data + medicaldata mining + temporal abstraction	The hepatitis temporal database collected at Chiba university hospital between 1982--2001 was recently given to challenge the KDD research. The database is large where each patient corresponds to 983 tests represented as sequences of irregular timestamp points with different lengths. This paper presents a temporal abstraction approach to mining knowledge from this hepatitis database. Exploiting hepatitis background knowledge and data analysis, we introduce new notions and methods for abstracting short-term changed and long-term changed tests. The abstracted data allow us to apply different machine learning methods for finding knowledge part of which is considered as new and interesting by medical doctors.
7C45C5BA	Knowledge Discovery and Data Mining	david jensen + hannah blau + matthew rattigan	2003	Information awareness: a prospective technical assessment	national security +  + technology assessment + data mining + simulation model + social network analysis + Information systems applications + privacy + Information systems	eol>Information awareness + relational data mining + social network analysis + ranking classifiers + iterative classification + collective classification + TIA + privacy + technology assessment	Recent proposals to apply data mining systems to problems in law enforcement, national security, and fraud detection have attracted both media attention and technical critiques of their expected accuracy and impact on privacy. Unfortunately, the majority of technical critiques have been based on simplistic assumptions about data, classifiers, inference procedures, and the overall architecture of such systems. We consider these critiques in detail, and we construct a simulation model that more closely matches realistic systems. We show how both the accuracy and privacy impact of a hypothetical system could be substantially improved, and we discuss the necessary and sufficient conditions for this improvement to be achieved. This analysis is neither a defense nor a critique of any particular system concept. Rather, our model suggests alternative technical designs that could mitigate some concerns, but also raises more specific conditions that must be met for such systems to be both accurate and socially desirable.
7B5EE6A2	Knowledge Discovery and Data Mining	ruoming jin + gagan agrawal	2003	Efficient decision tree construction on streaming data	 + decision tree + data mining + sampling + Machine learning + Information systems applications + Computing methodologies + sample size + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- Data Min- ing+I.2.6 [Artificial Intelligence+Learning Keywords Streaming Data+Decision Tree+Sampling	Decision tree construction is a well studied problem in data mining. Recently, there has been much interest in mining streaming data. Domingos and Hulten have presented a one-pass algorithm for decision tree construction. Their work uses Hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed.In this paper, we revisit this problem. We make the following two contributions: 1) We present a numerical interval pruning (NIP) approach for efficiently processing numerical attributes. Our results show an average of 39% reduction in execution times. 2) We exploit the properties of the gain function entropy (and gini) to reduce the sample size required for obtaining a given bound on the accuracy. Our experimental results show a 37% reduction in the number of data instances required.
8152391E	Knowledge Discovery and Data Mining	steven a klooster + christopher potter + vipin kumar + michael steinbach + pangning tan	2003	Discovery of climate indices using clustering	Cluster analysis +  + eigenvalues + Learning paradigms + Computing methodologies + time series + scientific data + Unsupervised learning + Machine learning + clustering + principal component analysis + singular value decomposition	+I.5.3 [Pattern Recognition+Clustering+I.5.4 [Pattern Recognition+ApplicationsâClimate Keywords clustering+singular value decomposition+time series+Earth science data+mining scientific data	To analyze the effect of the oceans and atmosphere on land climate, Earth Scientists have developed climate indices, which are time series that summarize the behavior of selected regions of the Earth's oceans and atmosphere. In the past, Earth scientists have used observation and, more recently, eigenvalue analysis techniques, such as principal components analysis (PCA) and singular value decomposition (SVD), to discover climate indices. However, eigenvalue techniques are only useful for finding a few of the strongest signals. Furthermore, they impose a condition that all discovered signals must be orthogonal to each other, making it difficult to attach a physical interpretation to them. This paper presents an alternative clustering-based methodology for the discovery of climate indices that overcomes these limitiations and is based on clusters that represent regions with relatively homogeneous behavior. The centroids of these clusters are time series that summarize the behavior of the ocean or atmosphere in those regions. Some of these centroids correspond to known climate indices and provide a validation of our methodology; other centroids are variants of known indices that may provide better predictive power for some land areas; and still other indices may represent potentially new Earth science phenomena. Finally, we show that cluster based indices generally outperform SVD derived indices, both in terms of area weighted correlation and direct correlation with the known indices.
76838D85	Knowledge Discovery and Data Mining	christopher jermaine	2003	Playing hide-and-seek with correlations	minimum cut +  + association rule + human interaction + data mining + association rules + Information systems applications + Information systems	eol>Data mining + correlations + association rules + minimum cut	"We present a method for very high-dimensional correlation analysis. The method relies equally on rigorous search strategies and on human interaction. At each step, the method conservatively ""shaves off"" a fraction of the database tuples and attributes, so that most of the correlations present in the data are not affected by the decomposition. Instead, the correlations become more obvious to the user, because they are hidden in a much smaller portion of the database. This process can be repeated iteratively and interactively, until only the most important correlations remain.The main technical difficulty of the approach is figuring out how to ""shave off"" part of the database so as to preserve most correlations. We develop an algorithm for this problem that has a polynomial running time and guarantees result quality."
779CBF34	Knowledge Discovery and Data Mining	stefano lonardi + eamonn keogh + bill chiu	2003	Probabilistic discovery of time series motifs	 + randomized algorithms + data mining + Information systems applications + time series + randomized algorithm + Information systems	eol>Time Series + Data Mining + Motifs + Randomized Algorithms	"Several important time series data mining problems reduce to the core task of finding approximately repeated subsequences in a longer time series. In an earlier work, we formalized the idea of approximately repeated subsequences by introducing the notion of time series motifs. Two limitations of this work were the poor scalability of the motif discovery algorithm, and the inability to discover motifs in the presence of noise.Here we address these limitations by introducing a novel algorithm inspired by recent advances in the problem of pattern discovery in biosequences. Our algorithm is probabilistic in nature, but as we show empirically and theoretically, it can find time series motifs with very high probability even in the presence of noise or ""don't care"" symbols. Not only is the algorithm fast, but it is an anytime algorithm, producing likely candidate motifs almost immediately, and gradually improving the quality of results over time."
750F00A0	Knowledge Discovery and Data Mining	shubir kapoor + sholom m weiss + stephen j buckley + soren damgaard	2003	Knowledge-based data mining	knowledge base +  + expert system + expert systems + data mining + Information systems applications + machine learning + knowledge systems + decision rule + Information systems	decision rule induction + expert systems + sales leads	We describe techniques for combining two types of knowledge systems: expert and machine learning. Both the expert system and the learning system represent information by logical decision rules or trees. Unlike the classical views of knowledge-base evaluation or refinement, our view accepts the contents of the knowledge base as completely correct. The knowledge base and the results of its stored cases will provide direction for the discovery of new relationships in the form of newly induced decision rules. An expert system called SEAS was built to discover sales leads for computer products and solutions. The system interviews executives by asking questions, and based on the responses, recommends products that may improve a business' operations. Leveraging this expert system, we record the results of the interviews and the program's recommendations. The very same data stored by the expert system is used to find new predictive rules. Among the potential advantages of this approach are (a) the capability to spot new sales trends and (b) the substitution of less expensive probabilistic rules that use database data instead of interviews.
75206CE0	Knowledge Discovery and Data Mining	jiawei han + jiong yang + hwanjo yu	2003	Classifying large data sets using SVMs with hierarchical clusters	 + support vector machines + data mining + regression analysis + Learning paradigms + Computing methodologies + hierarchical cluster + machine learning + support vector machine + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + hierarchical clustering + Supervised learning by classification + pattern recognition	+hierarchical cluster	Support vector machines (SVMs) have been promising methods for classification and regression analysis because of their solid mathematical foundations which convery several salient properties that other methods hardly provide. However, despite the prominent properties of SVMs, they are not as favored for large-scale data mining as for pattern recognition or machine learning because the training complexity of SVMs is highly dependent on the size of a data set. Many real-world data mining applications involve millions or billions of data records where even multiple scans of the entire data are too expensive to perform. This paper presents a new method, Clustering-Based SVM (CB-SVM), which is specifically designed for handling very large data sets. CB-SVM applies a hierarchical micro-clustering algorithm that scans the entire data set only once to provide an SVM with high quality samples that carry the statistical summaries of the data such that the summaries maximize the benefit of learning the SVM. CB-SVM tries to generate the best SVM boundary for very large data sets given limited amount of resources. Our experiments on synthetic and real data sets show that CB-SVM is highly scalable for very large data sets while also generating high classification accuracy.
7FD73356	Knowledge Discovery and Data Mining	srinivasan parthasarathy + matthew eric otey + gangmin li + dhabaleswar k panda + s narravula + amol ghoting	2003	Towards NIC-based intrusion detection	 + network security + next generation network + data mining + Learning paradigms + Computing methodologies + intrusion detection + anomaly detection + Supervised learning + Classification and regression trees + network interface card + Machine learning + Machine learning approaches + Supervised learning by classification	+trusion detection+data mining Categories and Subject Descriptors I.5.2 [Pattern Recognition+Design Methodology- classifier design and evaluation	We present and evaluate a NIC-based network intrusion detection system. Intrusion detection at the NIC makes the system potentially tamper-proof and is naturally extensible to work in a distributed setting. Simple anomaly detection and signature detection based models have been implemented on the NIC firmware, which has its own processor and memory. We empirically evaluate such systems from the perspective of quality and performance (bandwidth of acceptable messages) under varying conditions of host load. The preliminary results we obtain are very encouraging and lead us to believe that such NIC-based security schemes could very well be a crucial part of next generation network security systems.
758DB577	Knowledge Discovery and Data Mining	ananth grama + mehmet koyuturk	2003	PROXIMUS: a framework for analyzing very high dimensional discrete-attributed datasets	matrix decomposition + data analysis + association rule mining		This paper presents an efficient framework for error-bounded compression of high-dimensional discrete attributed datasets. Such datasets, which frequently arise in a wide variety of applications, pose some of the most significant challenges in data analysis. Subsampling and compression are two key technologies for analyzing these datasets. PROXIMUS provides a technique for reducing large datasets into a much smaller set of representative patterns, on which traditional (expensive) analysis algorithms can be applied with minimal loss of accuracy. We show desirable properties of PROXIMUS in terms of runtime, scalability to large datasets, and performance in terms of capability to represent data in a compact form. We also demonstrate applications of PROXIMUS in association rule mining. In doing so, we establish PROXIMUS as a tool for preprocessing data before applying computationally expensive algorithms or as a tool for directly extracting correlated patterns. Our experimental results show that use of the compressed data for association rule mining provides excellent precision and recall values (near 100%) across a range of support thresholds while reducing the time required for association rule mining drastically. Copyright 2003 ACM.
7B51FF8B	Knowledge Discovery and Data Mining	somayajulu sripada + ehud reiter + jin yu + jim hunter	2003	Generating English summaries of time series data using the Gricean maxims	 + summarization + weather forecasting + data analysis + natural language processing + Computing methodologies + Natural language processing + time series data + Artificial intelligence	eol>Time series data + Summarization + Natural Language Processing + Gricean maxims	We are developing technology for generating English textual summaries of time-series data, in three domains: weather forecasts, gas-turbine sensor readings, and hospital intensive care data. Our weather-forecast generator is currently operational and being used daily by a meteorological company. We generate summaries in three steps: (a) selecting the most important trends and patterns to communicate; (b) mapping these patterns onto words and phrases; and (c) generating actual texts based on these words and phrases. In this paper we focus on the first step, (a), selecting the information to communicate, and describe how we perform this using modified versions of standard data analysis algorithms such as segmentation. The modifications arose out of empirical work with users and domain experts, and in fact can all be regarded as applications of the Gricean maxims of Quality, Quantity, Relevance, and Manner, which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text. The Gricean maxims are perhaps a key element of adapting data analysis algorithms for effective communication of information to human users, and should be considered by other researchers interested in communicating data to human users.
7F7EC79F	Knowledge Discovery and Data Mining	oren etzioni + alexander yates + craig a knoblock + rattapoom tuchinda	2003	To buy or not to buy: mining airfare data to minimize ticket purchase price	 + hidden variables + prediction model + web mining + data mining + Machine learning + Computing methodologies + world wide web + internet	+I.2.6 [Artificial Intelligence+Learning	As product prices become increasingly available on the World Wide Web, consumers attempt to understand how corporations vary these prices over time. However, corporations change prices based on proprietary algorithms and hidden variables (e.g., the number of unsold seats on a flight). Is it possible to develop data mining techniques that will enable consumers to predict price changes under these conditions?This paper reports on a pilot study in the domain of airline ticket prices where we recorded over 12,000 price observations over a 41 day period. When trained on this data, Hamlet --- our multi-strategy data mining algorithm --- generated a predictive model that saved 341 simulated passengers $198,074 by advising them when to buy and when to postpone ticket purchases. Remarkably, a clairvoyant algorithm with complete knowledge of future prices could save at most $320,572 in our simulation, thus HAMLET's savings were 61.8% of optimal. The algorithm's savings of $198,074 represents an average savings of 23.8% for the 341 passengers for whom savings are possible. Overall, HAMLET saved 4.4% of the ticket price averaged over the entire set of 4,488 simulated passengers. Our pilot study suggests that mining of price data available over the web has the potential to save consumers substantial sums of money per annum.
7B626573	Knowledge Discovery and Data Mining	charu c aggarwal + mohammed j zaki	2003	XRules: an effective structural classifier for XML data	 + rule based + bag of words + data mining + xml document + Information systems applications + classification tree + semi structured data + classification + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Data Mining Keywords XML/Semi-structured data+Classification+Tree Mining	XML documents have recently become ubiquitous because of their varied applicability in a number of applications. Classification is an important problem in the data mining domain, but current classification methods for XML documents use IR-based methods in which each document is treated as a bag of words. Such techniques ignore a significant amount of information hidden inside the documents. In this paper we discuss the problem of rule based classification of XML data by using frequent discriminatory substructures within XML documents. Such a technique is more capable of finding the classification characteristics of documents. In addition, the technique can also be extended to cost sensitive classification. We show the effectiveness of the method with respect to other classifiers. We note that the methodology discussed in this paper is applicable to any kind of semi-structured data.
7DA7D466	Knowledge Discovery and Data Mining	jiawei han + xifeng yan	2003	CloseGraph: mining closed frequent graph patterns	 + data structure + Information systems applications + graph representation + Information systems	+-/.0/e%	Recent research on pattern discovery has progressed form mining frequent itemsets and sequences to mining structured patterns including trees, lattices, and graphs. As a general data structure, graph can model complicated relations among data with wide applications in bioinformatics, Web exploration, and etc. However, mining large graph patterns in challenging due to the presence of an exponential number of frequent subgraphs. Instead of mining all the subgraphs, we propose to mine closed frequent graph patterns. A graph g is closed in a database if there exists no proper supergraph of g that has the same support as g. A closed graph pattern mining algorithm, CloseGraph, is developed by exploring several interesting pruning methods. Our performance study shows that CloseGraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increases the efficiency of mining, especially in the presence of large graph patterns.
7AE4E1EC	Knowledge Discovery and Data Mining	jon kleinberg + david kempe + eva tardos	2003	Maximizing the spread of influence through a social network	social network + Theory of computation +  + optimization problem + viral marketing + word of mouth + social networks + social network analysis + approximation algorithms + Design and analysis of algorithms + computer experiment	+F.2.2 [Analysis of Algorithms and Problem Complexity+Non	"Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of ""word of mouth"" in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target?We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks.We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks."
7D8C613B	Knowledge Discovery and Data Mining	toshihiro kamishima	2003	Nantonac collaborative filtering: recommendation based on order responses	 + collaborative filtering + Retrieval tasks and goals + Information systems applications + Clustering and classification + Information retrieval + Information extraction + Clustering + Data mining + Information systems + recommender system + Document filtering + order	+Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval+Information Search and RetrievalâClustering+Information filtering Keywords Order+Recommender System+Collaborative Filtering	A recommender system suggests the items expected to be preferred by the users. Recommender systems use collaborative filtering to recommend items by summarizing the preferences of people who have tendencies similar to the user preference. Traditionally, the degree of preference is represented by a scale, for example, one that ranges from one to five. This type of measuring technique is called the semantic differential (SD) method. Web adopted the ranking method, however, rather than the SD method, since the SD method is intrinsically not suited for representing individual preferences. In the ranking method, the preferences are represented by orders, which are sorted item sequences according to the users' preferences. We here propose some methods to recommed items based on these order responses, and carry out the comparison experiments of these methods.
7D50D6C5	Knowledge Discovery and Data Mining	karlton sequeira + mohammed j zaki + boleslaw k szymanski + christopher d carothers	2003	Improving spatial locality of programs via data mining	 + data mining + Information systems applications + Information systems	Program Locality + Code Restructuring + Page Clustering	In most computer systems, page fault rate is currently minimized by generic page replacement algorithms which try to model the temporal locality inherent in programs. In this paper, we propose two algorithms, one greedy and the other stochastic, designed for program specific code restructuring as a means of increasing spatial locality within a program. Both algorithms effectively decrease average working set size and hence the page fault rate. Our methods are more effective than traditional approaches due to use of domain information. We illustrate the efficacy of our algorithms on actual data mining algorithms.
816F9809	Knowledge Discovery and Data Mining	tim churches + peter christen + rohan a baxter	2003	A Comparison of Fast Blocking Methods for Record Linkage	 + record linkage + inverse document frequency + term frequency + indexation + data integrity	eol>Record linkage + object reconciliation + data integration	"
Blocking methods are used in record linkage systems to reduce the number of candidate record comparison pairs to a feasible number whilst still maintaining linkage accuracy. Blocking methods partition the data sets into blocks or clusters of records which share a blocking attribute or are otherwise similar with respect to a defined criterion. We compare two new blocking methods, bigram indexing and canopy clustering with TFIDF (Term Frequency/Inverse Document Frequency), with two older methods of standard traditional blocking and sorted neighbourhood blocking. The results show that recently blocking methods such as bigram indexing and canopy clustering provide scalable blocking methods while maintaining or improving upon record linkage accuracy. There is a potential for large performance speed-ups and better accuracy to be achieved by these new blocking methods.
"
7A039F40	Knowledge Discovery and Data Mining	chun tang + aidong zhang + jian pei	2003	Mining phenotypes and informative genes from gene expression data	phenotype +  + synthetic data + bioinformatics + Information systems applications + Data mining + heuristic search + Information systems	Phenotype + informative genes + array data + bioinformatics	Mining microarray gene expression data is an important research topic in bioinformatics with broad applications. While most of the previous studies focus on clustering either genes or samples, it is interesting to ask whether we can partition the complete set of samples into exclusive groups (called phenotypes) and find a set of informative genes that can manifest the phenotype structure. In this paper, we propose a new problem of simultaneously mining phenotypes and informative genes from gene expression data. Some statistics-based metrics are proposed to measure the quality of the mining results. Two interesting algorithms are developed: the heuristic search and the mutual reinforcing adjustment method. We present an extensive performance study on both real-world data sets and synthetic data sets. The mining results from the two proposed methods are clearly better than those from the previous methods. They are ready for the real-world applications. Between the two methods, the mutual reinforcing adjustment method is in general more scalable, more effective and with better quality of the mining results.
7638D3B2	Knowledge Discovery and Data Mining	r bharat rao + sathyakama sandilya + radu stefan niculescu + colin germond + harsha rao	2003	Clinical and financial outcomes analysis with existing hospital patient records	 + treatment effect + quality assurance + data mining + colon cancer + Information systems applications + knowledge discovery + acute myocardial infarction + Information systems	+Temporal Reasoning+Bayes Nets+HMMs	"Existing patient records are a valuable resource for automated outcomes analysis and knowledge discovery. However, key clinical data in these records is typically recorded in unstructured form as free text and images, and most structured clinical information is poorly organized. Time-consuming interpretation and analysis is required to convert these records into structured clinical data. Thus, only a tiny fraction of this resource is utilized. We present REMIND, a Bayesian Framework for Reliable Extraction and Meaningful Inference from Nonstructured Data. REMIND integrates and blends the structured and unstructured clinical data in patient records to automatically created high-quality structured clinical data. This structuring allows existing patient records to be mined for quality assurance, regulatory compliance, and to relate financial and clinical factors. We demonstrate REMIND on two medical applications: (a) Extract ""recurrence"", the key outcome for measuring treatment effectiveness, for colon cancer patients (ii) Extract key diagnoses and complications for acute myocardial infarction (heart attack) patients, and demonstrate the impact of these clinical factors on financial outcomes."
754567ED	Knowledge Discovery and Data Mining	hang yu + eechien chang	2003	Distributed multivariate regression based on influential observations	 + sampling + multivariate linear regression + Learning paradigms + Computing methodologies + random sampling + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + multivariate regression + Supervised learning by classification + learning curve	No keyword found	Large-scale data sets are sometimes logically and physically distributed in separate databases. The issues of mining these data sets are not just their sizes, but also the distributed nature. The complication is that communicating all the data to a central database would be too slow. To reduce communication costs, one could compress the data during transmission. Another method is random sampling. We propose an approach for distributed multivariate regression based on sampling and discuss its relationship with the compression method. The central idea is motivated by the observation that, although communication is limited, each individual site can still scan and process all the data it holds. Thus it is possible for the site to communicate only influential samples without seeing data in other sites. We exploit this observation and derive a method that provides tradeoff between communication cost and accuracy. Experimental results show that it is better than the compression method and random sampling.
58C960C7	Knowledge Discovery and Data Mining	zhigang li + margaret h dunham + liangang liu	2003	Considering correlation between variables to improve spatiotemporal forecasting	flood control + multivariate analysis + natural environment		The importance of forecasting cannot be overemphasized in modern environment surveillance applications, including flood control, rainfall analysis, pollution study, nuclear leakage prevention and so on. That is why we proposed STIFF (SpatioTemporal Integrated Forecasting Framework) in previous work [11], trying to answer such a challenging problem of doing forecasting in natural environment with both spatial and temporal characteristics involved. However, despite its promising performance on univariate-based data, STIFF is not sophisticated enough for more complicated environmental data derived from multiple correlated variables. Therefore in this paper we add multivariate analysis to the solution, take the correlation between different variables into account and further extend STIFF to address spatiotemporal forecasting involving multiple variables. Our experiments show that this introduction and integration of multivariate correlation not only has a more reasonable and rational interpretation of the data itself, but also produces a higher accuracy and slightly more balanced behavior.
776C955E	Knowledge Discovery and Data Mining	vijay s iyengar	2004	On detecting space-time clusters	 + multiple hypothesis testing + Information systems applications + randomized algorithm + Data mining + monte carlo + heuristic search + Information systems + monte carlo method + space time + search space + public health + clusters	+Categories and Subject Descriptors+H.2.8 [Database Management+Database Applications- Data Mining General Terms+Algorithms+Experimentation Keywords+Clusters+space-time region+spatial scan statis- tic+search+Monte Carlo	Detection of space-time clusters is an important function in various domains (e.g., epidemiology and public health). The pioneering work on the spatial scan statistic is often used as the basis to detect and evaluate such clusters. State-of-the-art systems based on this approach detect clusters with restrictive shapes that cannot model growth and shifts in location over time. We extend these methods significantly by using the flexible square pyramid shape to model such effects. A heuristic search method is developed to detect the most likely clusters using a randomized algorithm in combination with geometric shapes processing. The use of Monte Carlo methods in the original scan statistic formulation is continued in our work to address the multiple hypothesis testing issues. Our method is applied to a real data set on brain cancer occurrences over a 19 year period. The cluster detected by our method shows both growth and movement which could not have been modeled with the simpler cylindrical shapes used earlier. Our general framework can be extended quite easily to handle other flexible shapes for the space-time clusters.
7AEF8111	Knowledge Discovery and Data Mining	jian pei + wei wang + chen wang + baile shi + yongtai zhu	2004	Scalable mining of large disk-based graph databases	 + graph database + indexation + Information systems applications + index + Data mining + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Applications+Data Mining General Terms+Algorithms+Performances. Keywords+Graph mining+index+graph database+frequent graph	Mining frequent structural patterns from graph databases is an interesting problem with broad applications. Most of the previous studies focus on pruning unfruitful search subspaces effectively, but few of them address the mining on large, disk-based databases. As many graph databases in applications cannot be held into main memory, scalable mining of large, disk-based graph databases remains a challenging problem. In this paper, we develop an effective index structure, ADI (for <u>ad</u>jacency <u>i</u>ndex), to support mining various graph patterns over large databases that cannot be held into main memory. The index is simple and efficient to build. Moreover, the new index structure can be easily adopted in various existing graph pattern mining algorithms. As an example, we adapt the well-known gSpan algorithm by using the ADI structure. The experimental results show that the new index structure enables the scalable graph pattern mining over large databases. In one set of the experiments, the new disk-based method can mine graph databases with one million graphs, while the original gSpan algorithm can only handle databases of up to 300 thousand graphs. Moreover, our new method is faster than gSpan when both can run in main memory.
7D06E612	Knowledge Discovery and Data Mining	xin zhang + nikos mamoulis + david w cheung + yutao shou	2004	Fast mining of spatial collocations	 + Geographic visualization + Visualization + spatial database + transaction data + Spatial-temporal systems + Information systems applications + Human-centered computing + Data mining + Visualization application domains + Information systems	No keyword found	Spatial collocation patterns associate the co-existence of non-spatial features in a spatial neighborhood. An example of such a pattern can associate contaminated water reservoirs with certain deceases in their spatial neighborhood. Previous work on discovering collocation patterns converts neighborhoods of feature instances to itemsets and applies mining techniques for transactional data to discover the patterns. We propose a method that combines the discovery of spatial neighborhoods with the mining process. Our technique is an extension of a spatial join algorithm that operates on multiple inputs and counts long pattern instances. As demonstrated by experimentation, it yields significant performance improvements compared to previous approaches.
7C4443D1	Knowledge Discovery and Data Mining	shyam kapur + ramnath balasubramanyan + dmitry pavlov + byron dom + jignashu parikh	2004	Document preprocessing for naive Bayes classification and clustering with mixture of multinomials	 + data transformations + performance + text clustering + naive bayes + Machine learning + Computing methodologies + clustering + classification + naive bayes classifier	+Categories and Subject Descriptors+I.2.6 [Artificial Intelligence+Learning General Terms+Algorithms+Experimentation. Keywords+Classification+clustering+data transformations+performance+Naive Bayes+mixture of multinomials	Naive Bayes classifier has long been used for text categorization tasks. Its sibling from the unsupervised world, the probabilistic mixture of multinomial models, has likewise been successfully applied to text clustering problems. Despite the strong independence assumptions that these models make, their attractiveness come from low computational cost, relatively low memory consumption, ability to handle heterogeneous features and multiple classes, and often competitiveness with the top of the line models. Recently, there has been several attempts to alleviate the problems of Naive Bayes by performing heuristic feature transformations, such as IDF, normalization by the length of the documents and taking the logarithms of the counts. We justify the use of these techniques and apply them to two problems: classification of products in Yahoo! Shopping and clustering the vectors of collocated terms in user queries to Yahoo! Search. The experimental evaluation allows us to draw conclusions about the promise that these transformations carry with regard to alleviating the strong assumptions of the multinomial model.
755157B2	Knowledge Discovery and Data Mining	jerome p reiter + ashish p sanil + alan f karr + xiaodong lin	2004	Privacy preserving regression modelling via distributed computation	 + data mining + linear regression + Information systems applications + Computing methodologies + data integrity + relational database + data confidentiality + Data mining + distributed computing + computation + Information systems + secure multi party computation + regression + Machine learning + regression equation + goodness of fit + data integration	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data Mining+I.2.6 [Artificial Intelligence+Learning. General Terms Algorithms+Security. Keywords Data confidentiality+data integration+secure multi-party computation	Reluctance of data owners to share their possibly confidential or proprietary data with others who own related databases is a serious impediment to conducting a mutually beneficial data mining analysis. We address the case of vertically partitioned data -- multiple data owners/agencies each possess a few attributes of every data record. We focus on the case of the agencies wanting to conduct a linear regression analysis with complete records without disclosing values of their own attributes. This paper describes an algorithm that enables such agencies to compute the exact regression coefficients of the global regression equation and also perform some basic goodness-of-fit diagnostics while protecting the confidentiality of their data. In more general settings beyond the privacy scenario, this algorithm can also be viewed as method for the distributed computation for regression analyses.
5B41B5CA	Knowledge Discovery and Data Mining	soumen chakrabarti	2004	Discovering links between lexical and surface features in questions and answers	indexation + machine learning + information retrieval system + question answering system + feature vector		Information retrieval systems, based on keyword match, are evolving to question answering systems that return short passages or direct answers to questions, rather than URLs pointing to whole pages. Most open-domain question answering systems depend on manually designed hierarchies of question types. A question is first classified to a fixed type, and then hand-engineered rules associated with the type yield keywords and/or predictive annotations that are likely to match indexed answer passages. Here we seek a more data-driven approach, assisted by machine learning. We propose a simple log-linear model over a pair of feature vectors, one derived from the question and the other derived from the a candidate passage. Features are extracted using a lexical network and surface context as in named entity extraction, except that there is no direct supervision available in the form of fixed entity types and their examples. Using the log-linear model, we filter candidate passages and see substantial improvement in the mean rank at which the first answer is found. The model parameters distill and reveal linguistic artifacts coupling questions and their answers, which can be used for better annotation and indexing.
7D9D6716	Knowledge Discovery and Data Mining	marko krema + chad cumby + rayid ghani + andrew fano	2004	Predicting customer shopping lists from point-of-sale purchase data	 + transaction data + data mining + point of sale + Information systems applications + machine learning + classification + Data mining + business case + applications + Information systems	+Machine learning+Classificati	This paper describes a prototype that predicts the shopping lists for customers in a retail store. The shopping list prediction is one aspect of a larger system we have developed for retailers to provide individual and personalized interactions with customers as they navigate through the retail store. Instead of using traditional personalization approaches, such as clustering or segmentation, we learn separate classifiers for each customer from historical transactional data. This allows us to make very fine-grained and accurate predictions about what items a particular individual customer will buy on a given shopping trip.We formally frame the shopping list prediction as a classification problem, describe the algorithms and methodology behind our system, its impact on the business case in which we frame it, and explore some of the properties of the data source that make it an interesting testbed for KDD algorithms. Our results show that we can predict a shopper's shopping list with high levels of accuracy, precision, and recall. We believe that this work impacts both the data mining and the retail business community. The formulation of shopping list prediction as a machine learning problem results in algorithms that should be useful beyond retail shopping list prediction. For retailers, the result is not only a practical system that increases revenues by up to 11%, but also enhances customer experience and loyalty by giving them the tools to individually interact with customers and anticipate their needs.
8056EC0A	Knowledge Discovery and Data Mining	abdur chowdhury + aleksander kolcz + joshua alspector	2004	Improved robustness of signature-based near-replica detection via lexicon randomization	data cleaning + data mining + web mining + deduplication + web pages	deduplication + data cleaning + spam filtering + web mining	"
Detection of near duplicate documents is an important problem in many data mining and information filtering applications. When faced with massive quantities of data, traditional duplicate detection techniques relying on direct interdocument similarity computation (e.g., using the cosine measure) are often not feasible given the time and memory performance constraints. On the other hand, fingerprint-based methods, such as I-Match, are very attractive computationally but may be brittle with respect to small changes to document content. We focus on approaches to nearreplica detection that are based upon large-collection statistics and present a general technique of increasing their robustness via multiple lexicon randomization. In experiments with large web-page and spam-email datasets the proposed method is shown to consistently outperform traditional I-Match, with the relative improvement in duplicatedocument recall reaching as high as 40-60%. The large gains in detection accuracy are offset by only small increases in computational requirements.
"
7AE4430D	Sigkdd Explorations	bart goethals + mohammed j zaki	2004	Advances in frequent itemset mining implementations: report on FIMI'03	association rule mining		"
1.2 Some Recommendations
"
7514FF58	Knowledge Discovery and Data Mining	glen jeh + jennifer widom	2004	Mining the space of graph properties	 + satisfiability + data mining + Information systems applications + structural similarity + Data mining + Information systems	+for analysis is often a challenge+especially when little is known about the data to begin with. Moreover+by fixing	Existing data mining algorithms on graphs look for nodes satisfying specific properties, such as specific notions of structural similarity or specific measures of link-based importance. While such analyses for predetermined properties can be effective in well-understood domains, sometimes identifying an appropriate property for analysis can be a challenge, and focusing on a single property may neglect other important aspects of the data. In this paper, we develop a foundation for mining the properties themselves. We present a theoretical framework defining the space of graph properties, a variety of mining queries enabled by the framework, techniques to handle the enormous size of the query space, and an experimental system called F-Miner that demonstrates the utility and feasibility of property mining.
77292E30	Knowledge Discovery and Data Mining	david heckerman	2004	Graphical models for data mining	analytics +  + variational method + object oriented + graphical model + data mining + olap + relational model + oltp + business intelligence	+Invited Talk	I will discuss the use of graphical models for data mining. I will review key research areas including structure learning, variational methods, a relational modeling, and describe applications ranging from web traffic analysis to AIDS vaccine design.
7EA96771	Knowledge Discovery and Data Mining	eric haseltine	2004	User-centered design for KDD	analytics +  + object oriented + olap + business intelligence + oltp + user centered design + system engineering	No keyword found	"During initial development, KDD solutions often focus heavily on algorithms, architectures, software, hardware, and systems engineering challenges, without first thoroughly exploring how end-users will employ the new KDD technology. As a result of such ""system-centered"" design, many useless features are implemented that prolong development and significantly add to life cycle cost, while making the system hard to operate and use. This presentation will describe an alternate ""user-centered"" approach -- borrowed from the consumer products industry -- that can produce KDD solutions with shorter development cycles, lower costs, and much better usability."
78DD7B78	Knowledge Discovery and Data Mining	bin he + kevin chenchuan chang + jiawei han	2004	Discovering complex matchings across web query interfaces: a correlation mining approach	 + Information integration + Information systems applications + data integrity + Data mining + Database management system engines + Information systems + Computer systems organization + Extraction, transformation and loading + information integration + Data management systems + search space + Other architectures + Database design and models + data integration + Heterogeneous (hybrid) systems + deep web + Architectures	+Categories and Subject Descriptors H.2.5 [Database Management+Heterogeneous Databases+H.2.8 [Database Management+Database ApplicationsâData Mining General Terms Algorithms+Measurement	"To enable information integration, schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources. While complex matchings are common, because of their far more complex search space, most existing techniques focus on simple 1:1 matchings. To tackle this challenge, this paper takes a conceptually novel approach by viewing schema matching as correlation mining, for our task of matching Web query interfaces to integrate the myriad databases on the Internet. On this ""deep Web,"" query interfaces generally form complex matchings between attribute groups (e.g., [author] corresponds to [first name, last name] in the Books domain). We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes (e.g., [first name, last name]) tend to be co-present in query interfaces and thus positively correlated. In contrast, synonym attributes are negatively correlated because they rarely co-occur. This insight enables us to discover complex matchings by a correlation mining approach. In particular, we develop the DCM framework, which consists of data preparation, dual mining of positive and negative correlations, and finally matching selection. Unlike previous correlation mining algorithms, which mainly focus on finding strong positive correlations, our algorithm cares both positive and negative correlations, especially the subtlety of negative correlations, due to its special importance in schema matching. This leads to the introduction of a new correlation measure, $H$-measure, distinct from those proposed in previous work. We evaluate our approach extensively and the results show good accuracy for discovering complex matchings."
6CA516BB	Knowledge Discovery and Data Mining	jonathan traupman + robert wilensky	2004	Collaborative Quality Filtering: Establishing Consensus or Recovering Ground Truth?	recommender system + ground truth + factor analysis		We present a algorithm based on factor analysis for performing collaborative quality filtering (CQF). Unlike previous approaches to CQF, which estimate the consensus opinion of a group of reviewers, our algorithm uses a generative model of the review process to estimate the latent intrinsic quality of the items under reviews. We run several tests that demonstrate that consensus and intrinsic quality are, in fact different and unrelated aspects of quality. These results suggest that asymptotic consensus, which purports to model peer review, is, in fact, not recovering the ground truth quality of reviewed items.
7C609409	Knowledge Discovery and Data Mining	satoshi morinaga + kenji yamanishi	2004	Tracking dynamics of topic trends using a finite mixture model	 + finite mixture model + model selection + Information systems applications + knowledge management + Data mining + data collection + Information systems + satisfiability + text mining + real time + trend analysis + crm	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- Data Mining Keywords topic analysis+model selection+CRM+text mining	In a wide range of business areas dealing with text data streams, including CRM, knowledge management, and Web monitoring services, it is an important issue to discover topic trends and analyze their dynamics in real-time. Specifically we consider the following three tasks in topic trend analysis: 1)Topic Structure Identification; identifying what kinds of main topics exist and how important they are, 2)Topic Emergence Detection; detecting the emergence of a new topic and recognizing how it grows, 3)Topic Characterization; identifying the characteristics for each of main topics. For real topic analysis systems, we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way, and be dealt with in a single framework. This paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically. In this framework we propose the usage of a time-stamp based discounting learning algorithm in order to realize real-time topic structure identification. This enables tracking the topic structure adaptively by forgetting out-of-date statistics. Further we apply the theory of dynamic model selection to detecting changes of main components in the finite mixture model in order to realize topic emergence detection. We demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamics of topic trends in a timely fashion.
010701DC	Knowledge Discovery and Data Mining	jaideep srivastava + prasanna desikan	2004	Mining Temporally Changing Web Usage Graphs	web personalization + data collection + social network + web mining		Web mining has been explored to a vast degree and different techniques have been proposed for a variety of applications that include Web Search, Web Classification, Web Personalization etc. Most research on Web mining has been from a âdata-centricâ point of view. The focus has been primarily on developing measures and applications based on data collected from content, structure and usage of Web until a particular time instance. In this project we examine another dimension of Web Mining, namely temporal dimension. Web data has been evolving over time, reflecting the ongoing trends. These changes in data in the temporal dimension reveal new kind of information. This information has not captured the attention of the Web mining research community to a large extent. In this paper, we highlight the significance of studying the evolving nature of the Web graphs. We have classified the approach to such problems at three levels of analysis: single node, sub-graphs and whole graphs. We provide a framework to approach problems in this kind of analysis and identify interesting problems at each level. Our experiments verify the significance of such an analysis and also point to future directions in this area. The approach we take is generic and can be applied to other domains, where data can be modeled as a graph, such as network intrusion detection or social networks.
7CB07222	Knowledge Discovery and Data Mining	selim aksoy + giovanni b marchisio + carsten tusk + krzysztof koperski	2004	Interactive training of advanced classifiers for mining remote sensing image archives	data fusion +  + algorithms + digital image processing + Computer vision + Computer vision problems + Computer vision tasks + decision tree classifier + Computing methodologies + remote sensing + missing data + Scene understanding + decision tree + design + Machine learning + land + Artificial intelligence	+Categories and Subject Descriptors+I.5.2 [Pattern Recog- nition+Design Methodology+I.4.8 [Image Processing and Computer Vision+Scene Analysis General Terms+Algorithms+Design+Experimentation Keywords+Decision tree classifiers+missing data+data fu- sion	Advances in satellite technology and availability of downloaded images constantly increase the sizes of remote sensing image archives. Automatic content extraction, classification and content-based retrieval have become highly desired goals for the development of intelligent remote sensing databases. The common approach for mining these databases uses rules created by analysts. However, incorporating GIS information and human expert knowledge with digital image processing improves remote sensing image analysis. We developed a system that uses decision tree classifiers for interactive learning of land cover models and mining of image archives. Decision trees provide a promising solution for this problem because they can operate on both numerical (continuous) and categorical (discrete) data sources, and they do not require any assumptions about neither the distributions nor the independence of attribute values. This is especially important for the fusion of measurements from different sources like spectral data, DEM data and other ancillary GIS data. Furthermore, using surrogate splits provides the capability of dealing with missing data during both training and classification, and enables handling instrument malfunctions or the cases where one or more measurements do not exist for some locations. Quantitative and qualitative performance evaluation showed that decision trees provide powerful tools for modeling both pixel and region contents of images and mining of remote sensing image archives.
784F0F48	Knowledge Discovery and Data Mining	giles hooker	2004	Diagnosing extrapolation: tree-based density estimation	 + Cluster analysis + visualization + product distribution + Learning paradigms + Information systems applications + Computing methodologies + machine learning + Data mining + uniform distribution + Unsupervised learning + Information systems + extrapolation + statistical test + interpretation + Machine learning + clustering + Probability and statistics + Mathematics of computing + density estimation + Distribution functions	+Categories and Subject Descriptors G.3 [Probability and Statistics+Distribution Functions+I.5.3 [Pattern Recognition+ClusteringâAlgorithms+H.2.8 [Database Management+Database ApplicationsâData Mining General Terms Algorithms+Measurement+Documentation+Verification Keywords Visualization+Density Estimation+Extrapolation+Diagnostics+Interpretation+CART+C4.5+Trees-based models+Modeling method- ologies+Clustering	There has historically been very little concern with extrapolation in Machine Learning, yet extrapolation can be critical to diagnose. Predictor functions are almost always learned on a set of highly correlated data comprising a very small segment of predictor space. Moreover, flexible predictors, by their very nature, are not controlled at points of extrapolation. This becomes a problem for diagnostic tools that require evaluation on a product distribution. It is also an issue when we are trying to optimize a response over some variable in the input space. Finally, it can be a problem in non-static systems in which the underlying predictor distribution gradually drifts with time or when typographical errors misrecord the values of some predictors.We present a diagnosis for extrapolation as a statistical test for a point originating from the data distribution as opposed to a null hypothesis uniform distribution. This allows us to employ general classification methods for estimating such a test statistic. Further, we observe that CART can be modified to accept an exact distribution as an argument, providing a better classification tool which becomes our extrapolation-detection procedure. We explore some of the advantages of this approach and present examples of its practical application.
7B7FC5A9	Knowledge Discovery and Data Mining	chandrika kamath + aleksandar lazarevic + ramdev kanapady	2004	Effective localized regression for damage detection in large complex mechanical structures	 + Visualization + prediction model + complex structure + finite element + data mining + time complexity + Information systems applications + Human-centered computing + Data mining + Visualization application domains + Information systems + regression model + Geographic visualization + natural frequency + Spatial-temporal systems + hierarchical clustering + clustering + Scientific visualization	+H.2.8 [Database Management+Database Applications (data	In this paper, we propose a novel data mining technique for the efficient damage detection within the large-scale complex mechanical structures. Every mechanical structure is defined by the set of finite elements that are called structure elements. Large-scale complex structures may have extremely large number of structure elements, and predicting the failure in every single element using the original set of natural frequencies as features is exceptionally time-consuming task. Traditional data mining techniques simply predict failure in each structure element individually using global prediction models that are built considering all data records. In order to reduce the time complexity of these models, we propose a localized clustering-regression based approach that consists of two phases: (1) building a local cluster around a data record of interest and (2) predicting an intensity of damage only in those structure elements that correspond to data records from the built cluster. For each test data record, we first build a cluster of data records from training data around it. Then, for each data record that belongs to discovered cluster, we identify corresponding structure elements and we build a localized regression model for each of these structure elements. These regression models for specific structure elements are constructed using only a specific set of relevant natural frequencies and merely those data records that correspond to the failure of that structure element. Experiments performed on the problem of damage prediction in a large electric transmission tower frame indicate that the proposed localized clustering-regression based approach is significantly more accurate and more computationally efficient than our previous hierarchical clustering approach, as well as global prediction models.
7B78D41F	Knowledge Discovery and Data Mining	xiaodong lin + young truong + chris beecher	2004	Learning a complex metabolomic dataset using random forests and support vector machines	 + support vector machines + detection limit + Information systems applications + Computing methodologies + random forest + Data mining + outlier detection + missing data + Information systems + support vector machine + metabolomics + Machine learning + variable selection	+Categories and Subject Descriptors+H.2.8 [Database Management+Database applications- Data Mining+I.2.6 [Artificial Intelligence+Learning General Terms+Measurement Keywords+Metabolomics+Random Forest+Support Vec- tor Machines+Missing Data	"Metabolomics is the ""omics"" science of biochemistry. The associated data include the quantitative measurements of all small molecule metabolites in a biological sample. These datasets provide a window into dynamic biochemical networks and conjointly with other ""omic"" data, genes and proteins, have great potential to unravel complex human diseases. The dataset used in this study has 63 individuals, normal and diseased, and the diseased are drug treated or not, so there are three classes. The goal is to classify these individuals using the observed metabolite levels for 317 measured metabolites. There are a number of statistical challenges: non-normal data, the number of samples is less than the number of metabolites; there are missing data and the fact that data are missing is informative (assay values below detection limits can point to a specific class); also, there are high correlations among the metabolites. We investigate support vector machines (SVM), and random forest (RF), for outlier detection, variable selection and classification. We use the variables selected with RF in SVM and visa versa. The benefit of this study is insight into interplay of variable selection and classification methods. We link our selected predictors to the biochemistry of the disease."
7CFF8DF9	Knowledge Discovery and Data Mining	foto n afrati + aristides gionis + heikki mannila	2004	Approximating a collection of frequent sets	 + Theory of computation + correspondence problem + data mining + Information systems applications + empirical evidence + Data mining + Design and analysis of algorithms + Information systems	0	One of the most well-studied problems in data mining is computing the collection of frequent item sets in large transactional databases. One obstacle for the applicability of frequent-set mining is that the size of the output collection can be far too large to be carefully examined and understood by the users. Even restricting the output to the border of the frequent item-set collection does not help much in alleviating the problem.In this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem: What are the k sets that best approximate a collection of frequent item sets? Our measure of approximating a collection of sets by k sets is defined to be the size of the collection covered by the the k sets, i.e., the part of the collection that is included in one of the k sets. We also specify a bound on the number of extra sets that are allowed to be covered. We examine different problem variants for which we demonstrate the hardness of the corresponding problems and we provide simple polynomial-time approximation algorithms. We give empirical evidence showing that the approximation methods work well in practice.
00CBF6C7	Knowledge Discovery and Data Mining	junhai guo + mario medvedovic	2004	Bayesian Model-Averaging in Unsupervised Learning From Microarray Data	finite mixture model + functional genomics + microarray data + biological process + gibbs sampler + unsupervised learning + mixture model + bayesian information criterion	Guides + instructions + authors kit + conference publications	"
Unsupervised identification of patterns in microarray data has been a productive approach to uncovering relationships between genes and the biological process in which they are involved. Traditional model-based clustering approaches as well as some recently developed model-based mining approaches for integrating genomic and functional genomic data rely on one's ability to determine the correct number of clusters or modules in the data. In this paper we demonstrate that the performance of such methods in general can be significantly improved by accounting for uncertainties inherent to the process of identifying the optimal number of clusters in the data. We demonstrate that the Bayesian averaging approach to clustering via infinite mixture model offers a more robust performance than the traditional finite mixture model in which the optimal number of clusters is determined using the Bayesian Information Criterion. This performance improvement is demonstrated through a simulation study and by the analysis of a relatively large microarray dataset. Finally, we describe the novel heuristic modification of the Gibbs sampler used to fit the infinite mixture mode that effectively deals with issues of slow mixing.
"
59B526FD	Knowledge Discovery and Data Mining	thilo maier	2004	Improving the Web Usage Analysis Process: A UML Model of the ETL Process	object oriented + relational data + data warehousing + data warehouse		Integrating OLAP and Web usage analysis in a data warehousing environment is a promising approach for sophisticated analysis of the Web channel in multi-channel environments of organizations. Populating the data warehouse is a laborious and time-consuming task (especially for small projects), which is â in practice â a big obstacle for concrete ECRM projects. Especially if Web usage analysis researchers need to conduct experiments with a Web warehouse, an intuitive and easy to deploy ETL component is essential. In this paper we propose a logical object-oriented relational data storage model in UML, which is based on a formal model. A concrete Java instance of our model simplifies modeling and automating the ETL process. The Java instance of our model has been integrated into our WUSAN (Web USage ANalyis) system. Finally, we illustrate the usage of our model for Web usage analysis purposes, though the model is principally not restricted to this domain.
7F04CC04	Knowledge Discovery and Data Mining	vipin kumar + shashi shekhar + hui xiong + pangning tan	2004	Exploiting a support-based upper bound of Pearson's correlation coefficient for efficiently identifying strongly correlated pairs	 + Information systems applications + statistical computing + upper bound + Data mining + Information systems	Pearson's Correlation Coe cient + Statistical Computing	Given a user-specified minimum correlation threshold Î¸ and a market basket database with N items and T transactions, an all-strong-pairs correlation query finds all item pairs with correlations above the threshold Î¸. However, when the number of items and transactions are large, the computation cost of this query can be very high. In this paper, we identify an upper bound of Pearson's correlation coefficient for binary variables. This upper bound is not only much cheaper to compute than Pearson's correlation coefficient but also exhibits a special monotone property which allows pruning of many item pairs even without computing their upper bounds. A Two-step All-strong-Pairs corrElation que Ry (TAPER) algorithm is proposed to exploit these properties in a filter-and-refine manner. Furthermore, we provide an algebraic cost model which shows that the computation savings from pruning is independent or improves when the number of items is increased in data sets with common Zipf or linear rank-support distributions. Experimental results from synthetic and real data sets exhibit similar trends and show that the TAPER algorithm can be an order of magnitude faster than brute-force alternatives.
78E9980E	Knowledge Discovery and Data Mining	ravi kumar + d sivakumar + uma mahadevan	2004	A graph-theoretic approach to extract storylines from search results	 + Web applications + Information retrieval + World Wide Web + Graph theory + bipartite graph + local search + Information systems + Web services + Discrete mathematics + clustering + link analysis + graph representation + Graph algorithms + Mathematics of computing	+Algorithms+Experimentation+Measurement+Human Factors Keywords Link analysis+Communities+Clustering+Storylines+Search results	We present a graph-theoretic approach to discover storylines from search results. Storylines are windows that offer glimpses into interesting themes latent among the top search results for a query; they are different from, and complementary to, clusters obtained through traditional approaches. Our framework is axiomatically developed and combinatorial in nature, based on generalizations of the maximum induced matching problem on bipartite graphs. The core algorithmic task involved is to mine for signature structures in a robust graph representation of the search results. We present a very fast algorithm for this task based on local search. Experiments show that the collection of storylines extracted through our algorithm offers a concise organization of the wealth of information hidden beyond the first page of search results.
76F1C48F	Knowledge Discovery and Data Mining	youxuan jin + jonathan tang + sam skrivan + adam yeh	2004	Analytical view of business data	 + data warehousing + Software notations and tools + decision support + business intelligence + Information systems + analytics + Modeling and simulation + programming model + Mathematics of computing + Software and its engineering + object oriented + Object oriented languages + information retrieval + component model + Information systems applications + Language types + Computing methodologies + Simulation theory + Systems theory + oltp + code generation + General programming languages + olap + Information theory	+OLAP+Business Intelligence+Analytics+Entity Persistence+Object-oriented+OLTP+Application Framework+Information retrieval and navigation	"This paper describes a logical extension to Microsoft Business Framework (MBF) called Analytical View (AV). AV consists of three components: Model Service for design time, Business Intelligence Entity (BIE) for programming model, and IntellDrill for runtime navigation between OLTP and OLAP data sources. AV feature-set fulfills enterprise application requirements for Analysis and Decision Support, complementing the transactional feature-set currently provided by MBF. Model Service automatically transforms an ""object oriented model (transactional view)"" to a ""multi-dimensional model (analytical view)"" without the traditional Extraction/Transformation/Loading (ETL) overhead and complexity. It infers dimensionality from the object layer where richer metadata is stored, eliminating the ""guesswork"" that a traditional data warehousing process requires when going through physical database schema. BI Entities are classes code-generated by Model Service. As an intrinsic part of the framework, BI Entities enable a consistent object oriented way of programming model with strong types and rich semantics for OLAP, similar to what MBF object persistence technology does for OLTP data. More importantly, data contained in BI Entities have a higher degree of ""application awareness,"" such as the integrated application level security and customizability. IntelliDrill links together all the information islands in MBF using metadata. Because of the automatic transformation from transactional view to analytical view enabled by Model Service, we have the ability to understand natively what kind of drill-ability an object would have, thus making information navigation in MBF fully discover-able with built-in ontology."
79B22CA5	Knowledge Discovery and Data Mining	lei liu + yu zhu	2004	Optimal randomization for privacy preserving data mining	 + data mining + Information systems applications + mixture model + Data mining + Information systems + Theory and algorithms for application domains + Theory of computation + Database and storage security + Security and privacy + mutual information + density estimation + Theory of database privacy and security + Database theory	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data mining+H.2.0 [Database Management+Generalâ Security+integrity+and protection General Terms Theory+Algorithms+Security Keywords mixture model	Randomization is an economical and efficient approach for privacy preserving data mining (PPDM). In order to guarantee the performance of data mining and the protection of individual privacy, optimal randomization schemes need to be employed. This paper demonstrates the construction of optimal randomization schemes for privacy preserving density estimation. We propose a general framework for randomization using mixture models. The impact of randomization on data mining is quantified by performance degradation and mutual information loss, while privacy and privacy loss are quantified by interval-based metrics. Two different types of problems are defined to identify optimal randomization for PPDM. Illustrative examples and simulation results are reported.
7B97EC8D	Knowledge Discovery and Data Mining	milind r naphade + john r smith + apostol natsev	2004	Semantic representation: search and mining of multimedia content	 + digital media + data mining + Information retrieval + Information systems	+General Terms+Algorithms+Design+Experimentation Keywords+Semantic Indexing+Model Vectors+TRECVID	Semantic understanding of multimedia content is critical in enabling effective access to all forms of digital media data. By making large media repositories searchable, semantic content descriptions greatly enhance the value of such data. Automatic semantic understanding is a very challenging problem and most media databases resort to describing content in terms of low-level features or using manually ascribed annotations. Recent techniques focus on detecting semantic concepts in video, such as indoor, outdoor, face, people, nature, etc. This approach works for a fixed lexicon for which annotated training examples exist. In this paper we consider the problem of using such semantic concept detection to map the video clips into semantic spaces. This is done by constructing a model vector that acts as a compact semantic representation of the underlying content. We then present experiments in the semantic spaces leveraging such information for enhanced semantic retrieval, classification, visualization, and data mining purposes. We evaluate these ideas using a large video corpus and demonstrate significant performance gains in retrieval effectiveness.
NE8489	Knowledge Discovery and Data Mining	Jieping Ye+Qi Li+Hui Xiong+Haesun Park+Ravi Janardan+Vipin Kumar	2004	IDR/QR: an incremental dimension reduction algorithm via QR decomposition.	 + Information systems applications + Data mining + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Management+Database Applications- Data Mining General Terms+Algorithms Keywords+Dimension reduction+Linear Discriminant Anal- ysis+incremental learning+QR Decomposition	Dimension reduction is critical for many database and data mining applications, such as efficient storage and retrieval of high-dimensional data. In the literature, a well-known dimension reduction scheme is Linear Discriminant Analysis (LDA). The common aspect of previously proposed LDA based algorithms is the use of Singular Value Decomposition (SVD). Due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in LDA, there is little work on designing incremental LDA algorithms. In this paper, we propose an LDA based incremental dimension reduction algorithm, called IDR/QR, which applies QR Decomposition rather than SVD. Unlike other LDA based algorithms, this algorithm does not require the whole data matrix in main memory. This is desirable for large data sets. More importantly, with the insertion of new data items, the IDR/QR algorithm can constrain the computational cost by applying efficient QR-updating techniques. Finally, we evaluate the effectiveness of the IDR/QR algorithm in terms of classification accuracy on the reduced dimensional space. Our experiments on several real-world data sets reveal that the accuracy achieved by the IDR/QR algorithm is very close to the best possible accuracy achieved by other LDA based algorithms. However, the IDR/QR algorithm has much less computational cost, especially when new data items are dynamically inserted.
7BB4F64B	Knowledge Discovery and Data Mining	charu c aggarwal + jiawei han + jianyong wang + philip s yu	2004	On demand classification of data streams	 + classification system + data streams + Information systems applications + classification + Data mining + real time + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data Mining General Terms Algorithms Keywords data streams+classification	Current models of the classification problem do not effectively handle bursts of particular classes coming in at different times. In fact, the current model of the classification problem simply concentrates on methods for one-pass classification modeling of very large data sets. Our model for data stream classification views the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing streams are used for dynamic classification of data sets. This model reflects real life situations effectively, since it is desirable to classify test streams in real time over an evolving training and test stream. The aim here is to create a classification system in which the training model can adapt quickly to the changes of the underlying data stream. In order to achieve this goal, we propose an on-demand classification process which can dynamically select the appropriate window of past training data to build the classifier. The empirical results indicate that the system maintains a high classification accuracy in an evolving data stream, while providing an efficient solution to the classification task.
7F8D268F	Knowledge Discovery and Data Mining	raymond j mooney + sugato basu + mikhail bilenko	2004	A probabilistic framework for semi-supervised clustering	 + Machine learning + Computing methodologies + Information systems applications + euclidean distance + Data mining + probabilistic model + objective function + Information systems	+Categories and Subject Descriptors+I.2.6 [Artificial Intelligence+Learning+H.2.8 [Database Management+Database Applications- Data Mining. General Terms+Algorithms. Keywords+Semi-supervised Clustering+Hidden Markov Random Fields+Distance Metric Learning	Unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints, i.e., pairs of instances labeled as belonging to same or different clusters. In recent years, a number of algorithms have been proposed for enhancing clustering quality by employing such supervision. Such methods use the constraints to either modify the objective function, or to learn the distance measure. We propose a probabilistic model for semi-supervised clustering based on Hidden Markov Random Fields (HMRFs) that provides a principled framework for incorporating supervision into prototype-based clustering. The model generalizes a previous approach that combines constraints and Euclidean distance learning, and allows the use of a broad range of clustering distortion measures, including Bregman divergences (e.g., Euclidean distance and I-divergence) and directional similarity measures (e.g., cosine similarity). We present an algorithm that performs partitional semi-supervised clustering of data by minimizing an objective function derived from the posterior energy of the HMRF model. Experimental results on several text data sets demonstrate the advantages of the proposed framework.
5C5C5B4A	Knowledge Discovery and Data Mining	filippo menczer + mehmet a nacar + mehmet s aktas	2004	Using Hyperlink Features to Personalize Web Search	feature extraction		Personalized search has gained great popularity to improve search effectiveness in recent years. The objective of personalized search is to provide users with information tailored to their individual contexts. We propose to personalize Web search based on features extracted from hyperlinks, such as anchor terms or URL tokens. Our methodology personalizes PageRank vectors by weighting links based on the match between hyperlinks and user profiles. In particular, here we describe a profile representation using Internet domain features extracted from URLs. Users specify interest profiles as binary vectors where each feature corresponds to a set of one or more DNS tree nodes. Given a profile vector, a weighted PageRank is computed assigning a weight to each URL based on the match between the URL and the profile. We present promising results from an experiment in which users were allowed to select among nine URL features combining the top two levels of the DNS tree, leading to 29 pre-computed PageRank vectors from a Yahoo crawl. Personalized PageRank performed favorably compared to pure similarity based ranking and traditional PageRank.
7C4BA4F7	Knowledge Discovery and Data Mining	christos faloutsos + edoardo m airoldi	2004	Recovering latent time-series from their observed sums: network tomography with particle filters.	 + traffic flow + Computing methodologies + time series + statistical model + particle filter + dynamic system + hidden variables + tomography + local area network + self organization + Machine learning + network tomography + mcmc	No keyword found	Hidden variables, evolving over time, appear in multiple settings, where it is valuable to recover them, typically from observed sums. Our driving application is 'network tomography', where we need to estimate the origin-destination (OD) traffic flows to determine, e.g., who is communicating with whom in a local area network. This information allows network engineers and managers to solve problems in design, routing, configuration debugging, monitoring and pricing. Unfortunately the direct measurement of the OD traffic is usually difficult, or even impossible; instead, we can easily measure the loads on every link, that is, sums of desirable OD flows.In this paper we propose i-FILTER, a method to solve this problem, which improves the state-of-the-art by (a) introducing explicit time dependence, and by (b) using realistic, non-Gaussian marginals in the statistical models for the traffic flows, as never attempted before. We give experiments on real data, where i-FILTER scales linearly with new observations and out-performs the best existing solutions, in a wide variety of settings. Specifically, on real network traffic measured at CMU, and at AT&T, i-FILTER reduced the estimation errors between 15% and 46% in all cases.
78C89FA1	Knowledge Discovery and Data Mining	guizhen yang	2004	The complexity of mining maximal frequent itemsets and maximal frequent patterns	Theory of computation +  + decision problem + complexity + data mining + complex data + partial order + Design and analysis of algorithms + theory	+pattern+P-complete+P-hard	Mining maximal frequent itemsets is one of the most fundamental problems in data mining. In this paper we study the complexity-theoretic aspects of maximal frequent itemset mining, from the perspective of counting the number of solutions. We present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transactions, given an arbitrary support threshold, is #P-complete, thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is NP-hard. This result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in P.We also extend our complexity analysis to other similar data mining problems dealing with complex data structures, such as sequences, trees, and graphs, which have attracted intensive research interests in recent years. Normally, in these problems a partial order among frequent patterns can be defined in such a way as to preserve the downward closure property, with maximal frequent patterns being those without any successor with respect to this partial order. We investigate several variants of these mining problems in which the patterns of interest are subsequences, subtrees, or subgraphs, and show that the associated problems of counting the number of maximal frequent patterns are all either #P-complete or #P-hard.
7C9CA2F9	Knowledge Discovery and Data Mining	david l selinger + shenghuo zhu + paat rusmevichientong	2004	Identifying early buyers from purchase data	semidefinite programming + social network +  + marketing strategy + consumer behavior + directed graph + Information systems applications + market research + Data mining + Information systems	consumer behavior + early buyers + semidefinite programming + social network	Market research has shown that consumers exhibit a variety of different purchasing behaviors; specifically, some tend to purchase products earlier than other consumers. Identifying such early buyers can help personalize marketing strategies, potentially improving their effectiveness. In this paper, we present a non-parametric approach to the problem of identifying early buyers from purchase data. Our formulation takes as inputs the detailed purchase information of each consumer, with which we construct a weighted directed graph whose nodes correspond to consumers and whose edges correspond to purchases consumers have in common; the edge weights indicate how frequently consumers purchase products earlier than other consumers.Identifying early buyers corresponds to the problem of finding a subset of nodes in the graph with maximum difference between the weights of the outgoing and incoming edges. This problem is a variation of the maximum cut problem in a directed graph. We provide an approximation algorithm based on semidefinite programming (SDP) relaxations pioneered by Goemans and Williamson, and analyze its performance. We apply the algorithm to real purchase data from Amazon.com, providing new insights into consumer behaviors.
7FBD2E0F	Knowledge Discovery and Data Mining	theodoros evgeniou + massimiliano pontil	2004	Regularized multi--task learning	 + support vector machine + support vector machines + multi task learning + regularization + Machine learning + kernel method + kernel methods + Computing methodologies + kernel function	+Algorithms+Theory. Keywords MultiâTask Learning+Support Vector Machines+Regular- ization+Kernel Methods	Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.
756FC5DF	Knowledge Discovery and Data Mining	siegfried nijssen + joost n kok	2004	A quickstart in frequent structure mining can make a difference	 + graphs + satisfiability + structures + Information systems applications + Data mining + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Management+Database ApplicationsâData Mining General Terms+Algorithms+Performance+Theory Keywords+Frequent Item Sets+Graphs+Semi-Structures+Structures	"Given a database, structure mining algorithms search for substructures that satisfy constraints such as minimum frequency, minimum confidence, minimum interest and maximum frequency. Examples of substructures include graphs, trees and paths. For these substructures many mining algorithms have been proposed. In order to make graph mining more efficient, we investigate the use of the ""quickstart principle"", which is based on the fact that these classes of structures are contained in each other, thus allowing for the development of structure mining algorithms that split the search into steps of increasing complexity. We introduce the GrAph/Sequence/Tree extractiON (Gaston) algorithm that implements this idea by searching first for frequent paths, then frequent free trees and finally cyclic graphs. We investigate two alternatives for computing the frequency of structures and present experimental results to relate these alternatives."
NE8542	Knowledge Discovery and Data Mining	Liang Huai Yang+Mong Li Lee+Wynne Hsu+Xinyu Guo	2004	2PXMiner: an efficient two pass mining of frequent XML query patterns.	 + Information systems applications + Data mining + Information systems	+Categories & Subject Descriptors+H.2.8 [Database Man- agement+Database Applications- Data Mining General Terms+Algorithms+Performance Keywords+XML Query Pattern+Tree Mining	Caching the results of frequent query patterns can improve the performance of query evaluation. This paper describes a 2-pass mining algorithm called 2PXMiner to discover frequent XML query patterns. We design 3 data structures to expedite the mining process. Experiments results indicate that 2PXMiner is both efficient and scalable.
78733CB1	Knowledge Discovery and Data Mining	rohini k srihari + xiaoyun wu	2004	Incorporating prior knowledge with weighted margin support vector machines	 + support vector machines + support vector machine + Supervised learning + Classification and regression trees + Machine learning + Learning paradigms + Computing methodologies + Machine learning approaches + sequential minimal optimization + machine learning + Supervised learning by classification	+Categories and Subject Descriptors I.2.6 [Artificial Intelligence+Learning+I.5.4 [Pattern Recognition+Design Methodologyâclassifier design and evaluation General Terms Algorithms+Performance Keywords Text Categorization+Support Vector Machines+Incorporat- ing Prior Knowledge	Like many purely data-driven machine learning methods, Support Vector Machine (SVM) classifiers are learned exclusively from the evidence presented in the training dataset; thus a larger training dataset is required for better performance. In some applications, there might be human knowledge available that, in principle, could compensate for the lack of data. In this paper, we propose a simple generalization of SVM: Weighted Margin SVM (WMSVMs) that permits the incorporation of prior knowledge. We show that Sequential Minimal Optimization can be used in training WMSVM. We discuss the issues of incorporating prior knowledge using this rather general formulation. The experimental results show that the proposed methods of incorporating prior knowledge is effective.
7972C30C	Knowledge Discovery and Data Mining	amin saberi + mohammad mahdian + jennifer chayes + christian borgs	2004	Exploring the community structure of newsgroups	spectral method +  + community structure + Discrete mathematics + Information retrieval + clustering + Graph theory + small world network + Mathematics of computing + Information systems	+Terms Algorithms+Theory Keywords Spectral Method+Usenet+Clustering	We propose to use the community structure of Usenet for organizing and retrieving the information stored in newsgroups. In particular, we study the network formed by cross-posts, messages that are posted to two or more newsgroups simultaneously. We present what is, to our knowledge, by far the most detailed data that has been collected on Usenet cross-postings. We analyze this network to show that it is a small-world network with significant clustering. We also present a spectral algorithm which clusters newsgroups based on the cross-post matrix. The result of our clustering provides a topical classification of newsgroups. Our clustering gives many examples of significant relationships that would be missed by semantic clustering methods.
7675096F	Knowledge Discovery and Data Mining	junichi takeuchi + takayuki nakata	2004	Mining traffic data from probe-car system for travel time prediction	 + state space model + Machine learning + Computing methodologies + time series + minimum description length + statistical analysis + ar model + data collection + seasonal adjustment	+Categories and Subject Descriptors I.2.6 [Artificial Intelligence+Learning General Terms Algorithms+Experimentation Keywords ITS+travel time+probe-car+time series+information criterion	We are developing a technique to predict travel time of a vehicle for an objective road section, based on real time traffic data collected through a probe-car system. In the area of Intelligent Transport System (ITS), travel time prediction is an important subject. Probe-car system is an upcoming data collection method, in which a number of vehicles are used as moving sensors to detect actual traffic situation. It can collect data concerning much larger area, compared with traditional fixed detectors. Our prediction technique is based on statistical analysis using AR model with seasonal adjustment and MDL (Minimum Description Length) criterion. Seasonal adjustment is used to handle periodicities of 24 hours in traffic data. Alternatively, we employ state space model, which can handle time series with periodicities. It is important to select really effective data for prediction, among the data from widespread area, which are collected via probe-car system. We do this using MDL criterion. That is, we find the explanatory variables that really have influence on the future travel time. In this paper, we experimentally show effectiveness of our method using probe-car data collected in Nagoya Metropolitan Area in 2002.
NE8537	Knowledge Discovery and Data Mining	Peter TiÃ±o+Ata KabÃ¡n+Yi Sun	2004	A generative probabilistic approach to visualizing sets of symbolic sequences.	 + Information retrieval + Information systems	+trieval General Terms+Algorithms+Design+Theory Keywords+Hidden Markov model+latent space models+topographic mapping	There is a notable interest in extending probabilistic generative modeling principles to accommodate for more complex structured data types. In this paper we develop a generative probabilistic model for visualizing sets of discrete symbolic sequences. The model, a constrained mixture of discrete hidden Markov models, is a generalization of density-based visualization methods previously developed for static data sets. We illustrate our approach on sequences representing web-log data and chorals by J.S. Bach.
NE8534	Knowledge Discovery and Data Mining	Jouni K. SeppÃ¤nen+Heikki Mannila	2004	Dense itemsets.	 + Information systems applications + Data mining + Information systems	+General Terms+Algorithms+Experimentation Keywords+Frequent itemsets+error tolerance	Frequent itemset mining has been the subject of a lot of work in data mining research ever since association rules were introduced. In this paper we address a problem with frequent itemsets: that they only count rows where all their attributes are present, and do not allow for any noise. We show that generalizing the concept of frequency while preserving the performance of mining algorithms is nontrivial, and introduce a generalization of frequent itemsets, dense itemsets. Dense itemsets do not require all attributes to be present at the same time; instead, the itemset needs to define a sufficiently large submatrix that exceeds a given density threshold of attributes present.We consider the problem of computing all dense itemsets in a database. We give a levelwise algorithm for this problem, and also study the top-$k$ variations, i.e., finding the k densest sets with a given support, or the k best-supported sets with a given density. These algorithms select the other parameter automatically, which simplifies mining dense itemsets in an explorative way. We show that the concept captures natural facets of data sets, and give extensive empirical results on the performance of the algorithms. Combining the concept of dense itemsets with set cover ideas, we also show that dense itemsets can be used to obtain succinct descriptions of large datasets. We also discuss some variations of dense itemsets.
797CE46B	Knowledge Discovery and Data Mining	andrew w moore + ke yang + ting liu	2004	The IOC algorithm: efficient many-class non-parametric classification for high-dimensional data	 + high dimensional data + k nearest neighbor + bayes classifier + metric tree + computer vision + Machine learning + Computing methodologies + classification	+Algorithms+Design+Performance Keywords k nearest neighbor+classification+high dimension+metric tree	This paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets.K nearest neighbor remains a popular classification technique, especially in areas such as computer vision, drug activity prediction and astrophysics. Furthermore, many more modern classifiers, such as kernel-based Bayes classifiers or the prediction phase of SVMs, require computational regimes similar to k-NN. We believe that tractable k-NN algorithms therefore continue to be important.This paper relies on the insight that even with many classes, the task of finding the majority class among the k nearest neighbors of a query need not require us to explicitly find those k nearest neighbors. This insight was previously used in (Liu et al., 2003) in two algorithms called KNS2 and KNS3 which dealt with fast classification in the case of two classes. In this paper we show how a different approach, IOC (standing for the International Olympic Committee) can apply to the case of n classes where n > 2.IOC assumes a slightly different processing of the datapoints in the neighborhood of the query. This allows it to search a set of metric trees, one for each class. During the searches it is possible to quickly prune away classes that cannot possibly be the majority.We give experimental results on datasets of up to 5.8 x 105 records and 1.5 x 103 attributes, frequently showing an order of magnitude acceleration compared with each of (i) conventional linear scan, (ii) a well-known independent SR-tree implementation of conventional k-NN and (iii) a highly optimized conventional k-NN metric tree search.
7B260335	Knowledge Discovery and Data Mining	zhongfei zhang + ruofei zhang + sandeep khanzode	2004	A data mining approach to modeling relationships among categories in image collection	 + fuzzy set + feature space + data mining + Information systems applications + Multimedia databases + classification tree + image features + Data mining + Information systems + relationships + image retrieval + Multimedia information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ data mining+image databases General Terms Algorithms+Measurement+Design Keywords Relationships+semantic category+fuzzy model+image collec- tion	This paper proposes a data mining approach to modeling relationships among categories in image collection. In our approach, with image feature grouping, a visual dictionary is created for color, texture, and shape feature attributes respectively. Labeling each training image with the keywords in the visual dictionary, a classification tree is built. Based on the statistical properties of the feature space we define a structure, called Î±-Semantics Graph, to discover the hidden semantic relationships among the semantic categories embodied in the image collection. With the Î±-Semantics Graph, each semantic category is modeled as a unique fuzzy set to explicitly address the semantic uncertainty and semantic overlap among the categories in the feature space. The model is utilized in the semantics-intensive image retrieval application. An algorithm using the classification accuracy measures is developed to combine the built classification tree with the fuzzy set modeling method to deliver semantically relevant image retrieval for a given query image. The experimental evaluations have demonstrated that the proposed approach models the semantic relationships effectively and the image retrieval prototype system utilizing the derived model is promising both in effectiveness and efficiency.
7FE489FF	Knowledge Discovery and Data Mining	chun tang + daxin jiang + aidong zhang + murali ramanathan + jian pei	2004	Mining coherent gene clusters from gene-sample-time microarray data	 + candidate gene + microarray data + synthetic data + bioinformatics + Information systems applications + time series + clustering + gene cluster + Data mining + Information systems	Bioinformatics + clustering + microarray data	Extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications. In this paper, we explore a novel type of gene-sample-time microarray data sets, which records the expression levels of various genes under a set of samples during a series of time points. In particular, we propose the mining of coherent gene clusters from such data sets. Each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series. The coherent gene clusters may identify the samples corresponding to some phenotypes (e.g., diseases), and suggest the candidate genes correlated to the phenotypes. We present two efficient algorithms, namely the Sample-Gene Search and the Gene-Sample Search, to mine the complete set of coherent gene clusters. We empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets. The test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters.
NE8560	Knowledge Discovery and Data Mining	David S. Vogel+Morgan C. Wang	2004	1-dimensional splines as building blocks for improving accuracy of risk outcomes models.	 + Machine learning + Computing methodologies	+prediction+adaptive+risk+outcomes	Transformation of both the response variable and the predictors is commonly used in fitting regression models. However, these transformation methods do not always provide the maximum linear correlation between the response variable and the predictors, especially when there are non-linear relationships between predictors and the response such as the medical data set used in this study. A spline based transformation method is proposed that is second order smooth, continuous, and minimizes the mean squared error between the response and each predictor. Since the computation time for generating this spline is O(n), the processing time is reasonable with massive data sets. In contrast to cubic smoothing splines, the resulting transformation equations also display a high level of efficiency for scoring. Data used for predicting health outcomes contains an abundance of non-linear relationships between predictors and the outcomes requiring an algorithm for modeling them accurately. Thus, a transformation that fits an adaptive cubic spline to each of a set of variables is proposed. These curves are used as a set of transformation functions on the predictors. A case study of how the transformed variables can be fed into a simple linear regression model to predict risk outcomes is presented. The results show significant improvement over the performance of the original variables in both linear and non-linear models.
NE8497	Knowledge Discovery and Data Mining	Tsuyoshi IDÃ+Hisashi KASHIMA	2004	Eigenspace-based anomaly detection in computer systems.	 + Management of computing and information systems + System management + File systems management + Machine learning + Computing methodologies + Professional topics + Information systems applications + Data mining + Social and professional topics + Information systems	+principal eigenvector+Perron-Frobenius theorem+von Mises-Fisher distribution+singular value de	"We report on an automated runtime anomaly detection method at the application layer of multi-node computer systems. Although several network management systems are available in the market, none of them have sufficient capabilities to detect faults in multi-tier Web-based systems with redundancy. We model a Web-based system as a weighted graph, where each node represents a ""service"" and each edge represents a dependency between services. Since the edge weights vary greatly over time, the problem we address is that of anomaly detection from a time sequence of graphs.In our method, we first extract a feature vector from the adjacency matrix that represents the activities of all of the services. The heart of our method is to use the principal eigenvector of the eigenclusters of the graph. Then we derive a probability distribution for an anomaly measure defined for a time-series of directional data derived from the graph sequence. Given a critical probability, the threshold value is adaptively updated using a novel online algorithm.We demonstrate that a fault in a Web application can be automatically detected and the faulty services are identified without using detailed knowledge of the behavior of the system."
763662B0	Knowledge Discovery and Data Mining	lian yan + david verbel + olivier saidi	2004	Predicting prostate cancer recurrence via maximizing the concordance index	 + cox proportional hazards model + censored data + neural networks + indexation + Learning paradigms + Information systems applications + Computing methodologies + machine learning + Data mining + neural network + Information systems + support vector machine + survival analysis + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + nomogram + Supervised learning by classification + objective function	+concordance index+neural networks+prostate cancer recurrence+nomogram	In order to effectively use machine learning algorithms, e.g., neural networks, for the analysis of survival data, the correct treatment of censored data is crucial. The concordance index (CI) is a typical metric for quantifying the predictive ability of a survival model. We propose a new algorithm that directly uses the CI as the objective function to train a model, which predicts whether an event will eventually occur or not. Directly optimizing the CI allows the model to make complete use of the information from both censored and non-censored observations. In particular, we approximate the CI via a differentiable function so that gradient-based methods can be used to train the model. We applied the new algorithm to predict the eventual recurrence of prostate cancer following radical prostatectomy. Compared with the traditional Cox proportional hazards model and several other algorithms based on neural networks and support vector machines, our algorithm achieves a significant improvement in being able to identify high-risk and low-risk groups of patients.
78764CD2	Knowledge Discovery and Data Mining	moninder singh + jayant r kalagnanam + sudhir verma + yuk wah wong + michael patek	2004	A system for automated mapping of bill-of-materials part numbers	 + domain knowledge + data mining + Information systems applications + Information retrieval + Data mining + Information systems	+Mapping	Part numbers are widely used within an enterprise throughout the manufacturing process. The point of entry of such part numbers into this process is normally via a Bill of Materials, or BOM, sent by a contact manufacturer or supplier. Each line of the BOM provides information about one part such as the supplier part number, the BOM receiver's corresponding internal part number, an unstructured textual part description, the supplier name, etc. However, in a substantial number of cases, the BOM receiver's internal part number is absent. Hence, before this part can be incorporated into the receiver's manufacturing process, it has to be mapped to an internal part (of the BOM receiver) based on the information of the part in the BOM. Historically, this mapping process has been done manually which is a highly time-consuming, labor intensive and error-prone process. This paper describes a system for automating the mapping of BOM part numbers. The system uses a two step modeling and mapping approach. First, the system uses historical BOM data, receiver's part specifications data and receiver's part taxonomic data along with domain knowledge to automatically learn classification models for mapping a given BOM part description to successively lower levels of the receiver's part taxonomy to reduce the set of potential internal parts to which the BOM part could map to. Then, information about various part parameters is extracted from the BOM part description and compared to the specifications data of the potential internal parts to choose the final mapped internal part. Mappings done by the system are very accurate, and the system is currently being deployed within IBM for mapping BOMs received by the corporate procurement/manufacturing divisions.
7C33D673	Knowledge Discovery and Data Mining	andrew w moore + daniel b neill	2004	Rapid detection of significant spatial clusters	 + data structure + Information systems applications + Data mining + kd tree + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Apps- Data Mining General Terms Algorithms Keywords Cluster detection	Given an N x N grid of squares, where each square has a count cij and an underlying population pij, our goal is to find the rectangular region with the highest density, and to calculate its significance by randomization. An arbitrary density function D, dependent on a region's total count C and total population P, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff's spatial scan statistic DK to find the most significant spatial disease cluster. A naive approach to finding the maximum density region requires O(N4) time, and is generally computationally infeasible. We present a multiresolution algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufficiently dense regions, this method finds the maximum density region in O((N log N)2) time, in practice resulting in significant (20-2000x) speedups on both real and simulated datasets.
58C38F27	Knowledge Discovery and Data Mining	mrudula pavuluri + olfa nasraoui	2004	Complete This Puzzle: A Connectionist Approach to Accurate Web Recommendations Based on a Committee of Predictors	recommender system + web usage mining + collaborative filtering + neural network + ground truth	personalization + recommender systems + collaborative filtering + web usage mining + neural networks	We present a Context Ultra-Sensitive Approach based on two-step Recommender systems (CUSA-2-step-Rec). Our approach relies on a committee of profile-specific neural networks. This approach provides recommendations that are accurate and fast to train because only the URLs relevant to a specific profile are used to define the architecture of each network. Similar to the task of completing the missing pieces of a puzzle, each neural network is trained to predict the missing URLs of several complete ground-truth sessions from a given profile, given as input several incomplete subsessions. We compare the proposed approach with collaborative filtering showing that our approach achieves higher coverage and precision while being faster, and requiring lower main memory at recommendation time. While most recommenders are inherently context sensitive, our approach is context ultra-sensitive because a different recommendation model is designed for each profile separately.
7FA69AFC	Knowledge Discovery and Data Mining	rebecca n wright + zhiqiang yang	2004	Privacy-preserving Bayesian network structure computation on distributed heterogeneous data	computer network + distributed database +  + bayesian network + data mining + distributed databases + Information systems applications + data privacy + Data mining + Information systems	+Categories & Subject Descriptors+H.2.8 [Database Applications+Data Mining General Terms+Security Keywords+Bayesian Network+Privacy-Preserving Data Mining+Distributed Databases	As more and more activities are carried out using computers and computer networks, the amount of potentially sensitive data stored by business, governments, and other parties increases. Different parties may wish to benefit from cooperative use of their data, but privacy regulations and other privacy concerns may prevent the parties from sharing their data. Privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data is not revealed.In this paper, we present a privacy-preserving protocol for a particular data mining task: learning the Bayesian network structure for distributed heterogeneous data. In this setting, two parties owning confidential databases wish to learn the structure of Bayesian network on the combination of their databases without revealing anything about their data to each other. We give an efficient and privacy-preserving version of the K2 algorithm to construct the structure of a Bayesian network for the parties' joint data.
NE8551	Knowledge Discovery and Data Mining	Erick CantÃº-Paz+Shawn Newsam+Chandrika Kamath	2004	Feature selection in scientific applications.	 + Feature selection + Machine learning algorithms + Machine learning + Computing methodologies	+Categories and Subject Descriptors I.5.2 [Pattern Recognition+Design Methodologyâfeature evaluation and selection. General Terms Algorithms+Experimentation+Measurement Keywords Feature selection+data mining+astronomical survey+plasma physics+remote	Numerous applications of data mining to scientific data involve the induction of a classification model. In many cases, the collection of data is not performed with this task in mind, and therefore, the data might contain irrelevant or redundant features that affect negatively the accuracy of the induction algorithms. The size and dimensionality of typical scientific data make it difficult to use any available domain information to identify features that discriminate between the classes of interest. Similarly, exploratory data analysis techniques have limitations on the amount and dimensionality of the data they can process effectively. In this paper, we describe applications of efficient feature selection methods to data sets from astronomy, plasma physics, and remote sensing. We use variations of recently proposed filter methods as well as traditional wrapper approaches, where practical. We discuss the general challenges of feature selection in scientific datasets, the strategies for success that were common among our diverse applications, and the lessons learned in solving these problems.
NE8468	Knowledge Discovery and Data Mining	TamÃ¡s HorvÃ¡th+Thomas GÃ¤rtner+Stefan Wrobel	2004	Cyclic pattern kernels for predictive graph mining.	 + Supervised learning + Classification and regression trees + Learning paradigms + Machine learning + Information systems applications + Machine learning approaches + Computing methodologies + Data mining + Supervised learning by classification + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- Data Mining+I.2.6 [Artificial Intelligence+Learning+I.5.2 [Pattern Recognition+Design Methodology- clas- sifier design and evaluation General Terms Algorithms+Experimentation Keywords graph mining+kernel methods+computational chemistry	With applications in biology, the world-wide web, and several other areas, mining of graph-structured objects has received significant interest recently. One of the major research directions in this field is concerned with predictive data mining in graph databases where each instance is represented by a graph. Some of the proposed approaches for this task rely on the excellent classification performance of support vector machines. To control the computational cost of these approaches, the underlying kernel functions are based on frequent patterns. In contrast to these approaches, we propose a kernel function based on a natural set of cyclic and tree patterns independent of their frequency, and discuss its computational aspects. To practically demonstrate the effectiveness of our approach, we use the popular NCI-HIV molecule dataset. Our experimental results show that cyclic pattern kernels can be computed quickly and offer predictive performance superior to recent graph kernels based on frequent patterns.
NE8462	Knowledge Discovery and Data Mining	Nilesh Dalvi+Pedro Domingos+Mausam+Sumit Sanghai+Deepak Verma	2004	Adversarial classification.	 + Feature selection + Machine learning algorithms + Multivariate statistics + Machine learning theory + Learning paradigms + Information systems applications + Computing methodologies + Data mining + Information systems + Theory and algorithms for application domains + Theory of computation + Logical and relational learning + Inductive logic learning + Markov decision processes + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + Probability and statistics + Mathematics of computing + Supervised learning by classification	No keyword found	Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.
5CE3E79E	Knowledge Discovery and Data Mining	xiaoyong chai + diklun lee + lin deng + wilfred ng	2004	Spying Out Accurate User Preferences for Search Engine Adaptation	search engine		Most existing search engines employ static ranking algorithms that do not adapt to the specific needs of users. Recently, some researchers have studied the use of clickthrough data to adapt a search engineâs ranking function. Clickthrough data indicate for each query the results that are clicked by users. As a kind of implicit relevance feedback information, clickthrough data can easily be collected by a search engine. However, clickthrough data is sparse and incomplete, thus, it is a challenge to discover accurate user preferences from it. In this paper, we propose a novel algorithm called âSpy NaÃ¯ve Bayesâ (SpyNB) to identify user preferences generated from clickthrough data. First, we treat the result items clicked by the users as sure positive examples and those not clicked by the users as unlabelled data. Then, we plant the sure positive examples (the spies) into the unlabelled set of result items and apply a naÃ¯ve Bayes classification to generate the reliable negative examples. These positive and negative examples allow us to discover more accurate userâs preferences. Finally, we employ the SpyNB algorithm with a ranking SVM optimizer to build an adaptive metasearch engine. Our experimental results show that, compared with the original ranking, SpyNB can significantly improve the average ranks of usersâ click by 20%.
77D5BF45	Knowledge Discovery and Data Mining	steve donoho	2004	Early detection of insider trading in option markets	 + decision tree + data mining + Machine learning + Information systems applications + Computing methodologies + knowledge discovery + neural net + logistic regression + Data mining + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications â Data mining+I.2.6 [Artificial Intelligence+Learning General Terms Algorithms+Design+Experimentation. Keywords Insider trading	"""Inside information"" comes in many forms: knowledge of a corporate takeover, a terrorist attack, unexpectedly poor earnings, the FDA's acceptance of a new drug, etc. Anyone who knows some piece of soon-to-break news possesses inside information. Historically, insider trading has been detected after the news is public, but this is often too late: fraud has been perpetrated, innocent investors have been disadvantaged, or terrorist acts have been carried out. This paper explores early detection of insider trading - detection before the news breaks. Data mining holds great promise for this emerging application, but the problem also poses significant challenges. We present the specific problem of insider trading in option markets, compare decision tree, logistic regression, and neural net results to results from an expert model, and discuss insights that knowledge discovery techniques shed upon this problem."
76CE2046	Sigkdd Explorations	ronaldo c prati + maria carolina monard + gustavo e a p a batista	2004	A study of the behavior of several methods for balancing machine learning training data	 + decision tree + sampling methods + data cleaning + machine learning + roc curve		"

"
755C4022	Knowledge Discovery and Data Mining	wei fan	2004	Systematic data selection to mine concept-drifting data streams	 + decision tree + concept drift + cross validation + data streams + decision trees + Information systems applications + Data mining + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications- Data Min- ing General Terms Algorithms Keywords data streams	"One major problem of existing methods to mine data streams is that it makes ad hoc choices to combine most recent data with some amount of old data to search the new hypothesis. The assumption is that the additional old data always helps produce a more accurate hypothesis than using the most recent data only. We first criticize this notion and point out that using old data blindly is not better than ""gambling""; in other words, it helps increase the accuracy only if we are ""lucky."" We discuss and analyze the situations where old data will help and what kind of old data will help. The practical problem on choosing the right example from old data is due to the formidable cost to compare different possibilities and models. This problem will go away if we have an algorithm that is extremely efficient to compare all sensible choices with little extra cost. Based on this observation, we propose a simple, efficient and accurate cross-validation decision tree ensemble method."
NE8526	Knowledge Discovery and Data Mining	Avraham A. Melkman+Eran Shaham	2004	Sleeved coclustering.	 + Cluster analysis + Learning paradigms + Machine learning + Information systems applications + Computing methodologies + Data mining + Unsupervised learning + Information systems	+Categories and Subject Descriptors+H.2.8 Database Management+Database Applications- Data Mining+I.5.3	A coCluster of a m x n matrix X is a submatrix determined by a subset of the rows and a subset of the columns. The problem of finding coClusters with specific properties is of interest, in particular, in the analysis of microarray experiments. In that case the entries of the matrix X are the expression levels of $m$ genes in each of $n$ tissue samples. One goal of the analysis is to extract a subset of the samples and a subset of the genes, such that the expression levels of the chosen genes behave similarly across the subset of the samples, presumably reflecting an underlying regulatory mechanism governing the expression level of the genes.We propose to base the similarity of the genes in a coCluster on a simple biological model, in which the strength of the regulatory mechanism in sample j is Hj, and the response strength of gene i to the regulatory mechanism is Gi. In other words, every two genes participating in a good coCluster should have expression values in each of the participating samples, whose ratio is a constant depending only on the two genes. Noise in the expression levels of genes is taken into account by allowing a deviation from the model, measured by a relative error criterion. The sleeve-width of the coCluster reflects the extent to which entry i,j in the coCluster is allowed to deviate, relatively, from being expressed as the product GiHj.We present a polynomial-time Monte-Carlo algorithm which outputs a list of coClusters whose sleeve-widths do not exceed a prespecified value. Moreover, we prove that the list includes, with fixed probability, a coCluster which is near-optimal in its dimensions. Extensive experimentation with synthetic data shows that the algorithm performs well.
NE8521	Knowledge Discovery and Data Mining	Aleksander KoÅcz+Abdur Chowdhury+Joshua Alspector	2004	Improved robustness of signature-based near-replica detection via lexicon randomization.	 + Information systems applications + Data mining + Information systems	No keyword found	Detection of near duplicate documents is an important problem in many data mining and information filtering applications. When faced with massive quantities of data, traditional duplicate detection techniques relying on direct inter-document similarity computation (e.g., using the cosine measure) are often not feasible given the time and memory performance constraints. On the other hand, fingerprint-based methods, such as I-Match, are very attractive computationally but may be brittle with respect to small changes to document content. We focus on approaches to near-replica detection that are based upon large-collection statistics and present a general technique of increasing their robustness via multiple lexicon randomization. In experiments with large web-page and spam-email datasets the proposed method is shown to consistently outperform traditional I-Match, with the relative improvement in duplicate-document recall reaching as high as 40-60%. The large gains in detection accuracy are offset by only small increases in computational requirements.
814DDE31	Knowledge Discovery and Data Mining	jiawei han + michael garland + andrew y wu	2004	Mining scale-free networks using geodesic clustering	scale free network +  + scale free networks + social networks + data structure + Information systems applications + Data mining + graph clustering + shortest path + Information systems + social network + data reduction + graphs + power law distribution + scale free + clustering	graphs + social networks + scale-free networks + clustering	Many real-world graphs have been shown to be scale-free---vertex degrees follow power law distributions, vertices tend to cluster, and the average length of all shortest paths is small. We present a new model for understanding scale-free networks based on multilevel geodesic approximation, using a new data structure called a multilevel mesh.Using this multilevel framework, we propose a new kind of graph clustering for data reduction of very large graph systems such as social, biological, or electronic networks. Finally, we apply our algorithms to real-world social networks and protein interaction graphs to show that they can reveal knowledge embedded in underlying graph structures. We also demonstrate how our data structures can be used to quickly answer approximate distance and shortest path queries on scale-free networks.
80E17C8D	Knowledge Discovery and Data Mining	jiawei han + hong cheng + xifeng yan	2004	IncSpan: incremental mining of sequential patterns in large database	 + pattern matching + Information systems applications + sequential pattern mining + Data mining + Information systems	incremental mining + buffering pattern + reverse pattern matching + shared projection	Many real life sequence databases grow incrementally. It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow, or when some new sequences are added into the database. Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates. However, it is nontrivial to mine sequential patterns incrementally, especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones. In this study, we develop an efficient algorithm, IncSpan, for incremental mining of sequential patterns, by exploring some interesting properties. Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin.
7BB98613	Knowledge Discovery and Data Mining	yanzan zhou + xin jin + bamshad mobasher	2004	Web usage mining based on probabilistic latent semantic analysis	 + association rule + web usage mining + probabilistic latent semantic analysis + web navigation + Machine learning + Information systems applications + Computing methodologies + Data mining + plsa + Information systems	Web usage mining + Latent variable models + User profiling + PLSA	The primary goal of Web usage mining is the discovery of patterns in the navigational behavior of Web users. Standard approaches, such as clustering of user sessions and discovering association rules or frequent navigational paths, do not generally provide the ability to automatically characterize or quantify the unobservable factors that lead to common navigational patterns. It is, therefore, necessary to develop techniques that can automatically discover hidden semantic relationships among users as well as between users and Web objects. Probabilistic Latent Semantic Analysis (PLSA) is particularly useful in this context, since it can uncover latent semantic associations among users and pages based on the co-occurrence patterns of these pages in user sessions. In this paper, we develop a unified framework for the discovery and analysis of Web navigational patterns based on PLSA. We show the flexibility of this framework in characterizing various relationships among users and Web objects. Since these relationships are measured in terms of probabilities, we are able to use probabilistic inference to perform a variety of analysis tasks such as user segmentation, page classification, as well as predictive tasks such as collaborative recommendations. We demonstrate the effectiveness of our approach through experiments performed on real-world data sets.
7E97094D	Knowledge Discovery and Data Mining	david jensen + jennifer neville + brian gallagher	2004	Why collective inference improves relational classification	relational data +  + Machine learning + Computing methodologies + Machine learning approaches + statistical model + relational learning	Relational learning + probabilistic relational models + collective inference	Procedures for collective inference make simultaneous statistical judgments about the same variables for a set of related data instances. For example, collective inference could be used to simultaneously classify a set of hyperlinked documents or infer the legitimacy of a set of related financial transactions. Several recent studies indicate that collective inference can significantly reduce classification error when compared with traditional inference techniques. We investigate the underlying mechanisms for this error reduction by reviewing past work on collective inference and characterizing different types of statistical models used for making inference in relational data. We show important differences among these models, and we characterize the necessary and sufficient conditions for reduced classification error based on experiments with real and simulated data.
7BE5F92C	Knowledge Discovery and Data Mining	jinwen ma + lin deng + dik lun lee + jian pei	2004	A rank sum test method for informative gene discovery	 + support vector machine + microarray data + test methods + Information systems applications + normal distribution + classification + Data mining + Information systems	No keyword found	Finding informative genes from microarray data is an important research problem in bioinformatics research and applications. Most of the existing methods rank features according to their discriminative capability and then find a subset of discriminative genes (usually top k genes). In particular, t-statistic criterion and its variants have been adopted extensively. This kind of methods rely on the statistics principle of t-test, which requires that the data follows a normal distribution. However, according to our investigation, the normality condition often cannot be met in real data sets.To avoid the assumption of the normality condition, in this paper, we propose a rank sum test method for informative gene discovery. The method uses a rank-sum statistic as the ranking criterion. Moreover, we propose using the significance level threshold, instead of the number of informative genes, as the parameter. The significance level threshold as a parameter carries the quality specification in statistics. We follow the Pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t-statistic method in theory.To verify the effectiveness of the rank sum method, we use support vector machine (SVM) to construct classifiers based on the identified informative genes on two well known data sets, namely colon data and leukemia data. The prediction accuracy reaches 96.2% on the colon data and 100% on the leukemia data. The results are clearly better than those from the previous feature ranking methods. By experiments, we also verify that using significance level threshold is more effective than directly specifying an arbitrary k.
766011FA	Knowledge Discovery and Data Mining	inderjit s dhillon + yuqiang guan + brian kulis	2004	Kernel k-means: spectral clustering and normalized cuts	 + Cluster analysis + Learning paradigms + Computing methodologies + Information retrieval + handwriting recognition + positive definite + local search + Unsupervised learning + Information systems + k means algorithm + eigenvectors + Machine learning + k means + graph partitioning + gene expression + spectral clustering + objective function	Spectral Clustering + Kernel k -means + Graph Partitioning	Kernel k-means and spectral clustering have both been used to identify clusters that are non-linearly separable in input space. Despite significant research, these methods have remained only loosely related. In this paper, we give an explicit theoretical connection between them. We show the generality of the weighted kernel k-means objective function, and derive the spectral clustering objective of normalized cut as a special case. Given a positive definite similarity matrix, our results lead to a novel weighted kernel k-means algorithm that monotonically decreases the normalized cut. This has important implications: a) eigenvector-based algorithms, which can be computationally prohibitive, are not essential for minimizing normalized cuts, b) various techniques, such as local search and acceleration schemes, may be used to improve the quality as well as speed of kernel k-means. Finally, we present results on several interesting data sets, including diametrical clustering of large gene-expression matrices and a handwriting recognition data set.
80C9116C	Knowledge Discovery and Data Mining	bing liu + minqing hu	2004	Mining and summarizing customer reviews	 + summarization + Information systems applications + Computing methodologies + e commerce + text mining + text summarization + Natural language processing + Data mining + Artificial intelligence + Information systems + Language resources	eol>Text mining + sentiment classification + summarization + reviews	Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.
7FA95F3D	Knowledge Discovery and Data Mining	dharmendra s modha + christos faloutsos + deepayan chakrabarti + spiros papadimitriou	2004	Fully automatic cross-associations	 + k means clustering + collaborative filtering + algorithms + information retrieval + Information retrieval + decomposition + principal component + parallel processing + Information systems + social network + Document representation + graphs + sparse matrix + clustering + scaling factor + information theory + first principle + Mathematics of computing + graph partitioning + intervention + Information theory	No keyword found	"Large, sparse binary matrices arise in numerous data mining applications, such as the analysis of market baskets, web graphs, social networks, co-citations, as well as information retrieval, collaborative filtering, sparse matrix reordering, etc. Virtually all popular methods for the analysis of such matrices---e.g., k-means clustering, METIS graph partitioning, SVD/PCA and frequent itemset mining---require the user to specify various parameters, such as the number of clusters, number of principal components, number of partitions, and ""support."" Choosing suitable values for such parameters is a challenging problem.Cross-association is a joint decomposition of a binary matrix into disjoint row and column groups such that the rectangular intersections of groups are homogeneous. Starting from first principles, we furnish a clear, information-theoretic criterion to choose a good cross-association as well as its parameters, namely, the number of row and column groups. We provide scalable algorithms to approach the optimal. Our algorithm is parameter-free, and requires no user intervention. In practice it scales linearly with the problem size, and is thus applicable to very large matrices. Finally, we present experiments on multiple synthetic and real-life datasets, where our method gives high-quality, intuitive results."
0C4B631B	Knowledge Discovery and Data Mining	daniel hunter + amy kao + eric k jones + daniel f bostwick + nicholas j pioch + james v white	2004	Multi-Hypothesis Abductive Reasoning for Link Discovery	structured data + abductive reasoning + knowledge representation + link analysis + data mining + data visualization + hidden markov model	{npioch + dhunter + jwhite + akao + bostwick + ejones}@alphatech + com	"
Intelligence agencies are under increasing pressure to âconnect the dotsâ between fragments of evidence from disparate sources to enable preemption of potential threats such as terrorist attacks. Most systems for threat detection in use today provide only data visualization tools for manual âlink analysis,â leading to methods that do not scale to massive data sets. The CADRE system (Continuous Analysis and Discovery from Relational Evidence) addresses this deficiency by automating the link analysis process. CADRE combines an expressive knowledge representation of threat patterns with efficient, constraint-based abductive reasoning algorithms to automatically infer links and construct coherent threat hypotheses from structured data. A compact, factored representation of multiple hypotheses avoids redundant storage and enables scaling to large data sets. CADRE efficiently manages the growth of the hypotheses using probabilistic evaluation models and a consistency checking algorithm to prune unlikely hypotheses.
"
05B4BB3E	Knowledge Discovery and Data Mining	vincent ng + benny y m fung	2004	Meta-classification of Multi-type Cancer Gene Expression Data	majority voting + data mining + gene expression + gene family	Gene expression + meta-classification + heterogeneous + multi-type	"
Massive publicly available gene expression data consisting of different experimental conditions and microarray platforms introduce new challenges in data mining when integrating multiple gene expression data. In this work, we proposed a metaclassification algorithm, which is called MIF algorithm, to perform multi-type cancer gene expression data classification. It uses regular histograms for gene expression levels of certain significant genes to represent sample profiles. Differences between profiles are then used to obtain dissimilarity measures and indicators of predictive classes. In order to demonstrate the robustness of the algorithm, 10 different data sets, which are individually published in 8 publications, are experimented. The results show that the MIF algorithm outperforms the simple majority-voting meta-classification algorithm and has a good meta-classification performance. In addition, we also compare our results with other researchers' works, and the comparisons are impressive. Finally, we have confirmed our findings with cancer/testis (CT) immunogenic gene families of heterogeneous samples.
"
7D3BA44E	Knowledge Discovery and Data Mining	michael steinbach + vipin kumar + pangning tan	2004	Support envelopes: a technique for exploring the structure of association patterns	 + transaction data + association analysis + Information systems applications + Data mining + formal concept analysis + Information systems	support envelope + association analysis + formal concept analysis + error-tolerant itemsets	This paper introduces support envelopes---a new tool for analyzing association patterns---and illustrates some of their properties, applications, and possible extensions. Specifically, the support envelope for a transaction data set and a specified pair of positive integers (m,n) consists of the items and transactions that need to be searched to find any association pattern involving m or more transactions and n or more items. For any transaction data set with M transactions and N items, there is a unique lattice of at most M*N support envelopes that captures the structure of the association patterns in that data set. Because support envelopes are not encumbered by a support threshold, this support lattice provides a complete view of the association structure of the data set, including association patterns that have low support. Furthermore, the boundary of the support lattice---the support boundary---has at most min(M,N) envelopes and is especially interesting since it bounds the maximum sizes of potential association patterns---not only for frequent, closed, and maximal itemsets, but also for patterns, such as error-tolerant itemsets, that are more general. The association structure can be represented graphically as a two-dimensional scatter plot of the (m,n) values associated with the support envelopes of the data set, a feature that is useful in the exploratory analysis of association patterns. Finally, the algorithm to compute support envelopes is simple and computationally efficient, and it is straightforward to parallelize the process of finding all the support envelopes.
80E6415F	Knowledge Discovery and Data Mining	krishna kummamuru + rakesh agrawal + raghu krishnapuram	2004	Learning spatially variant dissimilarity (SVaD) measures	 + feature space + Information systems applications + clustering + Data mining + feature vector + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data Mining General Terms Algorithms Keywords Clustering+Learning Dissimilarity Measures	Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed (dis)similarity measure between the data points in feature space. This makes the type of clusters identified highly dependent on the assumed similarity measure. Building on recent work in this area, we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data. The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure. Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature.
75C4F8B9	Knowledge Discovery and Data Mining	jude shavlik + mark shavlik	2004	Selection, combination, and evaluation of effective software sensors for detecting abnormal computer usage	 + user modeling + Computing methodologies + Systems security + intrusion detection + anomaly detection + machine learning + Operating systems security + user model + false alarm rate + intrusion detection system + winnow algorithm + Security and privacy + Machine learning + feature selection	eol>Intrusion detection + anomaly detection + machine learning + user modeling + Windows 2000 + feature selection + Winnow algorithm	"We present and empirically analyze a machine-learning approach for detecting intrusions on individual computers. Our Winnow-based algorithm continually monitors user and system behavior, recording such properties as the number of bytes transferred over the last 10 seconds, the programs that currently are running, and the load on the CPU. In all, hundreds of measurements are made and analyzed each second. Using this data, our algorithm creates a model that represents each particular computer's range of normal behavior. Parameters that determine when an alarm should be raised, due to abnormal activity, are set on a per-computer basis, based on an analysis of training data. A major issue in intrusion-detection systems is the need for very low false-alarm rates. Our empirical results suggest that it is possible to obtain high intrusion-detection rates (95%) and low false-alarm rates (less than one per day per computer), without ""stealing"" too many CPU cycles (less than 1%). We also report which system measurements are the most valuable in terms of detecting intrusions. A surprisingly large number of different measurements prove significantly useful."
8046EB2F	Knowledge Discovery and Data Mining	murat kantarcioÇ§lu + jiashun jin + chris clifton	2004	When do data mining results violate privacy?	 + data mining + secure multiparty computation + Information systems applications + privacy + Data mining + Information systems + Theory and algorithms for application domains + Theory of computation + Database and storage security + Security and privacy + Theory of database privacy and security + Database theory	Individual Privacy + Protect the ârecordâ	Privacy-preserving data mining has concentrated on obtaining valid results when the input data is private. An extreme example is Secure Multiparty Computation-based methods, where only the results are revealed. However, this still leaves a potential privacy breach: Do the results themselves violate privacy? This paper explores this issue, developing a framework under which this question can be addressed. Metrics are proposed, along with analysis that those metrics are consistent in the face of apparent problems.
7B8CFBD8	Knowledge Discovery and Data Mining	naval k verma + chid apte + naoki abe + robert schroko	2004	Cross channel optimized marketing by reinforcement learning	profitability +  + markov decision process + Machine learning + Computing methodologies + customer life time value + customer relationship management + crm + reinforcement learning	+Categories and Subject Descriptors I.2.6 [Artificial Intelligence+Learning General Terms Experimentation Keywords customer life time value+CRM+cost sensitive learning+rein- forcement	The issues of cross channel integration and customer life time value modeling are two of the most important topics surrounding customer relationship management (CRM) today. In the present paper, we describe and evaluate a novel solution that treats these two important issues in a unified framework of Markov Decision Processes (MDP). In particular, we report on the results of a joint project between IBM Research and Saks Fifth Avenue to investigate the applicability of this technology to real world problems. The business problem we use as a testbed for our evaluation is that of optimizing direct mail campaign mailings for maximization of profits in the store channel. We identify a problem common to cross-channel CRM, which we call the Cross-Channel Challenge, due to the lack of explicit linking between the marketing actions taken in one channel and the customer responses obtained in another. We provide a solution for this problem based on old and new techniques in reinforcement learning. Our in-laboratory experimental evaluation using actual customer interaction data show that as much as 7 to 8 per cent increase in the store profits can be expected, by employing a mailing policy automatically generated by our methodology. These results confirm that our approach is valid in dealing with the cross channel CRM scenarios in the real world.
7FF27F2D	Knowledge Discovery and Data Mining	arindam banerjee + j langford	2004	An objective evaluation criterion for clustering	evaluation + Cluster analysis +  + text analysis + Learning paradigms + Machine learning + Computing methodologies + object model + clustering + Unsupervised learning	Clustering + Evaluation + PAC bounds + MDL	We propose and test an objective criterion for evaluation of clustering performance: How well does a clustering algorithm run on unlabeled data aid a classification algorithm? The accuracy is quantified using the PAC-MDL bound [3] in a semisupervised setting. Clustering algorithms which naturally separate the data according to (hidden) labels with a small number of clusters perform well. A simple extension of the argument leads to an objective model selection method. Experimental results on text analysis datasets demonstrate that this approach empirically results in very competitive bounds on test set performance on natural datasets.
7D15BDCB	Knowledge Discovery and Data Mining	bianca zadrozny + naoki abe + john langford	2004	An iterative method for multi-class cost-sensitive learning	iteration method +  + boosting + multi class classification + Machine learning + Computing methodologies + binary classification	+Categories and Subject Descriptors I.2.6 [Artificial Intelligence+Learning General Terms Algorithms Keywords cost-sensitive learning	Cost-sensitive learning addresses the issue of classification in the presence of varying costs associated with different types of misclassification. In this paper, we present a method for solving multi-class cost-sensitive learning problems using any binary classification algorithm. This algorithm is derived using hree key ideas: 1) iterative weighting; 2) expanding data space; and 3) gradient boosting with stochastic ensembles. We establish some theoretical guarantees concerning the performance of this method. In particular, we show that a certain variant possesses the boosting property, given a form of weak learning assumption on the component binary classifier. We also empirically evaluate the performance of the proposed method using benchmark data sets and verify that our method generally achieves better results than representative methods for cost-sensitive learning, in terms of predictive performance (cost minimization) and, in many cases, computational efficiency.
7AC2A296	Knowledge Discovery and Data Mining	charles elkan + andrew t smith	2004	A Bayesian network framework for reject inference	conditional independence +  + bayesian network + Machine learning theory + Learning paradigms + propensity scores + sample selection bias + Data mining + biased sampling + Information systems + Theory and algorithms for application domains + Theory of computation + Machine learning approaches + Probability and statistics + Mathematics of computing + Supervised learning by classification + bayesian networks + clinical trial + rejection sampling + Information systems applications + Computing methodologies + Markov decision processes + Supervised learning + Classification and regression trees + Machine learning + missing at random + expectation maximization + propensity score	+Categories and Subject Descriptors G.3 [Probability and Statistics+statistical computing+H.2.8 [Database Management+Database Applicationsâ data mining+I.2.6 [Artificial Intelligence+Learningâ parameter learning+I.5.2 [Information Storage and Re- trieval+Design Methodologyâclassifier design and evalu- ation	Most learning methods assume that the training set is drawn randomly from the population to which the learned model is to be applied. However in many applications this assumption is invalid. For example, lending institutions create models of who is likely to repay a loan from training sets consisting of people in their records to whom loans were given in the past; however, the institution approved loan applications previously based on who was thought unlikely to default. Learning from only approved loans yields an incorrect model because the training set is a biased sample of the general population of applicants. The issue of including rejected samples in the learning process, or alternatively using rejected samples to adjust a model learned from accepted samples only, is called reject inference.The main contribution of this paper is a systematic analysis of different cases that arise in reject inference, with explanations of which cases arise in various real-world situations. We use Bayesian networks to formalize each case as a set of conditional independence relationships and identify eight cases, including the familiar missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) cases. For each case we present an overview of available learning algorithms. These algorithms have been published in separate fields of research, including epidemiology, econometrics, clinical trial evaluation, sociology, and credit scoring; our second major contribution is to describe these algorithms in a common framework.
7F09A5EB	Knowledge Discovery and Data Mining	vipin kumar + hui xiong + michael steinbach + pangning tan	2004	Generalizing the notion of support	 + algorithms + association analysis + support function + Information systems applications + Data mining + support + Information systems	association analysis + support + hyperclique	The goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non-traditional types of patterns and non-binary data. To that end, we describe a framework for generalizing support that is based on the simple, but useful observation that support can be viewed as the composition of two functions: a function that evaluates the strength or presence of a pattern in each object (transaction) and a function that summarizes these evaluations with a single number. A key goal of any framework is to allow people to more easily express, explore, and communicate ideas, and hence, we illustrate how our support framework can be used to describe support for a variety of commonly used association patterns, such as frequent itemsets, general Boolean patterns, and error-tolerant itemsets. We also present two examples of the practical usefulness of generalized support. One example shows the usefulness of support functions for continuous data. Another example shows how the hyperclique pattern---an association pattern originally defined for binary data---can be extended to continuous data by generalizing a support function.
5AFF3A61	Knowledge Discovery and Data Mining	rema padman + xue bai + edoardo m airoldi	2004	Markov blankets and meta-heuristics search: sentiment extraction from unstructured texts	machine learning + high frequency + market research + heuristic search		Extracting sentiments from unstructured text has emerged as an important problem in many disciplines. An accurate method would enable us, for example, to mine online opinions from the Internet and learn customersâ preferences for economic or marketing research, or for leveraging a strategic advantage. In this paper, we propose a two-stage Bayesian algorithm that is able to capture the dependencies among words, and, at the same time, finds a vocabulary that is efficient for the purpose of extracting sentiments. Experimental results on online movie reviews and online news show that our algorithm is able to select a parsimonious feature set with substantially fewer predictor variables than in the full data set and leads to better predictions about sentiment orientations than several state-of-the-art machine learning methods. Our findings suggest that sentiments are captured by conditional dependence relations among words, rather than by keywords or high-frequency words.
76C90D49	Knowledge Discovery and Data Mining	rong ge + martin ester + wen jin + zengjian hu	2004	A microeconomic data mining problem: customer-oriented catalog segmentation	 + data mining + Information systems applications + clustering + randomized algorithm + greedy algorithm + Data mining + Information systems	microeconomic data mining + catalog segmentation + clustering	The microeconomic framework for data mining [7] assumes that an enterprise chooses a decision maximizing the overall utility over all customers where the contribution of a customer is a function of the data available on that customer. In Catalog Segmentation, the enterprise wants to design k product catalogs of size r that maximize the overall number of catalog products purchased. However, there are many applications where a customer, once attracted to an enterprise, would purchase more products beyond the ones contained in the catalog. Therefore, in this paper, we investigate an alternative problem formulation, that we call Customer-Oriented Catalog Segmentation, where the overall utility is measured by the number of customers that have at least a specified minimum interest t in the catalogs. We formally introduce the Customer-Oriented Catalog Segmentation problem and discuss its complexity. Then we investigate two different paradigms to design efficient, approximate algorithms for the Customer-Oriented Catalog Segmentation problem, greedy (deterministic) and randomized algorithms. Since greedy algorithms may be trapped in a local optimum and randomized algorithms crucially depend on a reasonable initial solution, we explore a combination of these two paradigms. Our experimental evaluation on synthetic and real data demonstrates that the new algorithms yield catalogs of significantly higher utility compared to classical Catalog Segmentation algorithms.
7EF2A2FC	Knowledge Discovery and Data Mining	balaji padmanabhan + hong zhang + alexander tuzhilin	2004	On the discovery of significant statistical quantitative rules	 + satisfiability + market share + Information systems applications + confidence interval + Data mining + false discovery rate + resampling + Information systems	eol>Rule discovery + market share rules + statistical quantitative rules + nonparametric methods + resampling	"In this paper we study market share rules, rules that have a certain market share statistic associated with them. Such rules are particularly relevant for decision making from a business perspective. Motivated by market share rules, in this paper we consider statistical quantitative rules (SQ rules) that are quantitative rules in which the RHS can be any statistic that is computed for the segment satisfying the LHS of the rule. Building on prior work, we present a statistical approach for learning all significant SQ rules, i.e., SQ rules for which a desired statistic lies outside a confidence interval computed for this rule. In particular we show how resampling techniques can be effectively used to learn significant rules. Since our method considers the significance of a large number of rules in parallel, it is susceptible to learning a certain number of ""false"" rules. To address this, we present a technique that can determine the number of significant SQ rules that can be expected by chance alone, and suggest that this number can be used to determine a ""false discovery rate"" for the learning procedure. We apply our methods to online consumer purchase data and report the results."
5BA50A8F	Knowledge Discovery and Data Mining	maristella matera + rosa meo + pier luca lanzi + roberto esposito	2004	Integrating Web Conceptual Modeling and Web Usage Mining	conceptual model + case tool + spectrum + web usage mining + data mining + field experiment + query language + association rule	Association Rules + Inductive Databases + MINE RULE + Web Log + Web Structure Mining + Web Usage Mining	We present a case study about the application of the inductive database approach to the analysis of Web logs. We consider rich XML Web logs â called conceptual logs â that are generated by Web applications designed with the WebML conceptual model and developed with the WebRatio CASE tool. Conceptual logs integrate the usual information about user requests with meta-data concerning the structure of the content and the hypertext of a Web application. We apply a data mining language (MINE RULE) to conceptual logs in order to identify different types of patterns, such as: recurrent navigation paths, most frequently visited page contents, and anomalies (e.g., intrusion attempts or harmful usages of resources). We show that the exploitation of the nuggets of information embedded in the logs and of the specialized mining constructs provided by the query languages enables the rapid customization of the mining procedures following to the Web developersâ need. Given our on-field experience, we also suggest that the use of queries in advanced languages, as opposed to ad-hoc heuristics, eases the specification and the discovery of large spectrum of patterns.
595806D7	Knowledge Discovery and Data Mining	hema raghavan + andrew mccallum + james allan	2004	An Exploration of Entity Models, Collective Classification and Relation Description	bag of words + data mining + structured data + question answering + mean reciprocal rank + conditional random field + information retrieval + language model		"
Traditional information retrieval typically represents data using a bag of words; data mining typically uses a highly structured database representation. This paper explores the middle ground using a representation which we term entity models, in which questions about structured data may be posed and answered, but the complexities and task-specific restrictions of ontologies are avoided. An entity model is a language model or word distribution associated with an entity, such as a person, place or organization. Using these perentity language models, entities may be clustered, links may be detected or described with a short summary, entities may be collectively classified, and question answering may be performed. On a corpus of entities extracted from newswire and the Web, we group entities by profession with 90% accuracy, improve accuracy further on the task of classifying politicians as liberal or conservative using collective classification and conditional random fields, and answer questions about âwho a person isâ with mean reciprocal rank (MRR) of 0.52.
"
7782F415	Knowledge Discovery and Data Mining	qi li + ravi janardan + jieping ye	2004	GPCA: an efficient dimension reduction scheme for image compression and retrieval	 + image compression + Information retrieval + vector space + dimension reduction + data collection + singular value decomposition + principal component analysis + tensor product + Information systems	Dimension reduction + image compression + Principal Component Analysis + Singular Value Decomposition + tensor product + vector space	Recent years have witnessed a dramatic increase in the quantity of image data collected, due to advances in fields such as medical imaging, reconnaissance, surveillance, astronomy, multimedia etc. With this increase has come the need to be able to store, transmit, and query large volumes of image data efficiently. A common operation on image databases is the retrieval of all images that are similar to a query image. For this, the images in the database are often represented as vectors in a high-dimensional space and a query is answered by retrieving all image vectors that are proximal to the query image in this space, under a suitable similarity metric. To overcome problems associated with high dimensionality, such as high storage and retrieval times, a dimension reduction step is usually applied to the vectors to concentrate relevant information in a small number of dimensions. Principal Component Analysis (PCA) is a well-known dimension reduction scheme. However, since it works with vectorized representations of images, PCA does not take into account the spatial locality of pixels in images. In this paper, a new dimension reduction scheme, called Generalized Principal Component Analysis (GPCA), is presented. This scheme works directly with images in their native state, as two-dimensional matrices, by projecting the images to a vector space that is the tensor product of two lower-dimensional vector spaces. Experiments on databases of face images show that, for the same amount of storage, GPCA is superior to PCA in terms of quality of the compressed images, query precision, and computational cost.
7D1D04DD	Knowledge Discovery and Data Mining	marcus a maloof + jeremy z kolter	2004	Learning to detect malicious executables in the wild	 + concept learning + data mining + Computing / technology policy + Information systems applications + Computing methodologies + machine learning + Data mining + Computer crime + Information systems + malicious software + decision tree + security + support vector machine + Security and privacy + naive bayes + Intrusion/anomaly detection and malware mitigation + Machine learning + software security + roc curve + Social and professional topics	Data Mining + Concept Learning + Security + Malicious Software	In this paper, we describe the development of a fielded application for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the roc curve of 0.996. Results also suggest that our methodology will scale to larger collections of executables. To the best of our knowledge, ours is the only fielded application for this task developed using techniques from machine learning and data mining.
80E4D928	Knowledge Discovery and Data Mining	jiong yang + jan f prins + jun huan + wei wang	2004	SPIN: mining maximal frequent subgraphs from graph databases	 + spanning tree + Information systems applications + chemical structure + semi structured data + Data mining + Information systems	Subgraph Mining + Spanning Tree	One fundamental challenge for mining recurring subgraphs from semi-structured data sets is the overwhelming abundance of such patterns. In large graph databases, the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resources. In this paper, we propose a new algorithm that mines only maximal frequent subgraphs, i.e. subgraphs that are not a part of any other frequent subgraphs. This may exponentially decrease the size of the output set in the best case; in our experiments on practical data sets, mining maximal frequent subgraphs reduces the total number of mined patterns by two to three orders of magnitude.Our method first mines all frequent trees from a general graph database and then reconstructs all maximal subgraphs from the mined trees. Using two chemical structure benchmarks and a set of synthetic graph data sets, we demonstrate that, in addition to decreasing the output size, our algorithm can achieve a five-fold speed up over the current state-of-the-art subgraph mining algorithms.
75A8DA9F	Knowledge Discovery and Data Mining	jianyong wang + krishna gade + george karypis	2004	Efficient closed pattern mining in the presence of tough block constraints	 + algorithms + efficiency + patterns + Information systems applications + Data mining + Information systems + optimization + satisfiability + clustering + search space + toughness	+Categories and Subject Descriptors H.2.8 [Database Management+Database applicationsâ Data Mining	Various constrained frequent pattern mining problem formulations and associated algorithms have been developed that enable the user to specify various itemset-based constraints that better capture the underlying application requirements and characteristics. In this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern's items and its associated set of transactions. Block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real-valued datasets. However, developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset-based constraints studied earlier, these block constraints are tough as they are neither anti-monotone, monotone, nor convertible. To overcome this problem, we introduce a new class of pruning methods that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called CBMiner to find the closed itemsets that satisfy the block constraints.
7EFAF590	Knowledge Discovery and Data Mining	vanesa lobato + nicolas de abajo + sergio r cuesta + a diez	2004	ANN quality diagnostic models for packaging manufacturing: an industrial data mining case study	 + fmea + food packaging + operant conditioning + data mining + quality standard + Machine learning + environmental impact + Computing methodologies	+Categories and Subject Descriptors+I.5.2 [Pattern Recognition+Design Methodology General Terms+Algorithms+Management+Design. Keywords+Tinplate quality+ANNs+CRISP-DM+FMEA	World steel trade becomes more competitive every day and new high international quality standards and productivity levels can only be achieved by applying the latest computational technologies. Data driven analysis of complex processes is necessary in many industrial applications where analytical modeling is not possible. This paper presents the deployment of KDD technology in one real industrial problem: the development of new tinplate quality diagnostic models.The electrodeposition of tin on steel strips is the most critical stage of a complex process that involves a great amount of variables and operating conditions. Its optimization is not only a great commercial and economic challenge but also a compulsion due to the social impact of the tinplate product-more than 90% of the production is used for food packaging. The necessary certification with standards, like ISO 9000, requires the use of diagnostic models to minimize the costs and the environmental impact. This aim has been achieved following the multi-stage DM methodology CRISP-DM and a novel application of pro-active maintenance methods, as FMEA, for the identification of the specific process anomalies. Three DM tools have been used for the development of the models. The final results include two ANN tinplate quality diagnostic models, that provide the estimated quality of the final product just seconds after its production and only based on the process data. The results have much better performance than the classical Faraday's models widely used for the estimation.
00C3A33D	Knowledge Discovery and Data Mining	wenchi chou + kuenpin wu + tingyi sung + wenlian hsu + yifeng lin + richard tzonghan tsai	2004	A Maximum Entropy Approach to Biomedical Named Entity Recognition	machine learning + maximum entropy + rule based		"
Machine learning approaches are frequently used to solve name entity (NE) recognition (NER). In this paper we propose a hybrid method that uses maximum entropy (ME) as the underlying machine learning method incorporated with dictionary-based and rule-based methods for post-processing. Simply using ME for NER, inaccurate boundary detection of NEs and misclassification may occur. Some NEs are partially recognized by ME. In the post-processing stage, we use dictionary-based and rule-based methods to extend boundary of partially recognized NEs and to adjust classification. We use GENIA corpus 3.01 to conduct 10-fold crossverification experiments. To evaluate the performance, we consider the longest NE annotations. We evaluate our approach using standard precision (P), recall (R), and F-score, where F-score is defined as 2PR/(P+R). The precision, recall and F-score ([P, R, F]) of our ME module for overall 23 categories is [0.512, 0.538, 0.525], and after the postprocessing the performance becomes [0.729, 0.711, 0.72] for [P, R, F]. For protein, DNA and RNA classes, our method achieves [P, R, F] of [0.77, 0.80, 0.785], [0.653, 0.748, 0.7], and [0.716, 0.788, 0.752], respectively. The post-processing stage significantly improves the performance of our MEbased NER module.
"
5CB94E23	Knowledge Discovery and Data Mining	stephan bloehdorn + andreas hotho	2004	Boosting for text classification with semantic features	machine learning + difference set		Current text classification systems typically use term stems for representing document content. Semantic Web technologies allow the usage of features on a higher semantic level than single words for text classification purposes. In this paper we propose such an enhancement of the classical document representation through concepts extracted from background knowledge. Boosting, a successful machine learning technique is used for classification. Comparative experimental evaluations in three different settings support our approach through consistent improvement of the results. An analysis of the results shows that this improvement is due to two separate effects.
48C62BB2	Knowledge Discovery and Data Mining	j y chen + andrey sivachenko + lang li	2004	High-throughput Protein Interactome Data: Minable or Not?	cell signaling + false positive rate + high throughput + statistical significance + data mining		"
There is an emerging trend in post-genome biology to study the collection of thousands of protein interaction pairs (protein interactome) derived from high-throughput experiments. However, high-throughput protein interactome data, especially when derived from the Yeast 2-Hybrid (Y2H) method, have been generally believed to be irreproducible and unreliable, with an estimated high ânoise ratioâ of more than 50%. In this work, we performed a comprehensive study on approximately 70,000 protein interactions derived from a systematic yeast 2-hybrid (SY2H) method. We performed a comprehensive analysis of biases, reproducibility, statistical significance, and biologically significant patterns in this data set. Surprisingly, we found these protein interactions have a much higher quality. The data represented a comprehensive survey of the entire human proteome with no chromosomal location bias. The reproducibility rate of interactions among replicated searches was quite good, i.e., at 78.5%. The false positive rate, 5.5e-5, was two orders of magnitude better than that reported elsewhere. We further developed several statistical measures and concluded that a protein interaction only needs to appear in two different SY2H searches to become significant. We also developed techniques to show supporting evidence that âpromiscuousâ protein interactions were not random noises; instead, they could be ânetwork hubsâ of the cell signaling network. We also attributed the low noise in our data to the adoption of standard control in the experimental data generation process.
"
77C9721A	Knowledge Discovery and Data Mining	jiong yang + jinze liu + wei wang	2004	A framework for ontology-driven subspace clustering	 + domain knowledge + Information systems applications + search space + gene cluster + Data mining + ontology + Information systems	Subspace clustering + Ontology + Tendency Preserving	Traditional clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes. While domain knowledge is always the best way to justify clustering, few clustering algorithms have ever take domain knowledge into consideration. In this paper, the domain knowledge is represented by hierarchical ontology. We develop a framework by directly incorporating domain knowledge into clustering process, yielding a set of clusters with strong ontology implication. During the clustering process, ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithms. Meanwhile, the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace clusters with significant categorical enrichment onto the ontology hierarchy. Our experiments on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality. Meanwhile, many hierarchical organizations of gene clusters corresponding to a sub-hierarchies in gene ontology were also successfully captured.
7C55223D	Knowledge Discovery and Data Mining	g j janacek + anthony bagnall	2004	Clustering time series from ARMA models with clipped data	 + distance metric + Information systems applications + time series + euclidean distance + moving average + Data mining + Information systems + arma model + clustering + k means + binary sequence	+Categories and Subject Descriptors+H.2.8 [Database Management+Database Applications- Data Mining General Terms+algorithms+theory+experimentation. Keywords+time series+clustering+ARMA	Clustering time series is a problem that has applications in a wide variety of fields, and has recently attracted a large amount of research. In this paper we focus on clustering data derived from Autoregressive Moving Average (ARMA) models using k-means and k-medoids algorithms with the Euclidean distance between estimated model parameters. We justify our choice of clustering technique and distance metric by reproducing results obtained in related research. Our research aim is to assess the affects of discretising data into binary sequences of above and below the median, a process known as clipping, on the clustering of time series. It is known that the fitted AR parameters of clipped data tend asymptotically to the parameters for unclipped data. We exploit this result to demonstrate that for long series the clustering accuracy when using clipped data from the class of ARMA models is not significantly different to that achieved with unclipped data. Next we show that if the data contains outliers then using clipped data produces significantly better clusterings. We then demonstrate that using clipped series requires much less memory and operations such as distance calculations can be much faster. Finally, we demonstrate these advantages on three real world data sets.
5E8A63C0	Knowledge Discovery and Data Mining	jia li + osmar r zaiane + r s hayward	2004	Mission-Based Navigational Behaviour Modeling for Web Recommender Systems	recommender system + information need + web accessibility		Web recommender systems anticipate the information needs of on-line users and provide them with recommendations to facilitate and personalize their navigation. There are many approaches to building such systems. Among them, using web access logs to generate usersâ navigational models capable of building a web recommender system is a popular approach, given its non-intrusiveness. However, using only one information channel, namely the web access history, is often insufficient for accurate recommendation prediction. We therefore advocate the use of additional available information channels, such as the content of visited pages and the connectivity between web resources, to better model user navigational behavior. This helps in better modeling usersâ concurrent information needs. In this chapter, we investigate a novel hybrid web recommender system, which combines access history and the content of visited pages, as well as the connectivity between web resources in a web site, to model usersâ concurrent information needs and generate navigational patterns. Our experiments show that the combination of the three channels used in our system significantly improves the quality of web site recommendation and, further, that each additional channel used contributes to this improvement. In addition, we discuss cases on how to reach a compromise when not all channels are available.
75FDB045	Knowledge Discovery and Data Mining	shan wang + cuiping li + anthony k h tung + gao cong	2004	Incremental maintenance of quotient cube for median	 + data structure + Information systems applications + data cube + sliding window + Data mining + Information systems	Data Cube + Holistic Aggregation	Data cube pre-computation is an important concept for supporting OLAP(Online Analytical Processing) and has been studied extensively. It is often not feasible to compute a complete data cube due to the huge storage requirement. Recently proposed quotient cube addressed this issue through a partitioning method that groups cube cells into equivalence partitions. Such an approach is not only useful for distributive aggregate functions such as SUM but can also be applied to the holistic aggregate functions like MEDIAN.Maintaining a data cube for holistic aggregation is a hard problem since its difficulty lies in the fact that history tuple values must be kept in order to compute the new aggregate when tuples are inserted or deleted. The quotient cube makes the problem harder since we also need to maintain the equivalence classes. In this paper, we introduce two techniques called addset data structure and sliding window to deal with this problem. We develop efficient algorithms for maintaining a quotient cube with holistic aggregation functions that takes up reasonably small storage space. Performance study shows that our algorithms are effective, efficient and scalable over large databases.
11243640	Knowledge Discovery and Data Mining	yang dai + zhengdeng lei	2004	A Novel Approach for Prediction of Protein Subcellular Localization from Sequence Using Fourier Analysis and Support Vector Machines	support vector machine + fourier analysis		"

"
792AAFAB	Knowledge Discovery and Data Mining	carlos ordonez	2004	Programming the K-means clustering algorithm in SQL	sufficient statistic +  + k means clustering + data mining + indexation + Information systems applications + euclidean distance + machine learning + Data mining + sql + Information systems + synthetic data + clustering + k means	Clustering + SQL + relational DBMS + integration	Using SQL has not been considered an efficient and feasible way to implement data mining algorithms. Although this is true for many data mining, machine learning and statistical algorithms, this work shows it is feasible to get an efficient SQL implementation of the well-known K-means clustering algorithm that can work on top of a relational DBMS. The article emphasizes both correctness and performance. From a correctness point of view the article explains how to compute Euclidean distance, nearest-cluster queries and updating clustering results in SQL. From a performance point of view it is explained how to cluster large data sets defining and indexing tables to store and retrieve intermediate and final results, optimizing and avoiding joins, optimizing and simplifying clustering aggregations, and taking advantage of sufficient statistics. Experiments evaluate scalability with synthetic data sets varying size and dimensionality. The proposed K-means implementation can cluster large data sets and exhibits linear scalability.
80F850F6	Knowledge Discovery and Data Mining	alexandru niculescumizil + rich caruana	2004	Data mining in metric space: an empirical analysis of supervised learning performance criteria	 + cross entropy + data mining + supervised learning + precision + Learning paradigms + Computing methodologies + empirical study + metric space + difference set + k nearest neighbor + Supervised learning + Classification and regression trees + recall + Machine learning + Machine learning approaches + lift + neural net + multidimensional scaling + metrics + roc curve + Supervised learning by classification	Supervised Learning + Performance Evaluation + Metrics + ROC + Precision + Recall + Lift + Cross Entropy	Many criteria can be used to evaluate the performance of supervised learning. Different criteria are appropriate in different settings, and it is not always clear which criteria to use. A further complication is that learning methods that perform well on one criterion may not perform well on other criteria. For example, SVMs and boosting are designed to optimize accuracy, whereas neural nets typically optimize squared error or cross entropy. We conducted an empirical study using a variety of learning methods (SVMs, neural nets, k-nearest neighbor, bagged and boosted trees, and boosted stumps) to compare nine boolean classification performance metrics: Accuracy, Lift, F-Score, Area under the ROC Curve, Average Precision, Precision/Recall Break-Even Point, Squared Error, Cross Entropy, and Probability Calibration. Multidimensional scaling (MDS) shows that these metrics span a low dimensional manifold. The three metrics that are appropriate when predictions are interpreted as probabilities: squared error, cross entropy, and calibration, lay in one part of metric space far away from metrics that depend on the relative order of the predicted values: ROC area, average precision, break-even point, and lift. In between them fall two metrics that depend on comparing predictions to a threshold: accuracy and F-score. As expected, maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy, but perform poorly on probability metrics such as squared error. What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision. We introduce a new metric, SAR, that combines squared error, accuracy, and ROC area into one metric. MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics, suggesting that it is a good general purpose metric to use when more specific criteria are not known.
7AD1A318	Knowledge Discovery and Data Mining	assaf schuster + ran wolff + bobi gilburd	2004	k-TTP: a new privacy model for large-scale distributed environments	 + data grid + data mining + secure multiparty computation + Information systems applications + privacy + distributed environment + Data mining + association rule mining + Information systems + trusted third party + security	Privacy + security + data mining	Secure multiparty computation allows parties to jointly compute a function of their private inputs without revealing anything but the output. Theoretical results [2] provide a general construction of such protocols for any function. Protocols obtained in this way are, however, inefficient, and thus, practically speaking, useless when a large number of participants are involved.The contribution of this paper is to define a new privacy model -- k-privacy -- by means of an innovative, yet natural generalization of the accepted trusted third party model. This allows implementing cryptographically secure efficient primitives for real-world large-scale distributed systems.As an example for the usefulness of the proposed model, we employ k-privacy to introduce a technique for obtaining knowledge -- by way of an association-rule mining algorithm -- from large-scale Data Grids, while ensuring that the privacy is cryptographically secure.
5B8BF407	Knowledge Discovery and Data Mining	edwin o heierman + diane j cook + g michael youngblood	2004	Mining temporal sequences to discover interesting patterns	decision maker + knowledge discovery + data mining + real time	eol>Discovering interesting episodes + knowledge discovery + mining sequential data streams	"
When mining temporal sequences, knowledge discovery techniques can be applied that discover interesting patterns of interactions. Existing approaches use frequency, and sometimes length, as measurements for interestingness. Because these are temporal sequences, additional characteristics, such as periodicity, may also be interesting. We propose that information theoretic principles can be used to evaluate interesting characteristics of time-ordered input sequences. In this paper, we present a novel data mining technique based on the Minimum Description Length principle that discovers interesting features in a time-ordered sequence. We discuss features of our real-time mining approach, show applications of the knowledge mined by the approach, and present a technique to bootstrap a decision maker from the mined patterns.
"
7F7067A6	Knowledge Discovery and Data Mining	dharmendra s modha + joydeep ghosh + inderjit s dhillon + srujana merugu + arindam banerjee	2004	A generalized maximum entropy approach to bregman co-clustering and matrix approximation	co clustering + text clustering + recommender system + conditional expectation + data mining + probability distribution + objective function + microarray analysis + maximum entropy principle		"
Co-clustering, or simultaneous clustering of rows and columns of a two-dimensional data matrix, is rapidly becoming a powerful data analysis technique. Co-clustering has enjoyed wide success in varied application domains such as text clustering, gene-microarray analysis, natural language processing and image, speech and video analysis. In this paper, we introduce a partitional co-clustering formulation that is driven by the search for a good matrix approximation-every co-clustering is associated with an approximation of the original data matrix and the quality of co-clustering is determined by the approximation error. We allow the approximation error to be measured using a large class of loss functions called Bregman divergences that include squared Euclidean distance and KL-divergence as special cases. In addition, we permit multiple structurally different co-clustering schemes that preserve various linear statistics of the original data matrix. To accomplish the above tasks, we introduce a new minimum Bregman information (MBI) principle that simultaneously generalizes the maximum entropy and standard least squares principles, and leads to a matrix approximation that is optimal among all generalized additive models in a certain natural parameter space. Analysis based on this principle yields an elegant meta algorithm, special cases of which include most previously known alternate minimization based clustering algorithms such as kmeans and co-clustering algorithms such as information theoretic (Dhillon et al., 2003b) and minimum sum-squared residue co-clustering (Cho et al., 2004). To demonstrate the generality and flexibility of our co-clustering framework, we provide examples and empirical evidence on a vari-
"
752D2EB6	Sigkdd Explorations	gary m weiss	2004	Mining with rarity: a unifying framework	 + inductive bias + data mining + sampling	eol>Rare cases + rare classes + class imbalance + small disjuncts + sampling + inductive bias + cost-sensitive learning	"
Rare objects are often of great interest and great value. Until recently, however, rarity has not received much attention in the context of data mining. Now, as increasingly complex real-world problems are addressed, rarity, and the related problem of imbalanced data, are taking center stage. This article discusses the role that rare classes and rare cases play in data mining. The problems that can result from these two forms of rarity are described in detail, as are methods for addressing these problems. These descriptions utilize examples from existing research, so that this article provides a good survey of the literature on rarity in data mining. This article also demonstrates that rare classes and rare cases are very similar phenomena-both forms of rarity are shown to cause similar problems during data mining and benefit from the same remediation methods.
"
7D9782D4	Knowledge Discovery and Data Mining	ian davidson + goutam paul	2004	Locating secret messages in images	 + Learning paradigms + Computing methodologies + outlier detection + image restoration + Supervised learning + steganography + Classification and regression trees + sterilization + Machine learning + color image + Machine learning approaches + binary classification + Supervised learning by classification + steganalysis	eol>Steganography + Steganalysis + Outlier Detection	Steganography involves hiding messages in innocuous media such as images, while steganalysis is the field of detecting these secret messages. The ultimate goal of steganalysis is two-fold: making a binary classification of a file as stego-bearing or innocent, and secondly, locating the hidden message with an aim to extracting, sterilizing or manipulating it. Almost all steganalysis approaches (known as attacks) focus on the first of these two issues. In this paper, we explore the difficult related problem: given that we know an image file contains steganography, locate which pixels contain the message. We treat the hidden message location problem as outlier detection using probability/energy measures of images motivated by the image restoration community. Pixels contributing the most to the energy calculations of an image are deemed outliers. Typically, of the top third of one percent of most energized pixels (outliers), we find that 87% are stego-bearing in color images and 61% in grayscale images. In all image types only 1% of all pixels are stego-bearing indicating our techniques provides a substantial lift over random guessing.
768FAB8D	Knowledge Discovery and Data Mining	ian davidson + ashwin satyanarayana + giri kumar tayi + ashish grover	2004	A general approach to incorporate data quality matrices into data mining algorithms	 + six sigma + data mining + Learning paradigms + Computing methodologies + classification + data cleansing + data quality + decision tree + Supervised learning + quality assurance + Classification and regression trees + decision trees + Machine learning + Machine learning approaches + Supervised learning by classification	+Categories and Subject Descriptors I.5.2 [Pattern Recognition+Design Methodology- classifier design and evaluation General Terms+Algorithms+Experimentation Keywords+Data Quality+Ensemble Approaches+Six-Sigma+Classification+Decision Trees	Data quality is a central issue for many information-oriented organizations. Recent advances in the data quality field reflect the view that a database is the product of a manufacturing process. While routine errors, such as non-existent zip codes, can be detected and corrected using traditional data cleansing tools, many errors systemic to the manufacturing process cannot be addressed. Therefore, the product of the data manufacturing process is an imprecise recording of information about the entities of interest (i.e. customers, transactions or assets). In this way, the database is only one (flawed) version of the entities it is supposed to represent. Quality assurance systems such as Motorola's Six-Sigma and other continuous improvement methods document the data manufacturing process's shortcomings. A widespread method of documentation is quality matrices. In this paper, we explore the use of the readily available data quality matrices for the data mining classification task. We first illustrate that if we do not factor in these quality matrices, then our results for prediction are sub-optimal. We then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance.
779944B0	Sigkdd Explorations	huan liu + lance parsons + ehtesham haque	2004	Subspace clustering for high dimensional data: a review	 + high dimensional data	clustering survey + subspace clustering + projected clustering + high dimensional data	"
Subspace clustering is an extension of traditional clustering that seeks to Â¯nd clusters in diÂ®erent subspaces within a dataset. Often in high dimensional data, many dimensions are irrelevant and can mask existing clusters in noisy data. Feature selection removes irrelevant and redundant dimensions by analyzing the entire dataset. Subspace clustering algorithms localize the search for relevant dimensions allowing them to Â¯nd clusters that exist in multiple, possibly overlapping subspaces. There are two major branches of subspace clustering based on their search strategy. Topdown algorithms Â¯nd an initial clustering in the full set of dimensions and evaluate the subspaces of each cluster, iteratively improving the results. Bottom-up approaches Â¯nd dense regions in low dimensional spaces and combine them to form clusters. This paper presents a survey of the various subspace clustering algorithms along with a hierarchy organizing the algorithms by their deÂ¯ning characteristics. We then compare the two main approaches to subspace clustering using empirical scalability and accuracy tests and discuss some potential applications where subspace clustering could be particularly useful.
"
7A05DFCF	Knowledge Discovery and Data Mining	kevin s mccurley + christos faloutsos + andrew tomkins	2004	Fast discovery of connection subgraphs	social network +  + support vector machines + multi task learning + regularization + kernel methods + Information systems applications + world wide web + Data mining + real time + Information systems	+H.2.8 [Database Management+Database Applications	We define a connection subgraph as a small subgraph of a large graph that best captures the relationship between two nodes. The primary motivation for this work is to provide a paradigm for exploration and knowledge discovery in large social networks graphs. We present a formal definition of this problem, and an ideal solution based on electricity analogues. We then show how to accelerate the computations, to produce approximate, but high-quality connection subgraphs in real time on very large (disk resident) graphs.We describe our operational prototype, and we demonstrate results on a social network graph derived from the World Wide Web. Our graph contains 15 million nodes and 96 million edges, and our system still produces quality responses within seconds.
76FCE353	Information Systems	rakesh agarwal + ramakrishnan srikant + johannes gehrke + alexandre evfimievski	2004	Privacy preserving mining of association rules	data mining + privacy + association rule	Data mining + Privacy + Association rule + Privacy breach	"
We present a framework for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions. While it is feasible to recover association rules and preserve privacy using a straightforward ''uniform'' randomization, the discovered rules can unfortunately be exploited to find privacy breaches. We analyze the nature of privacy breaches and propose a class of randomization operators that are much more effective than uniform randomization in limiting the breaches. We derive formulae for an unbiased support estimator and its variance, which allow us to recover itemset supports from randomized datasets, and show how to incorporate these formulae into mining algorithms. Finally, we present experimental results that validate the algorithm by applying it on real datasets. r 2003 Published by Elsevier Ltd.
"
7DB72538	Knowledge Discovery and Data Mining	thomas l griffiths + mark steyvers + padhraic smyth + michal rosenzvi	2004	Probabilistic author-topic models for information discovery	digital library +  + algorithms + gibbs sampling + markov chain monte carlo + Machine learning + probability distribution + Computing methodologies + stochastic process + unsupervised learning	+unsupervised learning+Gibbs sampling+text	We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.
81674AF5	Knowledge Discovery and Data Mining	michail vlachos + dimitrios gunopulos + gautam das	2004	Rotation invariant distance measures for trajectories	 + trajectories + Information systems applications + time series + video tracking + lower bound + handwriting recognition + Data mining + Information systems + time warping + motion capture + standard deviation	+Categories and Subject Descriptors+H.2.8 [Database Man- agement+Database Applications+Data Mining General Terms+Algorithms Keywords+Trajectories+Time Warping+Rotation Invariance	For the discovery of similar patterns in 1D time-series, it is very typical to perform a normalization of the data (for example a transformation so that the data follow a zero mean and unit standard deviation). Such transformations can reveal latent patterns and are very commonly used in datamining applications. However, when dealing with multidimensional time-series, which appear naturally in applications such as video-tracking, motion-capture etc, similar motion patterns can also be expressed at different orientations. It is therefore imperative to provide support for additional transformations, such as rotation. In this work, we transform the positional information of moving data, into a space that is translation, scale and rotation invariant. Our distance measure in the new space is able to detect elastic matches and can be efficiently lower bounded, thus being computationally tractable. The proposed methods are easy to implement, fast to compute and can have many applications for real world problems, in areas such as handwriting recognition and posture estimation in motion-capture data. Finally, we empirically demonstrate the accuracy and the efficiency of the technique, using real and synthetic handwriting data.
0398F80E	Knowledge Discovery and Data Mining	jeanfrancois boulicaut + ruggero g pensa + jeremy besson + claire leschi	2004	Assessment of discretization techniques for relevant pattern discovery from gene expression data	hierarchical clustering + association rule mining		"
In the domain of gene expression data analysis, various researchers have recently emphasized the promising application of pattern discovery techniques like association rule mining or formal concept extraction from boolean matrices that encode gene properties. To take the most from these approaches, a needed step concerns gene property encoding (e.g., over-expression) and its need for the discretization of raw gene expression data. The impact of this preprocessing step on both the quantity and the relevancy of the extracted patterns is crucial. In this paper, we study the impact of discretization parameters by a sound comparison between the dendrograms, i.e., trees that are generated by a hierarchical clustering algorithm, computed from raw expression data and from the various derived boolean matrices. Thanks to a new similarity measure and practical validation over several gene expression data sets, we propose a method that supports the choice of a discretization technique and its parameters for each speciÂ¯c data set.
"
7536769F	Sigkdd Explorations	m dolores del castillo + j i serrano	2004	A multistrategy approach for digital text categorization from imbalanced documents	feature selection + genetic algorithms + genetic algorithm	Feature selection + multistrategy learning + genetic algorithms	"
The goal of the research described here is to develop a multistrategy classifier system that can be used for document categorization. The system automatically discovers classification patterns by applying several empirical learning methods to different representations for preclassified documents belonging to an imbalanced sample. The learners work in a parallel manner, where each learner carries out its own feature selection based on evolutionary techniques and then obtains a classification model. In classifying documents, the system combines the predictions of the learners by applying evolutionary techniques as well. The system relies on a modular, flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the thematic domain.
"
811B8620	Knowledge Discovery and Data Mining	stefano lonardi + eamonn keogh + chotirat ann ratanamahatana	2004	Towards parameter-free data mining	 + data mining + compression algorithm + Information systems applications + time series + clustering + anomaly detection + lines of code + Data mining + computability theory + Information systems	eol>Kolmogorov Complexity + Parameter-Free Data Mining + Anomaly Detection + Clustering	Most data mining algorithms require the setting of many input parameters. Two main dangers of working with parameter-laden algorithms are the following. First, incorrect settings may cause an algorithm to fail in finding the true patterns. Second, a perhaps more insidious problem is that the algorithm may report spurious patterns that do not really exist, or greatly overestimate the significance of the reported patterns. This is especially likely when the user fails to understand the role of parameters in the data mining process.Data mining algorithms should have as few parameters as possible, ideally none. A parameter-free algorithm would limit our ability to impose our prejudices, expectations, and presumptions on the problem at hand, and would let the data itself speak to us. In this work, we show that recent results in bioinformatics and computational theory hold great promise for a parameter-free data-mining paradigm. The results are motivated by observations in Kolmogorov complexity theory. However, as a practical matter, they can be implemented using any off-the-shelf compression algorithm with the addition of just a dozen or so lines of code. We will show that this approach is competitive or superior to the state-of-the-art approaches in anomaly/interestingness detection, classification, and clustering with empirical tests on time series/DNA/text/video datasets.
7DE65620	Knowledge Discovery and Data Mining	atulya velivelli + bei yu + chengxiang zhai	2004	A cross-collection mixture model for comparative text mining	 + em algorithm + Information retrieval + text mining + expectation maximization + clustering + mixture model + mixture models + business intelligence + Information systems	Comparative text mining + mixture models + clustering	In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme. This general problem subsumes many interesting applications, including business intelligence and opinion summarization. We propose a generative probabilistic mixture model for comparative text mining. The model simultaneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model.
7CC13C48	Knowledge Discovery and Data Mining	jiong yang + jiawei han + yifan li	2004	Clustering moving objects	 + k means clustering + algorithms + data clustering + Information systems applications + clustering + Data mining + real time + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Management+Database Applications- Data Mining General Terms+Algorithms. Keywords+Moving object+micro-cluster+clustering+algo- rithms	Due to the advances in positioning technologies, the real time information of moving objects becomes increasingly available, which has posed new challenges to the database research. As a long-standing technique to identify overall distribution patterns in data, clustering has achieved brilliant successes in analyzing static datasets. In this paper, we study the problem of clustering moving objects, which could catch interesting pattern changes during the motion process and provide better insight into the essence of the mobile data points. In order to catch the spatial-temporal regularities of moving objects and handle large amounts of data, micro-clustering [20] is employed. Efficient techniques are proposed to keep the moving micro-clusters geographically small. Important events such as the collisions among moving micro-clusters are also identified. In this way, high quality moving micro-clusters are dynamically maintained, which leads to fast and competitive clustering result at any given time instance. We validate our approaches with a through experimental evaluation, where orders of magnitude improvement on running time is observed over normal K-Means clustering method [14].
7E1A193D	Knowledge Discovery and Data Mining	nikos mamoulis + david w cheung + huiping cao + george kollios + marios hadjieleftheriou + yufei tao	2004	Mining, indexing, and querying historical spatiotemporal data	 + data management + indexing + trajectories + indexation + Information systems applications + Data mining + Information systems	+General Terms+Algorithms Categories & Subject Descriptors+H.2.8 [Database Man- agement+Database Applications- Data Mining Keywords+Spatiotemporal data+Trajectories+Pattern min- ing+Indexing	In many applications that track and analyze spatiotemporal data, movements obey periodic patterns; the objects follow the same routes (approximately) over regular time intervals. For example, people wake up at the same time and follow more or less the same route to their work everyday. The discovery of hidden periodic patterns in spatiotemporal data, apart from unveiling important information to the data analyst, can facilitate data management substantially. Based on this observation, we propose a framework that analyzes, manages, and queries object movements that follow such patterns. We define the spatiotemporal periodic pattern mining problem and propose an effective and fast mining algorithm for retrieving maximal periodic patterns. We also devise a novel, specialized index structure that can benefit from the discovered patterns to support more efficient execution of spatiotemporal queries. We evaluate our methods experimentally using datasets with object trajectories that exhibit periodicity.
754CE238	Knowledge Discovery and Data Mining	tong zhang + jinbo bi + kristin p bennett	2004	Column-generation boosting methods for mixture of kernels	 + boosting + Learning paradigms + kernel method + Computing methodologies + linear program + column generation + Supervised learning + cross validation + Classification and regression trees + Machine learning + kernel methods + Machine learning approaches + quadratic program + Supervised learning by classification	Kernel methods + Boosting + Column generation	We devise a boosting approach to classification and regression based on column generation using a mixture of kernels. Traditional kernel methods construct models based on a single positive semi-definite kernel with the type of kernel predefined and kernel parameters chosen according to cross-validation performance. Our approach creates models that are mixtures of a library of kernel models, and our algorithm automatically determines kernels to be used in the final model. The 1-norm and 2-norm regularization methods are employed to restrict the ensemble of kernel models. The proposed method produces sparser solutions, and thus significantly reduces the testing time. By extending the column generation (CG) optimization which existed for linear programs with 1-norm regularization to quadratic programs with 2-norm regularization, we are able to solve many learning formulations by leveraging various algorithms for constructing single kernel models. By giving different priorities to columns to be generated, we are able to scale CG boosting to large datasets. Experimental results on benchmark data are included to demonstrate its effectiveness.
7B763F3D	Knowledge Discovery and Data Mining	wijnand van stam + kamal ali	2004	TiVo: making show recommendations using a distributed collaborative filtering architecture	 + collaborative filtering + recommender system + client server architecture + Machine learning + Computing methodologies	eol>Collaborative-Filtering + Clustering Clickstreams	We describe the TiVo television show collaborative recommendation system which has been fielded in over one million TiVo clients for four years. Over this install base, TiVo currently has approximately 100 million ratings by users over approximately 30,000 distinct TV shows and movies. TiVo uses an item-item (show to show) form of collaborative filtering which obviates the need to keep any persistent memory of each user's viewing preferences at the TiVo server. Taking advantage of TiVo's client-server architecture has produced a novel collaborative filtering system in which the server does a minimum of work and most work is delegated to the numerous clients. Nevertheless, the server-side processing is also highly scalable and parallelizable. Although we have not performed formal empirical evaluations of its accuracy, internal studies have shown its recommendations to be useful even for multiple user households. TiVo's architecture also allows for throttling of the server so if more server-side resources become available, more correlations can be computed on the server allowing TiVo to make recommendations for niche audiences.
7A8296AB	Knowledge Discovery and Data Mining	andrew j connolly + andrew w moore + brigham anderson + robert c nichol	2004	Fast nonlinear regression via eigenimages applied to galactic morphology	 + nonlinear regression + feature space + morphology + linear regression + Computing methodologies + astronomy + missing data + local minima + nearest neighbor search + nearest neighbor + parametric model + regression + Machine learning	Astronomy + Morphology + Nearest Neighbor + Regression + Pincipal Component Analysis	Astronomy increasingly faces the issue of massive, unwieldly data sets. The Sloan Digital Sky Survey (SDSS) [11] has so far generated tens of millions of images of distant galaxies, of which only a tiny fraction have been morphologically classified. Morphological classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image. This is a nonlinear regression problem, whose challenges are threefold, 1) blurring of the image caused by atmosphere and mirror imperfections, 2) large numbers of local minima, and 3) massive data sets.Our strategy is to use the eigenimages of the parametric model to form a new feature space, and then to map both target image and the model parameters into this feature space. In this low-dimensional space we search for the best image-to-parameter match. To search the space, we sample it by creating a database of many random parameter vectors (prototypes) and mapping them into the feature space. The search problem then becomes one of finding the best prototype match, so the fitting process a nearest-neighbor search.In addition to the savings realized by decomposing the original space into an eigenspace, we can use the fact that the model is a linear sum of functions to reduce the prototypes further: the only prototypes stored are the components of the model function. A modified form of nearest neighbor is used to search among them.Additional complications arise in the form of missing data and heteroscedasticity, both of which are addressed with weighted linear regression. Compared to existing techniques, speed-ups ach-ieved are between 2 and 3 orders of magnitude. This should enable the analysis of the entire SDSS dataset.
7F9F23C4	Knowledge Discovery and Data Mining	qi li + vipin kumar + ravi janardan + hui xiong + jieping ye + haesun park	2004	IDR/QR: an incremental dimension reduction algorithm via QR decomposition	pattern classification + data preprocessing + data mining + QR decomposition + QR-updating technique + dimension reduction + high dimensional data + classification error rate + very large databases + incremental learning + IDR + scatter matrices + LDA-based incremental dimension reduction algorithm + linear discriminant analysis + learning (artificial intelligence) + statistical analysis + singular value decomposition + qr decomposition + eigenvalue problem	Dimension reduction + Linear Discriminant Analysis + incremental learning + QR Decomposition	Dimension reduction is a critical data preprocessing step for many database and data mining applications, such as efficient storage and retrieval of high-dimensional data. In the literature, a well-known dimension reduction algorithm is linear discriminant analysis (LDA). The common aspect of previously proposed LDA-based algorithms is the use of singular value decomposition (SVD). Due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in LDA, there has been little work on designing incremental LDA algorithms that can efficiently incorporate new data items as they become available. In this paper, we propose an LDA-based incremental dimension reduction algorithm, called IDR/QR, which applies QR decomposition rather than SVD. Unlike other LDA-based algorithms, this algorithm does not require the whole data matrix in main memory. This is desirable for large data sets. More importantly, with the insertion of new data items, the IDR/QR algorithm can constrain the computational cost by applying efficient QR-updating techniques. Finally, we evaluate the effectiveness of the IDR/QR algorithm in terms of classification error rate on the reduced dimensional space. Our experiments on several real-world data sets reveal that the classification error rate achieved by the IDR/QR algorithm is very close to the best possible one achieved by other LDA-based algorithms. However, the IDR/QR algorithm has much less computational cost, especially when new data items are inserted dynamically.
7B13E4E5	Knowledge Discovery and Data Mining	huan liu + lei yu	2004	Redundancy based feature selection for microarray data	 + gene selection + Feature selection + Machine learning algorithms + microarray data + Information systems applications + Computing methodologies + empirical study + Data mining + Information systems + microarray data analysis + Machine learning + feature selection + gene expression	feature redundancy + gene selection + microarray data	In gene expression microarray data analysis, selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes. The problem becomes particularly challenging due to the large number of features (genes) and small sample size. Traditional gene selection methods often select the top-ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes. Latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy. Hence, we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes. The efficiency and effectiveness of our method in comparison with representative methods has been demonstrated through an empirical study using public microarray data sets.
7EDA860E	Knowledge Discovery and Data Mining	ata kaban + peter tino + yi sun	2004	A generative probabilistic approach to visualizing sets of symbolic sequences	topographic map + data type + probabilistic model + hidden markov model + em algorithm + complex structure + topographic mapping	Hidden Markov model + latent space models + topographic mapping + EM algorithm	"
There is a notable interest in extending probabilistic generative modeling principles to accommodate for more complex structured data types. In this paper we develop a generative probabilistic model for visualizing sets of discrete symbolic sequences. The model, a constrained mixture of discrete hidden Markov models, is a generalization of density-based visualization methods previously developed for static data sets. We illustrate our approach on sequences representing web-log data and chorals by J.S. Bach.
"
014921D8	Knowledge Discovery and Data Mining	keith marsolo + srinivasan parthasarathy + hongyuan li + d a polshakov	2004	A New Approach to Protein Structure Mining and Alignment	sequence alignment + structure alignment + data mining + amino acid + protein domains + protein structure + amino acid sequence + protein structure prediction		"
One of the largest areas of bioinformatic and data mining research has been in the protein domain. These efforts have included protein structure prediction, folding pathway prediction, sequence alignment, ab initio simulation, structure alignment, substructure detection and many others. Substructure detection is generally defined as the mining of a molecule's 3D structure in order to find interesting/frequent domains. Sequence alignment involves determining the similarity of two (or more) protein molecules based on the how well their amino acid sequences âmatch.â There are potential pitfalls when trying solve both of these problems, however. In the case of substructure mining, focusing solely on structural information can lead to the discovery of biologically irrelevant substructures. With sequence alignment, the alignment results can vary greatly, depending on the substitution matrix used. In this paper we describe a method that combines the benefits of both substructure mining and sequence alignment in an attempt to determine the similarity between protein molecules. In the absence of biological information, our work will quickly and efficiently mine a protein molecule in order to determine frequent local structures. With the addition of biological sequence information, however, our algorithm provides a way to align proteins with similar local structures and sequence, yielding a global alignment between molecules. We present a novel structure mining/alignment algorithm as well as some additional work into a new clustering metric for amino acids based on several different physico-chemical properties. This metric is used with our alignment algorithm in order to provide a mechanism for globally aligning protein molecules.
"
7998154E	Sigkdd Explorations	herna l viktor + hongyu guo	2004	Learning from imbalanced data sets with boosting and data generation: the DataBoost-IM approach	 + decision tree + boosting + data mining + synthetic data + machine learning		"
Learning from imbalanced data sets, where the number of examples of one (majority) class is much higher than the others, presents an important challenge to the machine learning community. Traditional machine learning algorithms may be biased towards the majority class, thus producing poor predictive accuracy over the minority class. In this paper, we describe a new approach that combines boosting, an ensemble-based learning algorithm, with data generation to improve the predictive power of classifiers against imbalanced data sets consisting of two classes. In the DataBoost-IM method, hard examples from both the majority and minority classes are identified during execution of the boosting algorithm. Subsequently, the hard examples are used to separately generate synthetic examples for the majority and minority classes. The synthetic data are then added to the original training set, and the class distribution and the total weights of the different classes in the new training set are rebalanced. The DataBoost-IM method was evaluated, in terms of the F-measures, G-mean and overall accuracy, against seventeen highly and moderately imbalanced data sets using decision trees as base classifiers. Our results are promising and show that the DataBoost-IM method compares well in comparison with a base classifier, a standard benchmarking boosting algorithm and three advanced boosting-based algorithms for imbalanced data set. Results indicate that our approach does not sacrifice one class in favor of the other, but produces high predictions against both minority and majority classes.
"
7E9FEE4E	Knowledge Discovery and Data Mining	venkatesh ganti + eugene agichtein	2004	Mining reference tables for automatic text segmentation	 + data warehouse + information extraction + data cleaning + Machine learning + Information systems applications + Computing methodologies + relational database + machine learning + Data mining + text segmentation + Information systems	Algorithms + design + performance + experimentation	Automatically segmenting unstructured text strings into structured records is necessary for importing the information contained in legacy sources and text collections into a data warehouse for subsequent querying, analysis, mining and integration. In this paper, we mine tables present in data warehouses and relational databases to develop an automatic segmentation system. Thus, we overcome limitations of existing supervised text segmentation approaches, which require comprehensive manually labeled training data. Our segmentation system is robust, accurate, and efficient, and requires no additional manual effort. Thorough evaluation on real datasets demonstrates the robustness and accuracy of our system, with segmentation accuracy exceeding state of the art supervised approaches.
7A1924D9	Knowledge Discovery and Data Mining	sunita sarawagi + william w cohen	2004	Exploiting dictionaries in named entity extraction: combining semi-Markov extraction processes and data integration methods	 + Document representation + information extraction + Dictionaries + sequential learning + Machine learning + Computing methodologies + Information retrieval + data integrity + data integration + Information systems	Learning + information extraction + named entity recognition + data integration + sequential learning	We consider the problem of improving named entity recognition (NER) systems by using external dictionaries---more specifically, the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary. This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name; however, the most useful similarity measures score entire candidate names. To correct this mismatch we formalize a semi-Markov extraction process, which is based on sequentially classifying segments of several adjacent words, rather than single words. In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions, this formalism also allows the direct use of other useful entity-level features, and provides a more natural formulation of the NER problem than sequential word classification. Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER.
751473EF	Knowledge Discovery and Data Mining	hisashi kashima + tsuyoshi ide	2004	Eigenspace-based anomaly detection in computer systems	online algorithm + algorithms + eigenvectors + time series + network management system + feature vector + anomaly detection + adjacency matrix + singular value decomposition + probability distribution		"
We report on an automated runtime anomaly detection method at the application layer of multi-node computer systems. Although several network management systems are available in the market, none of them have sufficient capabilities to detect faults in multi-tier Web-based systems with redundancy. We model a Web-based system as a weighted graph, where each node represents a âserviceâ and each edge represents a dependency between services. Since the edge weights vary greatly over time, the problem we address is that of anomaly detection from a time sequence of graphs. In our method, we first extract a feature vector from the adjacency matrix that represents the activities of all of the services. The heart of our method is to use the principal eigenvector of the eigenclusters of the graph. Then we derive a probability distribution for an anomaly measure defined for a time-series of directional data derived from the graph sequence. Given a critical probability, the threshold value is adaptively updated using a novel online algorithm. We demonstrate that a fault in a Web application can be automatically detected and the faulty services are identified without using detailed knowledge of the behavior of the system.
"
79BC31F3	Knowledge Discovery and Data Mining	akihiro nakashima + teruaki homma + hiromitsu fujikawa + katsuyuki yamazaki + hiroshi motoda + takashi washio + fuminori adachi + kenichi yoshida	2004	Density-based spam detector	 + Document representation + Machine learning + Information systems applications + Computing methodologies + Information retrieval + spam + Data mining + unsupervised learning + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Data Mining+H.3.1 [In- formation Storage and Retrieval+Content Analysis and Indexing+I.2.6 [Artificial Intelligence+Learning General Terms Performance+Experimentation+Security Keywords spam+unsupervised learning+document space density+direct- mapped	The volume of mass unsolicited electronic mail, often known as spam, has recently increased enormously and has become a serious threat to not only the Internet but also to society. This paper proposes a new spam detection method which uses document space density information. Although it requires extensive e-mail traffic to acquire the necessary information, an unsupervised learning engine with a short white list can achieve a 98% recall rate and 100% precision. A direct-mapped cache method contributes handling of over 13,000 e-mails per second. Experimental results, which were conducted using over 50 million actual e-mails of traffic, are also reported in this paper.
772BFE2C	Knowledge Discovery and Data Mining	ion muslea	2004	Machine learning for online query relaxation	 + nearest neighbor + disjunctive normal form + satisfiability + Machine learning + Computing methodologies + machine learning + decision rule	online query relaxation + failing query + rule learning + nearest neighbor + Web-based information sources	In this paper we provide a fast, data-driven solution to the failing query problem: given a query that returns an empty answer, how can one relax the query's constraints so that it returns a non-empty set of tuples? We introduce a novel algorithm, loqr, which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes. loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query.In a first step, loqr uses a small, randomly-chosen subset of the target database to learn a set of decision rules that predict whether an attribute's value satisfies the constraints in the failing query; this query-driven operation is performed online for each failing query. In the second step, loqr uses nearest-neighbor techniques to find the learned rule that is the most similar to the failing query; then it uses the attributes' values from this rule to relax the failing query's constraints. Our experiments on six application domains show that loqr is both robust and fast: it successfully relaxes more than 95% of the failing queries, and it takes under a second for processing queries that consist of up to 20 attributes (larger queries of up to 93 attributes are processed in several seconds).
7D3F00E8	Knowledge Discovery and Data Mining	rong jin + pangning tan	2004	Ordering patterns by combining opinions from multiple sources	ensemble learning +  + data mining + Information systems applications + meta search engine + Data mining + singular value decomposition + unsupervised learning + independent and identically distributed + Information systems	eol>Pattern Ordering + Multi-criteria optimization + Ensemble Learning	Pattern ordering is an important task in data mining because the number of patterns extracted by standard data mining algorithms often exceeds our capacity to manually analyze them. In this paper, we present an effective approach to address the pattern ordering problem by combining the rank information gathered from disparate sources. Although rank aggregation techniques have been developed for applications such as meta-search engines, they are not directly applicable to pattern ordering for two reasons. First, the techniques are mostly supervised, i.e., they require a sufficient amount of labeled data. Second, the objects to be ranked are assumed to be independent and identically distributed (i.i.d), an assumption that seldom holds in pattern ordering. The method proposed in this paper is an adaptation of the original Hedge algorithm, modified to work in an unsupervised learning setting. Techniques for addressing the i.i.d. violation in pattern ordering are also presented. Experimental results demonstrate that our unsupervised Hedge algorithm outperforms many alternative techniques such as those based on weighted average ranking and singular value decomposition.
7DB65202	Knowledge Discovery and Data Mining	deept kumar + naren ramakrishnan + malcolm potts + richard f helm + bud mishra	2004	Turning CARTwheels: an alternating algorithm for mining redescriptions	 + data mining + Machine learning + Information systems applications + Computing methodologies + classification tree + Data mining + algorithm design + Information systems	Classi cation trees + redescriptions + data mining in biological domains	We present an unusual algorithm involving classification trees---CARTwheels---where two trees are grown in opposite directions so that they are joined at their leaves. This approach finds application in a new data mining task we formulate, called redescription mining. A redescription is a shift-of-vocabulary, or a different way of communicating information about a given subset of data; the goal of redescription mining is to find subsets of data that afford multiple descriptions. We highlight the importance of this problem in domains such as bioinformatics, which exhibit an underlying richness and diversity of data descriptors (e.g., genes can be studied in a variety of ways). CARTwheels exploits the duality between class partitions and path partitions in an induced classification tree to model and mine redescriptions. It helps integrate multiple forms of characterizing datasets, situates the knowledge gained from one dataset in the context of others, and harnesses high-level abstractions for uncovering cryptic and subtle features of data. Algorithm design decisions, implementation details, and experimental results are presented.
7D5088AB	Knowledge Discovery and Data Mining	shuicheng yan + jun yan + hua li + qiansheng cheng + qiang yang + weiguo fan + wensi xi + benyu zhang + weiying ma + zheng chen	2004	IMMC: incremental maximum margin criterion	 + Theory of computation + satisfiability + Mathematical analysis + Machine learning + face recognition + Computing methodologies + Mathematics of computing + Mathematical optimization + Design and analysis of algorithms + principal component analysis	eol>Maximum Margin Criterion (MMC) + Principal Component Analysis (PCA) + Linear Discriminant Analysis (LDA)	Subspace learning approaches have attracted much attention in academia recently. However, the classical batch algorithms no longer satisfy the applications on streaming data or large-scale data. To meet this desirability, Incremental Principal Component Analysis (IPCA) algorithm has been well established, but it is an unsupervised subspace learning approach and is not optimal for general classification tasks, such as face recognition and Web document categorization. In this paper, we propose an incremental supervised subspace learning algorithm, called Incremental Maximum Margin Criterion (IMMC), to infer an adaptive subspace by optimizing the Maximum Margin Criterion. We also present the proof for convergence of the proposed algorithm. Experimental results on both synthetic dataset and real world datasets show that IMMC converges to the similar subspace as that of batch approach.
7DA5858E	Knowledge Discovery and Data Mining	balaji padmanabhan + zhiqiang zheng + haoqiang zheng	2004	A DEA approach for model combination	 + data envelopment analysis + Machine learning + Information systems applications + Computing methodologies + convex hull + Data mining + data envelope analysis + roc analysis + Information systems	eol>Model Combination + Data Envelopment Analysis + ROC	This paper proposes a novel Data Envelopment Analysis (DEA) based approach for model combination. We first prove that for the 2-class classification problems DEA models identify the same convex hull as the popular ROC analysis used for model combination. For general k-class classifiers, we then develop a DEA-based method to combine multiple classifiers. Experiments show that the method outperforms other benchmark methods and suggest that DEA can be a promising tool for model combination.
78BFB4A0	Knowledge Discovery and Data Mining	m c wang + david s vogel	2004	1-dimensional splines as building blocks for improving accuracy of risk outcomes models	adaptive + cubic spline + second order + 1 dimensional + spline + smoothing spline + data mining + linear model + prediction + mean square error + linear regression model + regression model + risk		"
Transformation of both the response variable and the predictors is commonly used in fitting regression models. However, these transformation methods do not always provide the maximum linear correlation between the response variable and the predictors, especially when there are non-linear relationships between predictors and the response such as the medical data set used in this study. A spline based transformation method is proposed that is second order smooth, continuous, and minimizes the mean squared error between the response and each predictor. Since the computation time for generating this spline is O(n), the processing time is reasonable with massive data sets. In contrast to cubic smoothing splines, the resulting transformation equations also display a high level of efficiency for scoring. Data used for predicting health outcomes contains an abundance of non-linear relationships between predictors and the outcomes requiring an algorithm for modeling them accurately. Thus, a transformation that fits an adaptive cubic spline to each of a set of variables is proposed. These curves are used as a set of transformation functions on the predictors. A case study of how the transformed variables can be fed into a simple linear regression model to predict risk outcomes is presented. The results show significant improvement over the performance of the original variables in both linear and nonlinear models.
"
7AF7A23F	Knowledge Discovery and Data Mining	christos faloutsos + hyungjeong yang + pinar duygulu + jiayu pan	2004	Automatic multimedia cross-modal correlation discovery	 + design + Information systems applications + Data mining + Information systems	ySupported by the Post-doctoral Fellowship Program of Ko- rea Science and Engineering Foundation (KOSEF)	"Given an image (or video clip, or audio song), how do we automatically assign keywords to it? The general problem is to find correlations across the media in a collection of multimedia objects like video clips, with colors, and/or motion, and/or audio, and/or text scripts. We propose a novel, graph-based approach, ""MMG"", to discover such cross-modal correlations.Our ""MMG"" method requires no tuning, no clustering, no user-determined constants; it can be applied to any multimedia collection, as long as we have a similarity function for each medium; and it scales linearly with the database size. We report auto-captioning experiments on the ""standard"" Corel image database of 680 MB, where it outperforms domain specific, fine-tuned methods by up to 10 percentage points in captioning accuracy (50% relative improvement)."
80045A43	Knowledge Discovery and Data Mining	jessica lin + stefano lonardi + eamonn keogh + donna m nystrom + jeffrey p lankford	2004	Visually mining and monitoring massive time series	 + Retrieval models and ranking + Visualization + visualization + Retrieval tasks and goals + data mining + Information systems applications + Human-centered computing + time series + Information retrieval + anomaly detection + Information extraction + Information retrieval query processing + Data mining + Visualization application domains + Information systems + Document filtering + design + false positive + Scientific visualization	eol>Time Series + Visualization + Motif Discovery + Anomaly Detection + Pattern Discovery	Moments before the launch of every space vehicle, engineering discipline specialists must make a critical go/no-go decision. The cost of a false positive, allowing a launch in spite of a fault, or a false negative, stopping a potentially successful launch, can be measured in the tens of millions of dollars, not including the cost in morale and other more intangible detriments. The Aerospace Corporation is responsible for providing engineering assessments critical to the go/no-go decision for every Department of Defense space vehicle. These assessments are made by constantly monitoring streaming telemetry data in the hours before launch. We will introduce VizTree, a novel time-series visualization tool to aid the Aerospace analysts who must make these engineering assessments. VizTree was developed at the University of California, Riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry. The use of a single tool for both aspects of the task allows a natural and intuitive transfer of mined knowledge to the monitoring task. Our visualization approach works by transforming the time series into a symbolic representation, and encoding the data in a modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties. We demonstrate the utility of our system by comparing it with state-of-the-art batch algorithms on several real and synthetic datasets.
7BE092D0	Knowledge Discovery and Data Mining	jeff schneider + andrew w moore + kaustav das	2004	Belief state approaches to signaling alarms in surveillance systems	 + Control methods + Computing methodologies + time series + probabilistic model + Search methodologies + Theory of computation + Machine learning + Algorithm design techniques + Dynamic programming + Design and analysis of algorithms + Artificial intelligence + public health	Probabilistic Model + Scan Statistic + Signaling Alarms + Surveillance Systems	Surveillance systems have long been used to monitor industrial processes and are becoming increasingly popular in public health and anti-terrorism applications. Most early detection systems produce a time series of p-values or some other statistic as their output. Typically, the decision to signal an alarm is based on a threshold or other simple algorithm such as CUSUM that accumulates detection information temporally.We formulate a POMDP model of underlying events and observations from a detector. We solve the model and show how it is used for single-output detectors. When dealing with spatio-temporal data, scan statistics are a popular method of building detectors. We describe the use of scan statistics in surveillance and how our POMDP model can be used to perform alarm signaling with them. We compare the results obtained by our method with simple thresholding and CUSUM on synthetic and semi-synthetic health data.
7F09E086	Knowledge Discovery and Data Mining	alexandrin popescul + lyle h ungar	2004	Cluster-based concept invention for statistical relational learning	 + algorithms + feature space + Machine learning + statistical relational learning + Computing methodologies + clustering + relational learning + artificial intelligence + feature selection	Relational Learning + Clustering + Feature Generation	"We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving ""topics"", and authors can be clustered based on documents they co-author giving ""communities"". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data."
789B67B6	Sigkdd Explorations	rohini k srihari + zhaohui zheng + xiaoyun wu	2004	Feature selection for text categorization on imbalanced data	information gain +  + naive bayes + odd ratio + logistic regression + feature selection		"
A number of feature selection metrics have been explored in text categorization, among which information gain (IG), chi-square (CHI), correlation coe cient (CC) and odds ratios (OR) are considered most e ective. CC and OR are one-sided metrics while IG and CHI are two-sided. Feature selection using one-sided metrics selects the features most indicative of membership only, while feature selection using two-sided metrics implicitly combines the features most indicative of membership (e.g. positive features) and nonmembership (e.g. negative features) by ignoring the signs of features. The former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data. In this work, we investigate the usefulness of explicit control of that combination within a proposed feature selection framework. Using multinomial nave Bayes and regularized logistic regression as classi ers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.
"
7B446F85	Knowledge Discovery and Data Mining	david poole	2004	Estimating the size of the telephone universe: a Bayesian Mark-recapture approach	 + prior distribution + Probabilistic algorithms + Statistical graphics + data mining + Probabilistic reasoning algorithms + Statistical paradigms + posterior distribution + bayesian approach + Sequential Monte Carlo methods + Probability and statistics + Mathematics of computing + Markov-chain Monte Carlo methods + bayesian inference	+mation	Mark-recapture models have for many years been used to estimate the unknown sizes of animal and bird populations. In this article we adapt a finite mixture mark-recapture model in order to estimate the number of active telephone lines in the USA. The idea is to use the calling patterns of lines that are observed on the long distance network to estimate the number of lines that do not appear on the network. We present a Bayesian approach and use Markov chain Monte Carlo methods to obtain inference from the posterior distributions of the model parameters. At the state level, our results are in fairly good agreement with recent published reports on line counts. For lines that are easily classified as business or residence, the estimates have low variance. When the classification is unknown, the variability increases considerably. Results are insensitive to changes in the prior distributions. We discuss the significant computational and data mining challenges caused by the scale of the data, approximately 350 million call-detail records per day observed over a number of weeks.
77E32C20	Sigkdd Explorations	nathalie japkowicz + taeho jo	2004	Class imbalances versus small disjuncts	 + resampling		"
W e c o n c l u d e t h e p a p e r b y s u m m a r i z i n g a n d t e s t i n g a n a p p r o a c h t h a t w e h a v e p r e v i o u s l y d e s i g n e d ( a n d d e s c r i b e d i n g r e a t e r l e n g t h , e l s e w h e r e [ 7 ] , [ 8 ] ) t h a t c o u n t e r s t h e e f f e c t o f s m a l l d i s j u n c t s . T h o u g h , t h e r e s u l t s p r e s e n t e d i n t h i s p a p e r m a y n o t b e f u l l y s u r p r i s i n g , i t i s i m p o r t a n t t o n o t e t h a t t h e p a p e r Â s m a i n c o n t r i b u t i o n r a t h e r l i e s i n t h e s h i f t o f f o c u s i t p r o p o s e s , w h i c h l e a d s t o n e w i n s i g h t s a n d s o l u t i o n s .
"
75A8CC4D	Knowledge Discovery and Data Mining	dan a simovici + szymon jaroszewicz	2004	Interestingness of frequent itemsets using Bayesian networks as background knowledge	 + bayesian network + association rule + background + Information systems applications + Data mining + knowledge + Information systems	association rule + frequent itemset + background knowledge + interestingness + Bayesian network	The paper presents a method for pruning frequent itemsets based on background knowledge represented by a Bayesian network. The interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the Bayesian network. Efficient algorithms are presented for finding interestingness of a collection of frequent itemsets, and for finding all attribute sets with a given minimum interestingness. Practical usefulness of the algorithms and their efficiency have been verified experimentally.
7951AA85	Knowledge Discovery and Data Mining	bing liu + kaidi zhao + andreas schaller + thomas m tirpak	2004	V-Miner: using enhanced parallel coordinates to mine product design and test data	 + edit distance + product design + data mining + parallel coordinates + Information systems applications + decision maker + Data mining + Information systems	+Categories and Subject Descriptors H.2.8 [Information Systems+Database Management-- Data Mining+I.3.m [Computer Graphics+Miscellaneous-- Visualization General Terms+Design+Human Factors. Keywords Change patterns+parallel coordinate visualization+rules	Analyzing data to find trends, correlations, and stable patterns is an important task in many industrial applications. This paper proposes a new technique based on parallel coordinate visualization. Previous work on parallel coordinate methods has shown that they are effective only when variables that are correlated and/or show similar patterns are displayed adjacently. Although current parallel coordinate tools allow the user to manually rearrange the order of variables, this process is very time-consuming when the number of variables is large. Automated assistance is required. This paper introduces an edit-distance based technique to rearrange variables so that interesting change patterns can be easily detected visually. The Visual Miner (V-Miner) software includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined or displayed by the system. In addition, the system can filter data according to rules sets imported from other data mining tools. This feature was found very helpful in practice, because it enables decision makers to visually identify interesting rules and data segments for further analysis or data mining. This paper begins with an introduction to the proposed techniques and the V-Miner system. Next, a case study illustrates how V-Miner has been used at Motorola to guide product design and test decisions.
7CDCB68D	Knowledge Discovery and Data Mining	giles hooker	2004	Discovering additive structure in black box functions	 + visualization + Feature selection + Machine learning algorithms + additive model + Computing methodologies + Modeling and simulation + interpretation + graphical model + Machine learning + additive models + Model development and analysis + feature selection + linear time + Modeling methodologies	+tics+I.5.2 [Pattern Recognition+Design Methodologyâ Feature Evaluation and Selection+I.6.5 [Simulation and Modeling+Model DevelopmentâModeling Methodologies General Terms Algorithms+Measurement+Design+Verification Keywords Visualization+Diagnostics+Functional ANOVA+Additive models+Graphical models+Interpretation+Feature Selection	Many automated learning procedures lack interpretability, operating effectively as a black box: providing a prediction tool but no explanation of the underlying dynamics that drive it. A common approach to interpretation is to plot the dependence of a learned function on one or two predictors. We present a method that seeks not to display the behavior of a function, but to evaluate the importance of non-additive interactions within any set of variables. Should the function be close to a sum of low dimensional components, these components can be viewed and even modeled parametrically. Alternatively, the work here provides an indication of where intrinsically high-dimensional behavior takes place.The calculations used in this paper correspond closely with the functional ANOVA decomposition; a well-developed construction in Statistics. In particular, the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of additive models. The algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes.
7B8CC94C	Knowledge Discovery and Data Mining	naiyao zhang + muyuan wang + zhiwei li + lie lu + weiying ma	2005	Web object indexing using domain knowledge	 + domain knowledge + Retrieval models and ranking + web pages + indexing + latent semantic indexing + Retrieval tasks and goals + information retrieval + indexation + Information systems applications + Clustering and classification + Information retrieval + Data mining + Clustering + Information systems + Applied computing + Document management and text processing + link analysis + deep web	Bad Record	A web object is defined to represent any meaningful object embedded in web pages (e.g. images, music) or pointed to by hyperlinks (e.g. downloadable files). In many cases, users would like to search for information of a certain 'object', rather than a web page containing the query terms. To facilitate web object searching and organizing, in this paper, we propose a novel approach to web object indexing, by discovering its inherent structure information with existed domain knowledge. In our approach, first, Layered LSI spaces are built for a better representation of the hierarchically structured domain knowledge, in order to emphasize the specific semantics and term space in each layer of the domain knowledge. Meanwhile, the web object representation is constructed by hyperlink analysis, and further pruned to remove the noises. Then an optimal matching between the web object and the domain knowledge is performed, in order to pick out the structure attributes of the web object from the knowledge. Finally, the obtained structure attributes are used to re-organize and index the web objects. Our approach also indicates a new promising way to use trust-worthy Deep Web knowledge to help organize dispersive information of Surface Web.
58C213C7	Knowledge Discovery and Data Mining	runa bhaumik + robin burke + bamshad mobasher + chad williams	2005	Analysis and detection of segment-focused attacks against collaborative recommendation	collaborative filtering + recommender system		Significant vulnerabilities have recently been identified in collaborative filtering recommender systems. These vulnerabilities mostly emanate from the open nature of such systems and their reliance on user-specified judgments for building profiles. Attackers can easily introduce biased data in an attempt to force the system to âadaptâ in a manner advantageous to them. Our research in secure personalization is examining a range of attack models, from the simple to the complex, and a variety of recommendation techniques. In this chapter, we explore an attack model that focuses on a subset of users with similar tastes and show that such an attack can be highly successful against both user-based and item-based collaborative filtering. We also introduce a detection model that can significantly decrease the impact of this attack.
5BE67205	Knowledge Discovery and Data Mining	kezhi mao + wenyin tang	2005	Feature selection algorithm for data with both nominal and continuous features	feature selection + search algorithm		Wrapper and filter are two commonly used feature selection schemes. Because of its computational efficiency, the filter method is often the first choice when dealing with large dataset. However, most of filter methods reported in the literature are developed for continuous feature selection. In this paper, we proposed a filter method for mixed data with both continuous and nominal features. The new algorithm includes a novel criterion for mixed feature evaluation, and a novel search algorithm for mixed feature subset generation. The proposed method is tested using a few benchmark real-world problems.
7628BC2B	Knowledge Discovery and Data Mining	ravi kumar + daniel gruhl + jasmine novak + andrew tomkins + ramanathan vaidhyanath guha	2005	The predictive power of online chatter	time series analysis +  + web pages + generic algorithm + prediction + Information retrieval + Information systems	+Algorithms+Experimentation+Measurements Keywords Blogs+Prediction+Sales rank+Time-series analysis	An increasing fraction of the global discourse is migrating online in the form of blogs, bulletin boards, web pages, wikis, editorials, and a dizzying array of new collaborative technologies. The migration has now proceeded to the point that topics reflecting certain individual products are sufficiently popular to allow targeted online tracking of the ebb and flow of chatter around these topics. Based on an analysis of around half a million sales rank values for 2,340 books over a period of four months, and correlating postings in blogs, media, and web pages, we are able to draw several interesting conclusions.First, carefully hand-crafted queries produce matching postings whose volume predicts sales ranks. Second, these queries can be automatically generated in many cases. And third, even though sales rank motion might be difficult to predict in general, algorithmic predictors can use online postings to successfully predict spikes in sales rank.
5DAF52A1	Knowledge Discovery and Data Mining	yu qian + dung t huynh + kang zhang	2005	PatZip: pattern-preserved spatial data compression	spatial data + spatial pattern		This paper presents a compression method, PatZip, to improve the efficiency of spatial pattern mining methods. PatZip can avoid overcompression and stop automatically before pattern is destroyed. Compared with existing compression methods, PatZip is deterministic and its result is reproducible, and original data can be easily recovered. The compression process is data-driven and parameter-free, and requires only O(nlogn) time for n data points.
5E077586	Knowledge Discovery and Data Mining	kamran karimi + howard j hamilton	2005	The TIMERS II algorithm for the discovery of causality	indexation + decision rule + spatial data		We present the Temporal Investigation Method for Enregistered Record Sequences II (TIMERS II), which can be used to classify the relationship between a decision attribute and a number of condition attributes as instantaneous, causal, or acausal. In this paper we consider it possible to refer to both previous and next values of attributes in temporal rules, and thus enhance the definition of acausality. We also present a new algorithm for distinguishing between causality and acausality.
75D08C4A	Knowledge Discovery and Data Mining	david jensen + matthew j rattigan	2005	The case for anomalous link detection	classification + data mining + naive bayes		"
In this paper, we describe the challenges inherent to the Link Prediction (LP) problem in multirelational data mining, and explore the reasons why many LP models have performed poorly. We present the alternate (and complimentary) task of Anomalous Link Discovery (ALD) and qualitatively demonstrate the effectiveness of simple LP models for the ALD task.
"
75291612	Knowledge Discovery and Data Mining	yuko maruyama + kenji yamanishi	2005	Dynamic syslog mining for network failure monitoring	 + Philosophical/theoretical foundations of artificial intelligence + network monitoring + event correlation + hidden markov model + model selection + Computing methodologies + time series + adaptive learning + Artificial intelligence + probabilistic model	+General Terms Experimentation+Theory Keywords Syslog mining+Failure detection+Correlation analysis+Prob- abilistic modeling+Model selection âThe current address is	Syslog monitoring technologies have recently received vast attentions in the areas of network management and network monitoring. They are used to address a wide range of important issues including network failure symptom detection and event correlation discovery. Syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time. This paper proposes a new methodology of dynamic syslog mining in order to detect failure symptoms with higher confidence and to discover sequential alarm patterns among computer devices. The key ideas of dynamic syslog mining are 1) to represent syslog behavior using a mixture of Hidden Markov Models, 2) to adaptively learn the model using an on-line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture components, and 3) to give anomaly scores using universal test statistics with a dynamically optimized threshold. Using real syslog data we demonstrate the validity of our methodology in the scenarios of failure symptom detection, emerging pattern identification, and correlation discovery.
5B5B8F0C	Knowledge Discovery and Data Mining	charu c aggarwal + philip s yu	2005	On Clustering Techniques for Change Diagnosis in Data Streams	categorical data + time change + change detection		In recent years, data streams have become ubiquitous in a variety of applications because of advances in hardware technology. Since data streams may be generated by applications which are time-changing in nature, it is often desirable to explore the underlying changing trends in the data. In this paper, we will explore and survey some of our recent methods for change detection. In particular, we will study methods for change detection which use clustering in order to provide a concise understanding of the underlying trends. We discuss our recent techniques which use micro-clustering in order to diagnose the changes in the underlying data. We also discuss the extension of this method to text and categorical data sets as well community detection in graph data streams.
790397D0	Knowledge Discovery and Data Mining	robert j kingan + alan m hochberg + ronald k pearson	2005	Disease progression modeling from historical clinical databases	 + Applied computing + censoring + logistic model + Modeling and simulation + Life and medical sciences + Consumer health + Computing methodologies + parameter estimation + cluster analysis + Model development and analysis + Health informatics	+J.3 [Life and Medical Sciences+Health+I.6.5 [Simulation and Modeling+Model Development General Terms Algorithms Keywords Disease progression modeling+cluster analysis+NIDDK liver transplant database	This paper considers the problem of modeling disease progression from historical clinical databases, with the ultimate objective of stratifying patients into groups with clearly distinguishable prognoses or suitability for different treatment strategies. To meet this objective, we describe a procedure that first fits clinical variables measured over time to a disease progression model. The resulting parameter estimates are then used as the basis for a stepwise clustering procedure to stratify patients into groups with distinct survival characteristics. As a practical illustration, we apply this procedure to survival prediction, using a liver transplant database from the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK).
59BC11EE	Knowledge Discovery and Data Mining	luo si + rong jin	2005	Adjusting mixture weights of gaussian mixture model via regularized probabilistic latent semantic analysis	mixture model + data analysis + speech recognition + knowledge discovery + probabilistic latent semantic analysis + modeling + data models + empirical study + empirical method + gaussian process + data mining + speaker recognition + gaussian mixture model		Mixture models, such as Gaussian Mixture Model, have been widely used in many applications for modeling data. Gaussian mixture model (GMM) assumes that data points are generated from a set of Gaussian models with the same set of mixture weights. A natural extension of GMM is the probabilistic latent semantic analysis (PLSA) model, which assigns different mixture weights for each data point. Thus, PLSA is more flexible than the GMM method. However, as a tradeoff, PLSA usually suffers from the overfitting problem. In this paper, we propose a regularized probabilistic latent semantic analysis model (RPLSA), which can properly adjust the amount of model flexibility so that not only the training data can be fit well but also the model is robust to avoid the overfitting problem. We conduct empirical study for the application of speaker identification to show the effectiveness of the new model. The experiment results on the NIST speaker recognition dataset indicate that the RPLSA model outperforms both the GMM and PLSA models substantially. The principle of RPLSA of appropriately adjusting model flexibility can be naturally extended to other applications and other types of mixture models.
59DB599A	Knowledge Discovery and Data Mining	blaÅ¾ fortuna + dunja mladenic + miha grcar + marko grobelnik	2005	Data sparsity issues in the collaborative filtering framework	k nearest neighbor + support vector + supervised learning + col + support vector machine + web personalization + sparse data + collaborative filtering		With the amount of available information on the Web growing rapidly with each day, the need to automatically filter the information in order to ensure greater user efficiency has emerged. Within the fields of user profiling and Web personalization several popular content filtering techniques have been developed. In this chapter we present one of such techniques â collaborative filtering. Apart from giving an overview of collaborative filtering approaches, we present the experimental results of confronting the k-Nearest Neighbor (kNN) algorithm with Support Vector Machine (SVM) in the collaborative filtering framework using datasets with different properties. While the k-Nearest Neighbor algorithm is usually used for collaborative filtering tasks, Support Vector Machine is considered a state-of-the-art classification algorithm. Since collaborative filtering can also be interpreted as a classification/regression task, virtually any supervised learning algorithm (such as SVM) can also be applied. Experiments were performed on two standard, publicly available datasets and, on the other hand, on a real-life corporate dataset that does not fit the profile of ideal data for collaborative filtering. We conclude that the quality of collaborative filtering recommendations is highly dependent on the sparsity of available data. Furthermore, we show that kNN is dominant on datasets with relatively low sparsity while SVM-based approaches may perform better on highly sparse data.
7D70AA11	Knowledge Discovery and Data Mining	masashi shimbo + taku kudo + takahiko ito + yuji matsumoto	2005	Application of kernels to link analysis	hits +  + spectrum + kernel method + Information retrieval + link analysis + graph laplacian + graph kernel + Information systems	+graph kernel+HITS+link analysis	The application of kernel methods to link analysis is explored. In particular, Kandola et al.'s Neumann kernels are shown to subsume not only the co-citation and bibliographic coupling relatedness but also Kleinberg's HITS importance. These popular measures of relatedness and importance correspond to the Neumann kernels at the extremes of their parameter range, and hence these kernels can be interpreted as defining a spectrum of link analysis measures intermediate between co-citation/bibliographic coupling and HITS. We also show that the kernels based on the graph Laplacian, including the regularized Laplacian and diffusion kernels, provide relatedness measures that overcome some limitations of co-citation relatedness. The property of these kernel-based link analysis measures is examined with a network of bibliographic citations. Practical issues in applying these methods to real data are discussed, and possible solutions are proposed.
58BF8D6A	Knowledge Discovery and Data Mining	junping zhang + stan z li	2005	Adaptive nonlinear auto-associative modeling through manifold learning	curse of dimensionality + error rate + manifold learning		We propose adaptive nonlinear auto-associative modeling (ANAM) based on Locally Linear Embedding algorithm (LLE) for learning intrinsic principal features of each concept separately and recognition thereby. Unlike traditional supervised manifold learning algorithm, the proposed ANAM algorithm has several advantages: 1) it implicitly embodies discriminant information because the suboptimal parameters of ANAM are determined based on error rate of the validation set. 2) it avoids the curse of dimensionality without loss accuracy because recognition is completed in the original space. Experiments on character and digit databases show that the advantages of the proposed ANAM algorithm.
7D3BBF2E	Knowledge Discovery and Data Mining	jianwei liu + shoujian yu + jiajin le	2005	Dynamic mining hierarchical topic from web news stream data using divisive-agglomerative clustering method	nearest neighbor		Given the popularity of Web news services, we focus our attention on mining hierarchical topic from Web news stream data. To address this problem, we present a Divisive-Agglomerative clustering method to find hierarchical topic from Web news stream. The novelty of the proposed algorithm is the ability to identify meaningful news topics while reducing the amount of computations by maintaining cluster structure incrementally. Our streaming news clustering algorithm also works by leveraging off the nearest neighbors of the incoming streaming news datasets and has ability of identifying the different shapes and different densities of clusters. Experimental results demonstrate that the proposed clustering algorithm produces high-quality topic discovery.
7DC30598	Knowledge Discovery and Data Mining	jie tang + hang li + yunbo cao + zhaohui tang	2005	Email data cleaning	 + Document representation + Retrieval models and ranking + support vector machine + data cleaning + Retrieval tasks and goals + Document filtering + Information retrieval + text mining + Information extraction + Information systems	+Algorithm+Design+Experimentation+Theory. Keywords Text Mining+Data Cleaning+Email Processing+Statistical Learning	Addressed in this paper is the issue of 'email data cleaning' for text mining. Many text mining applications need take emails as input. Email data is usually noisy and thus it is necessary to clean it before mining. Several products offer email cleaning features, however, the types of noises that can be eliminated are restricted. Despite the importance of the problem, email cleaning has received little attention in the research community. A thorough and systematic investigation on the issue is thus needed. In this paper, email cleaning is formalized as a problem of non-text filtering and text normalization. In this way, email cleaning becomes independent from any specific text mining processing. A cascaded approach is proposed, which cleans up an email in four passes including non-text filtering, paragraph normalization, sentence normalization, and word normalization. As far as we know, non-text filtering and paragraph normalization have not been investigated previously. Methods for performing the tasks on the basis of Support Vector Machines (SVM) have also been proposed in this paper. Features in the models have been defined. Experimental results indicate that the proposed SVM based methods can significantly outperform the baseline methods for email cleaning. The proposed method has been applied to term extraction, a typical text mining processing. Experimental results show that the accuracy of term extraction can be significantly improved by using the data cleaning method.
77ABB834	Knowledge Discovery and Data Mining	david gondek + thomas hofmann	2005	Non-redundant clustering with conditional ensembles	 + Cluster analysis + generic algorithm + Learning paradigms + Machine learning + Information systems applications + Computing methodologies + Data mining + Unsupervised learning + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data mining+I.5.3 [Pattern Recognition+Clusteringâ Algorithms General Terms Algorithms Keywords Non-Redundant Clustering+Cluster Ensembles	"Data may often contain multiple plausible clusterings. In order to discover a clustering which is useful to the user, constrained clustering techniques have been proposed to guide the search. Typically, these techniques assume background knowledge in the form of explicit information about the desired clustering. In contrast, we consider the setting in which the background knowledge is instead about an undesired clustering. Such knowledge may be obtained from an existing classification or precedent algorithm. The problem is then to find a novel, ""orthogonal"" clustering in the data. We present a general algorithmic framework which makes use of cluster ensemble methods to solve this problem. One key advantage of this approach is that it takes a base clustering method which is used as a black box, allowing the practitioner to select the most appropriate clustering method for the domain. We present experimental results on synthetic and text data which establish the competitiveness of this framework."
5F455356	Knowledge Discovery and Data Mining	philip k chan + hyoungrae kim	2005	Personalized Search Results with User Interest Hierarchies Learnt from Bookmarks	search engine + web search engine + web pages		Personalized web search incorporates an individual userâs interests when deciding relevant results to return. While, most web search engines are usually designed to serve all users, without considering the interests of individual users. We propose a method to (re)rank the results from a search engine using a learned user profile, called a user interest hierarchy (UIH), from web pages that are of interest to the user. The userâs interest in web pages will be determined implicitly, without directly asking the user. Experimental results indicate that our personalized ranking methods, when used with a popular search engine, can yield more potentially interesting web pages for individual users.
78268E08	Knowledge Discovery and Data Mining	m fassino + lian yan + patrick baldasare	2005	Enhancing the lift under budget constraints: an application in the mutual fund industry	 + lift curve + neural networks + Learning paradigms + budget constraint + Information systems applications + Computing methodologies + Data mining + neural network + customer relationship management + Information systems + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + roc curve + Supervised learning by classification + constrained optimization	+Categories and Subject Descriptors H.4 [Database Management+Database Applications- Data Mining+I.2.6 [Artificial Intelligence+Learning+I.5.2 [Pattern Recognition+Design Methodology- classifier design and evaluation General Terms	A lift curve, with the true positive rate on the y-axis and the customer pull (or contact) rate on the x-axis, is often used to depict the model performance in many data mining applications, especially in the area of customer relationship management (CRM). Typically, these applications concern only the model accuracy at a relatively small pull or contact/intervention rate of the whole customer base, which is predetermined by a budget constraint for the project, e.g., how many customers can be contacted every month. In this paper, we address the important problem of enhancing the lift (true positive rate) at a specified pull rate. We propose two distinct algorithms, which are applicable to different scenarios. In particular, when the binary class label of the training set is extracted from a continuous variable, we can optimize a training objective which takes into account the specified pull rate rather than the class prior, based on the often ignored continuous variable. In those cases where only the binary class label is available during training, we propose a constrained optimization algorithm to maximize the true positive rate related to a specific decision threshold at which the specified pull rate is achieved. We applied both algorithms to our projects of predicting defection (decline in account value) of mutual fund accounts for two major U.S. mutual fund companies and achieved substantial enhancement of the lift at the specified pull rate.
59753D97	Knowledge Discovery and Data Mining	lin lu + margaret h dunham + yu meng	2005	Mining significant usage patterns from clickstream data	web usage mining + customer behavior		Discovery of usage patterns from Web data is one of the primary purposes for Web Usage Mining. In this paper, a technique to generate Significant Usage Patterns (SUP) is proposed and used to acquire significant âuser preferred navigational trailsâ. The technique uses pipelined processing phases including sub-abstraction of sessionized Web clickstreams, clustering of the abstracted Web sessions, concept-based abstraction of the clustered sessions, and SUP generation. Using this technique, valuable customer behavior information can be extracted by Web site practitioners. Experiments conducted using Web log data provided by J.C.Penney demonstrate that SUPs of different types of customers are distinguishable and interpretable. This technique is particularly suited for analysis of dynamic websites.
7E11CCA7	Knowledge Discovery and Data Mining	sanyih hwang + jaideep srivastava + sandeep u mane	2005	Estimating missed actual positives using independent classifiers	capture recapture + conditional independence +  + data mining + capture recapture method + Contingency table analysis + Information systems applications + Data mining + Information systems + conditional mutual information + heuristic algorithm + Statistical paradigms + Probability and statistics + mutual information + Mathematics of computing + real time + Information theory	No keyword found	"Data mining is increasingly being applied in environments having very high rate of data generation like network intrusion detection [7], where routers generate about 300,000 -- 500,000 connections every minute. In such rare class data domains, the cost of missing a rare-class instance is much higher than that of other classes. However, the high cost for manual labeling of instances, the high rate at which data is collected as well as real-time response constraints do not always allow one to determine the actual classes for the collected unlabeled datasets. In our previous work [9], this problem of missed false negatives was explained in context of two different domains -- ""network intrusion detection"" and ""business opportunity classification"". In such cases, an estimate for the number of such missed high-cost, rare instances will aid in the evaluation of the performance of the modeling technique (e.g. classification) used. A capture-recapture method was used for estimating false negatives, using two or more learning methods (i.e. classifiers). This paper focuses on the dependence between the class labels assigned by such learners. We define the conditional independence for classifiers given a class label and show its relation to the conditional independence of the features sets (used by the classifiers) given a class label. The later is a computationally expensive problem and hence, a heuristic algorithm is proposed for obtaining conditionally independent (or less dependent) feature sets for the classifiers. Initial results of this algorithm on synthetic datasets are promising and further research is being pursued."
75A36CBC	Knowledge Discovery and Data Mining	joydeep ghosh + srujana merugu	2005	A distributed learning framework for heterogeneous data sources	conditional independence +  + maximum entropy principle + Information systems applications + Computing methodologies + privacy + Data mining + iterative algorithm + probabilistic model + Information systems + knowledge integration + closed form solution + maximum likelihood + Machine learning + data privacy	+privacy+heterogeneous data sources	We present a probabilistic model-based framework for distributed learning that takes into account privacy restrictions and is applicable to scenarios where the different sites have diverse, possibly overlapping subsets of features. Our framework decouples data privacy issues from knowledge integration issues by requiring the individual sites to share only privacy-safe probabilistic models of the local data, which are then integrated to obtain a global probabilistic model based on the union of the features available at all the sites. We provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principles and describe iterative algorithms that are guaranteed to converge to the optimal solution. For certain commonly occurring special cases involving hierarchically ordered feature sets or conditional independence, we obtain closed form solutions and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem. To address interpretability concerns, we also present a modified formulation where the global model is assumed to belong to a specified parametric family. Finally, to highlight the generality of our framework, we provide empirical results for various learning tasks such as clustering and classification on different kinds of datasets consisting of continuous vector, categorical and directional attributes. The results show that high quality global models can be obtained without much loss of privacy.
7A66873D	Knowledge Discovery and Data Mining	bin he + kevin chenchuan chang	2005	Making holistic schema matching robust: an ensemble approach	 + Information integration + majority voting + Information systems applications + ensemble + data integrity + Data mining + Database management system engines + Information systems + Computer systems organization + performance + Extraction, transformation and loading + Data management systems + Other architectures + Database design and models + data integration + Heterogeneous (hybrid) systems + deep web + Architectures	+Categories and Subject Descriptors H.2.5 [Database Management+Heterogeneous Databases+H.2.8 [Database Management+Database ApplicationsâData Mining General Terms Algorithms+Experimentation+Performance	"The Web has been rapidly ""deepened"" by myriad searchable databases online, where data are hidden behind query interfaces. As an essential task toward integrating these massive ""deep Web"" sources, large scale schema matching (i.e., discovering semantic correspondences of attributes across many query interfaces) has been actively studied recently. In particular, many works have emerged to address this problem by ""holistically"" matching many schemas at the same time and thus pursuing ""mining"" approaches in nature. However, while holistic schema matching has built its promise upon the large quantity of input schemas, it also suffers the robustness problem caused by noisy data quality. Such noises often inevitably arise in the automatic extraction of schema data, which is mandatory in large scale integration. For holistic matching to be viable, it is thus essential to make it robust against noisy schemas. To tackle this challenge, we propose a data-ensemble framework with sampling and voting techniques, which is inspired by bagging predictors. Specifically, our approach creates an ensemble of matchers, by randomizing input schema data into many independently downsampled trials, executing the same matcher on each trial and then aggregating their ranked results by taking majority voting. As a principled basis, we provide analytic justification of the effectiveness of this data-ensemble framework. Further, empirically, our experiments on real Web data show that the ""ensemblization"" indeed significantly boosts the matching accuracy under noisy schema input, and thus maintains the desired robustness of a holistic matcher."
5889C31D	Knowledge Discovery and Data Mining	katarzyna cichon + marzena kryszkiewicz	2005	Support oriented discovery of generalized disjunction-free representation of frequent patterns with negation	data mining		The discovery of frequent patterns has attracted a lot of attention in the data mining community. While an extensive research has been carried out for discovering positive patterns, little has been offered for discovering patterns with negation. An amount of frequent patterns with negation is usually huge and exceeds the number of frequent positive patterns by orders of magnitude. The problem can be significantly alleviated by applying the generalized disjunction-free literal sets representation, which is a concise lossless representation of all frequent patterns, both with and without negation. In this paper, we offer new efficient algorithm GDFLR-SO-Apriori for discovering this representation and evaluate it against the GDFLR-Apriori algorithm.
79E8051E	Knowledge Discovery and Data Mining	jiawei han + xiaoxin yin + philip s yu	2005	Cross-relational clustering with user's guidance	 + feature extraction + data mining + Information systems applications + clustering + relational database + relational databases + Data mining + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Management+Database applications â Data mining General Terms+Algorithms. Keywords+data mining+clustering+relational databases	Clustering is an essential data mining task with numerous applications. However, data in most real-life applications are high-dimensional in nature, and the related information often spreads across multiple relations. To ensure effective and efficient high-dimensional, cross-relational clustering, we propose a new approach, called CrossClus, which performs cross-relational clustering with user's guidance. We believe that user's guidance, even likely in very simple forms, could be essential for effective high-dimensional clustering since a user knows well the application requirements and data semantics. CrossClus is carried out as follows: A user specifies a clustering task and selects one or a small set of features pertinent to the task. CrossClus extracts the set of highly relevant features in multiple relations connected via linkages defined in the database schema, evaluates their effectiveness based on user's guidance, and identifies interesting clusters that fit user's needs. This method takes care of both quality in feature extraction and efficiency in clustering. Our comprehensive experiments demonstrate the effectiveness and scalability of this approach.
7B8E0E04	Knowledge Discovery and Data Mining	xin chen + yifang brook wu	2005	Web mining from competitors' websites	 + association rule + web mining + noun phrase + Information systems applications + Computing methodologies + Data mining + association rule mining + Information systems + Language resources + text mining + Natural language processing + Artificial intelligence	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications â Data mining+I.2.7 [Artificial Intelligence+Natural Language Processing â Text analysis General Terms Algorithms+Documentation+Languages+Measurement+Theory Keywords Text Mining+Background Knowledge+Association Rules Mining+Novelty	This paper presents a framework for user-oriented text mining. It is then illustrated with an example of discovering knowledge from competitors' websites. The knowledge to be discovered is in the form of association rules. A user's background knowledge is represented as a concept hierarchy developed from documents on his/her own website. The concept hierarchy captures the semantic usage of words and relationships among words in background documents. Association rules are identified among the noun phrases extracted from documents on competitors' websites. The interestingness measure, i.e. novelty, which measures the semantic distance between the antecedent and the consequent of a rule in the background knowledge, is computed from the co-occurrence frequency of words and the connection lengths among words in the concept hierarchy. A user evaluation of the novelty of discovered rules demonstrates that the correlation between the algorithm and the human judges is comparable to that between human judges.
68CE9E6A	Knowledge Discovery and Data Mining	chong wang + wenyuan wang	2005	Using term clustering and supervised term affinity construction to boost text classification	machine learning		The similarity measure is a crucial step in many machine learning problems. The traditional cosine similarity suffers from its inability to represent the semantic relationship of terms. This paper explores the kernel-based similarity measure by using term clustering. An affinity matrix of terms is constructed via the co-occurrence of the terms in both unsupervised and supervised ways. Normalized cut is employed to do the clustering to cut off the noisy edges. Diffusion kernel is adopted to measure the kernel-like similarity of the terms in the same cluster. Experiments demonstrate our methods can give satisfactory results, even when the training set is small.
7BA57B10	Knowledge Discovery and Data Mining	albertlaszlo barabasi	2005	The architecture of complexity: the structure and the dynamics of networks, from the web to the cell	 + distance function + drug design + scale free + self organization + world wide web + cellular network + pattern recognition	No keyword found	Networks with complex topology describe systems as diverse as the cell, the World Wide Web or the society. The emergence of most networks is driven by self-organizing processes that are governed by simple but generic laws. The analysis of the cellular network of various organisms shows that cells and complex man-made networks, such as the Internet or the world wide web, and many social and collaboration networks share the same large-scale topology. I will show that the scale-free topology of these complex webs have important consequences on their robustness against failures and attacks, with implications on drug design, the Internet's ability to survive attacks and failures, and the ability of ideas and innovations to spread on the network.
7E961D43	Knowledge Discovery and Data Mining	kave eshghi + george forman + stephane chiocchetti	2005	Finding similar files in large document repositories	 + Document representation + customer satisfaction + similarity + document management + Machine learning + Computing methodologies + Information retrieval + bipartite graph + content management + scalability + Information systems	+Algorithms+Documentation+Management+Performance Keywords content management+document management+near dupli- cate detection	Hewlett-Packard has many millions of technical support documents in a variety of collections. As part of content management, such collections are periodically merged and groomed. In the process, it becomes important to identify and weed out support documents that are largely duplicates of newer versions. Doing so improves the quality of the collection, eliminates chaff from search results, and improves customer satisfaction.The technical challenge is that through workflow and human processes, the knowledge of which documents are related is often lost. We required a method that could identify similar documents based on their content alone, without relying on metadata, which may be corrupt or missing.We present an approach for finding similar files that scales up to large document repositories. It is based on chunking the byte stream to find unique signatures that may be shared in multiple files. An analysis of the file-chunk graph yields clusters of related files. An optional bipartite graph partitioning algorithm can be applied to greatly increase scalability.
7B9B38A0	Knowledge Discovery and Data Mining	anne m denton + christopher besemann	2005	Integration of profile hidden Markov model output into association rule mining	 + hidden markov model + Information systems applications + model building + Data mining + association rule mining + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Management+Database ApplicationsâData Mining General Terms+Algorithms Keywords+association rule mining+profile hidden Markov model+model	Scientific models typically depend on parameters. Preserving the parameter dependence of models in the pattern mining context opens up several applications. Within association rule mining (ARM), the choice of parameters can be studied with more flexibly then in traditional model building. Studying support, confidence, and other rule metrics as a function of model parameters allows conclusions on assumptions underlying the models. We present efficient techniques to handle multiple model output data sets at little more than the cost of one. We integrate output from hidden Markov models into the association rule mining framework, demonstrating the potential for frequent pattern mining in the field of scientific modeling and experimentation.
75665DE4	Knowledge Discovery and Data Mining	hanspeter kriegel + martin pfeifle	2005	Density-based clustering of uncertain data	 + Probabilistic algorithms + distance function + location based service + face recognition + Sequential Monte Carlo methods + Probabilistic reasoning algorithms + Probability and statistics + Mathematics of computing + Markov-chain Monte Carlo methods	No keyword found	In many different application areas, e.g. sensor databases, location based services or face recognition systems, distances between odjects have to be computed based on vague and uncertain data. Commonly, the distances between these uncertain object descriptions are expressed by one numerical distance value. Based on such single-valued distance functions standard data mining algorithms can work without any changes. In this paper, we propose to express the similarity between two fuzzy objects by distance probability functions. These fuzzy distance functions assign a probability value to each possible distance value. By integrating these fuzzy distance functions directly into data mining algorithms, the full information provided by these functions is exploited. In order to demonstrate the benefits of this general approach, we enhance the density-based clustering algorithm DBSCAN so that it can work directly on these fuzzy distance functions. In a detailed experimental evaluation based on artificial and real-world data sets, we show the characteristics and benefits of our new approach.
7AE0756D	Knowledge Discovery and Data Mining	tim rey + alex n kalos	2005	Data mining in the chemical industry	 + six sigma + supply chain + chemical industry + data mining + Computing methodologies + complex system + Modeling and simulation + manufacturing + internal model + Model development and analysis + Modeling methodologies	+General Terms Management+Measurement+Documentation+Performance+Design+Human Factors+Standardization+Verification. Keywords Data mining+manufacturing+chemical industry	In this paper we describe the experience of introducing data mining to a large chemical manufacturing company. The multi-national nature of doing business with multiple business units, presents a unique opportunity for the deployment of data mining. While each business unit has its own objectives and challenges, which may be at odds with those of other units, they also share many common interests and resources. In this environment, data mining can be used to identify potential value-creating opportunities, through large site integration of multiple assets and synergies from the use of common assets, such as site-wide manufacturing facilities, and world-wide supply-chain, purchasing and other shared services. However, issues arise, on one hand from overly complex systems, and on the other hand, from the danger of reaching sub-optimal solutions, if a big enough picture is not considered when executing projects. The company-wide initiative and use of Six Sigma at all levels of the company provided a fertile ground for making the case for data mining and facilitating its acceptance. The Six Sigma mindset of measuring the performance of processes and analyzing data promotes data-based decision making, therefore making data mining a natural extension of this methodology. We will describe the approach for launching a data mining capability within this framework, the strategy for securing upper management support, drawing from internal modeling, statistical, and other communities, and from external consultants and universities. Lessons learned from industrial case studies, enterprise-wide tool evaluation and peer benchmarking will be discussed.
80354194	Knowledge Discovery and Data Mining	shlomo hershkop + salvatore j stolfo	2005	Combining email models for false positive reduction	 + Cluster analysis + Retrieval models and ranking + Email + Retrieval tasks and goals + data mining + Learning paradigms + Web applications + Computing methodologies + Information retrieval + World Wide Web + machine learning + Unsupervised learning + Information systems + Modeling and simulation + Internet communications tools + Machine learning + spam + Model development and analysis + false positive + Model verification and validation + correlation function	Data Mining + Email Mining + Spam + Multiple Classifiers + Model Combination + Aggregators + False Positive Reduction	Machine learning and data mining can be effectively used to model, classify and discover interesting information for a wide variety of data including email. The Email Mining Toolkit, EMT, has been designed to provide a wide range of analyses for arbitrary email sources. Depending upon the task, one can usually achieve very high accuracy, but with some amount of false positive tradeoff. Generally false positives are prohibitively expensive in the real world. In the case of spam detection, for example, even if one email is misclassified, this may be unacceptable if it is a very important email. Much work has been done to improve specific algorithms for the task of detecting unwanted messages, but less work has been report on leveraging multiple algorithms and correlating models in this particular domain of email analysis.EMT has been updated with new correlation functions allowing the analyst to integrate a number of EMT's user behavior models available in the core technology. We present results of combining classifier outputs for improving both accuracy and reducing false positives for the problem of spam detection. We apply these methods to a very large email data set and show results of different combination methods on these corpora. We introduce a new method to compare multiple and combined classifiers, and show how it differs from past work. The method analyzes the relative gain and maximum possible accuracy that can be achieved for certain combinations of classifiers to automatically choose the best combination.
76F418E4	Knowledge Discovery and Data Mining	aleksander kolcz	2005	Local sparsity control for naive Bayes with extreme misclassification costs	 + naive bayes + data mining + Information systems applications + text mining + Data mining + feature selection + naive bayes classifier + Information systems	No keyword found	In applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable. This limits the utility of a classifier to a range in which such constraints can be met. Naive Bayes, which has proven to be very useful in text mining applications due to high scalability, can be particularly affected. Although its 0/1 loss tends to be small, its misclassifications are often made with apparently high confidence. Aside from efforts to better calibrate Naive Bayes scores, it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance. Traditionally, sparsity is controlled globally, and the result for any particular document may vary. In this work we examine the merits of local sparsity control for Naive Bayes in the context of highly asymmetric misclassification costs. In experiments with three benchmark document collections we demonstrate clear advantages of document-level feature selection. In the extreme cost setting, multinomial Naive Bayes with local sparsity control is able to outperform even some of the recently proposed effective improvements to the Naive Bayes classifier. There are also indications that local feature selection may be preferable in different cost settings.
77CA1885	Knowledge Discovery and Data Mining	geetha jagannathan + rebecca n wright	2005	Privacy-preserving distributed k-means clustering over arbitrarily partitioned data	hits + computer network +  + k means clustering + private information + data mining + Information systems applications + link analysis + Data mining + graph kernel + Information systems	+Categories and Subject Descriptors H.2.8 [Database Applications+Data Mining General Terms Algorithms+Security	Advances in computer networking and database technologies have enabled the collection and storage of vast quantities of data. Data mining can extract valuable knowledge from this data, and organizations have realized that they can often obtain better results by pooling their data together. However, the collected data may contain sensitive or private information about the organizations or their customers, and privacy concerns are exacerbated if data is shared between multiple organizations.Distributed data mining is concerned with the computation of models from data that is distributed among multiple participants. Privacy-preserving distributed data mining seeks to allow for the cooperative computation of such models without the cooperating parties revealing any of their individual data items. Our paper makes two contributions in privacy-preserving data mining. First, we introduce the concept of arbitrarily partitioned data, which is a generalization of both horizontally and vertically partitioned data. Second, we provide an efficient privacy-preserving protocol for k-means clustering in the setting of arbitrarily partitioned data.
7DB97804	Knowledge Discovery and Data Mining	jing zhou + robert a stine + dean p foster + lyle h ungar	2005	Streaming feature selection using alpha-investing	 + stepwise regression + Statistical graphics + Feature selection + Machine learning algorithms + prediction model + penalty method + Computing methodologies + classification + Statistical paradigms + Machine learning + multiple regression + Probability and statistics + Mathematics of computing + false discovery rate + feature selection	+Multiple Regression+Feature Selection+False Discovery Rate	In Streaming Feature Selection (SFS), new features are sequentially considered for addition to a predictive model. When the space of potential features is large, SFS offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model. We describe Î±-investing, an adaptive complexity penalty method for SFS which dynamically adjusts the threshold on the error reduction required for adding a new feature. Î±-investing gives false discovery rate-style guarantees against overfitting. It differs from standard penalty methods such as AIC, BIC or RIC, which always drastically over- or under-fit in the limit of infinite numbers of non-predictive features. Empirical results show that SFS is competitive with much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with over a million potential features.
5CB1DABF	Knowledge Discovery and Data Mining	weiping ge + wei wang + baile shi + xiaorong li	2005	A privacy-preserving classification mining algorithm	decision tree + data type + data mining + probability distribution		Privacy-preserving classification mining is one of the fast-growing sub-areas of data mining. How to perturb original data and then build a decision tree based on perturbed data is the key research challenge. By applying transition probability matrix this paper proposes a novel privacy-preserving classification mining algorithm which suits all data types, arbitrary probability distribution of original data, and perturbing all attributes (including label attribute). Experimental results demonstrate that decision tree built using this algorithm on perturbed data has comparable classifying accuracy to decision tree built using un-privacy-preserving algorithm on original data.
7B870A4F	Knowledge Discovery and Data Mining	antti ukkonen + mikael fortelius + heikki mannila	2005	Finding partial orders from unordered 0-1 data	partial order + score function + genetics + total order + difference in differences		"
In applications such as paleontology and medical genetics the 0-1 data has an underlying unknown order (the ages of the fossil sites, the locations of markers in the genome). The order might be total or partial: for example, two sites in di erent parts of the globe might be ecologically incomparable, or the ordering of certain markers might be di erent in di erent subgroups of the data. We consider the following problem. Given a table over a set of 0-1 variables, nd a partial order for the rows minimizing a score function and being as speci c as possible. The score function can be, e.g., the number of changes from 1 to 0 in a column (for paleontology) or the likelihood of the marker sequence (for genomic data). Our solution for this task rst constructs small totally ordered fragments of the partial order, then nds good orientations for the fragments, and nally uses a simple and e cient heuristic method for nding a partial order that corresponds well with the collection of fragments. We describe the method, discuss its properties, and give empirical results on paleontological data demonstrating the usefulness of the method. In the application the use of the method highlighted some previously unknown properties of the data and pointed out probable errors in the data.
"
6846AE70	Knowledge Discovery and Data Mining	fang chu + d stott parker + carlo zaniolo + yizhou wang	2005	Improving mining quality by exploiting data dependency	data model + posterior probability + cost efficiency + spectrum + data mining + protein function prediction + missing values + belief propagation + data acquisition		The usefulness of the results produced by data mining methods can be critically impaired by several factors such as (1) low quality of data, including errors due to contamination, or incompleteness due to limited bandwidth for data acquisition, and (2) inadequacy of the data model for capturing complex probabilistic relationships in data. Fortunately, a wide spectrum of applications exhibit strong dependencies between data samples. For example, the readings of nearby sensors are generally correlated, and proteins interact with each other when performing crucial functions. Therefore, dependencies among data can be successfully exploited to remedy the problems mentioned above. In this paper, we propose a unified approach to improving mining quality using Markov networks as the data model to exploit local dependencies. Belief propagation is used to efficiently compute the marginal or maximum posterior probabilities, so as to clean the data, to infer missing values, or to improve the mining results from a model that ignores these dependencies. To illustrate the benefits and great generality of the technique, we present its application to three challenging problems: (i) cost-efficient sensor probing, (ii) enhancing protein function predictions, and (iii) sequence data denoising.
7CA70E28	Knowledge Discovery and Data Mining	haifeng chen + cristian ungureanu + kenji yoshihira + guofei jiang	2005	Failure detection and localization in component based systems by online tracking	 + Management of computing and information systems + System management + File systems management + Professional topics + Computing methodologies + e commerce + distributed computing + synthetic data + Machine learning + expectation maximization + prediction error + Social and professional topics + statistics	+Categories and Subject Descriptors K.6.4 [Management of Computing and Information Systems+System Management+I.2.6 [Artificial Intelligence+Learning General Terms Algorithms+Management Keywords Subspace decomposition+online tracking+statistics+failure detec- tion+distributed computing+Internet services	The increasing complexity of today's systems makes fast and accurate failure detection essential for their use in mission-critical applications. Various monitoring methods provide a large amount of data about system's behavior. Analyzing this data with advanced statistical methods holds the promise of not only detecting the errors faster, but also detecting errors which are difficult to catch with current monitoring tools. Two challenges to building such detection tools are: the high dimensionality of observation data, which makes the models expensive to apply, and frequent system changes, which make the models expensive to update. In this paper, we present algorithms to reduce the dimensionality of data in a way that makes it easy to adapt to system changes. We decompose the observation data into signal and noise subspaces. Two statistics, the Hotelling T2 score and squared prediction error (SPE) are calculated to represent the data characteristics in signal and noise subspaces respectively. Instead of tracking the original data, we use a sequentially discounting expectation maximization (SDEM) algorithm to learn the distribution of the two extracted statistics. A failure event can then be detected based on the abnormal change of the distribution. Applying our technique to component interaction data in a simple e-commerce application shows better accuracy than building independent profiles for each component. Additionally, experiments on synthetic data show that the detection accuracy is high even for changing systems.
7C28B39D	Knowledge Discovery and Data Mining	sheng zhong + rebecca n wright + zhiqiang yang	2005	Anonymity-preserving data collection	 + data mining + Data structures + Data layout + data collection + Information systems + trusted third party + Theory of computation + Data encryption + Security and privacy + Data management systems + anonymity + refinement + Computational complexity and cryptography + Cryptography	No keyword found	Protection of privacy has become an important problem in data mining. In particular, individuals have become increasingly unwilling to share their data, frequently resulting in individuals either refusing to share their data or providing incorrect data. In turn, such problems in data collection can affect the success of data mining, which relies on sufficient amounts of accurate data in order to produce meaningful results. Random perturbation and randomized response techniques can provide some level of privacy in data collection, but they have an associated cost in accuracy. Cryptographic privacy-preserving data mining methods provide good privacy and accuracy properties. However, in order to be efficient, those solutions must be tailored to specific mining tasks, thereby losing generality.In this paper, we propose efficient cryptographic techniques for online data collection in which data from a large number of respondents is collected anonymously, without the help of a trusted third party. That is, our solution allows the miner to collect the original data from each respondent, but in such a way that the miner cannot link a respondent's data to the respondent. An advantage of such a solution is that, because it does not change the actual data, its success does not depend on the underlying data mining problem. We provide proofs of the correctness and privacy of our solution, as well as experimental data that demonstrates its efficiency. We also extend our solution to tolerate certain kinds of malicious behavior of the participants.
7CA6FFD8	Knowledge Discovery and Data Mining	ryohei fujimaki + kazuo machida + takehisa yairi	2005	An approach to spacecraft anomaly detection problem using kernel feature space	 + expert system + feature space + Computing methodologies + Information systems applications + time series + anomaly detection + principal component + kernel function + Data mining + behavior modeling + Information systems + spacecraft + probabilistic reasoning + normal operator + international space station + a priori knowledge + qualitative reasoning + Machine learning + time series data + principal component analysis	+âAddress+4-6-1+Komaba+Meguro-ku+Tokyo 153-8904	"Development of advanced anomaly detection and failure diagnosis technologies for spacecraft is a quite significant issue in the space industry, because the space environment is harsh, distant and uncertain. While several modern approaches based on qualitative reasoning, expert systems, and probabilistic reasoning have been developed recently for this purpose, any of them has a common difficulty in obtaining accurate and complete a priori knowledge on the space systems from human experts. A reasonable alternative to this conventional anomaly detection method is to reuse a vast amount of telemetry data which is multi-dimensional time-series continuously produced from a number of system components in the spacecraft.This paper proposes a novel ""knowledge-free"" anomaly detection method for spacecraft based on Kernel Feature Space and directional distribution, which constructs a system behavior model from the past normal telemetry data from a set of telemetry data in normal operation and monitors the current system status by checking incoming data with the model.In this method, we regard anomaly phenomena as unexpected changes of causal associations in the spacecraft system, and hypothesize that the significant causal associations inside the system will appear in the form of principal component directions in a high-dimensional non-linear feature space which is constructed by a kernel function and a set of data.We have confirmed the effectiveness of the proposed anomaly detection method by applying it to the telemetry data obtained from a simulator of an orbital transfer vehicle designed to make a rendezvous maneuver with the International Space Station."
5D75CE7B	Knowledge Discovery and Data Mining	hua huo + boqin feng	2005	Retrieval based on language model with relative entropy and feedback	language model + information retrieval + relative entropy + query language		A new method for information retrieval which is on the basis of language model with relative entropy and feedback is presented in this paper. The method builds a query language model and document language models respectively for the query and the documents. We rank the documents according to the relative entropies of the estimated document language models with respect to the estimated query language model. The feedback documents are used to estimate a query model by the approach that we assume that the feedback documents are generated by a combined model in which one component is the feedback document language model and the other is the collection language model. Experimental results show that the method is effective for feedback documents and performs better than the basic language modeling approach. The results also indicate that the performance of the method is sensitive to both the smoothing parameters and the interpolation coefficients used to estimate the values of the language models.
58FD83BF	Knowledge Discovery and Data Mining	zhihua zhou + ming li	2005	SETRED: self-training with editing	semi supervised learning + 		"
Self-training is a semi-supervised learning algorithm in which a learner keeps on labeling unlabeled examples and retraining itself on an enlarged labeled training set. Since the self-training process may erroneously label some unlabeled examples, sometimes the learned hypothesis does not perform well. In this paper, a new algorithm named Setred is proposed, which utilizes a speciÂ¯c data editing method to identify and remove the mislabeled examples from the self-labeled data. In detail, in each iteration of the self-training process, the local cut edge weight statistic is used to help estimate whether a newly labeled example is reliable or not, and only the reliable self-labeled examples are used to enlarge the labeled training set. Experiments show that the introduction of data editing is beneÂ¯cial, and the learned hypotheses of Setred outperform those learned by the standard self-training algorithm.
"
777E229B	Knowledge Discovery and Data Mining	bhavani raskutti + alan herschtal	2005	Predicting the product purchase patterns of corporate customers	 + feature space + Information systems applications + machine learning + Data mining + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Application+Data mining General Terms+Design Keywords+Upsell+Learning from Few Positive Examples+Area under ROC+SVM Applications	This paper describes TIPPPS (Time Interleaved Product Purchase Prediction System), which analyses billing data of corporate customers in a large telecommunications company in order to predict high value upsell opportunities. The challenges presented by this prediction problem are significant. Firstly, the diversity of products used by corporate telecommunications customers is huge. This, coupled with low product take-up rates, makes this a problem of learning from a very high dimensional feature space with very few minority examples. Further, it is important to give priority specifically to the identification of those new customers who are of high value. These challenges are overcome by introducing a number of modifications to standard data pre-processing and machine learning algorithms, the most important of which are time-interleaving of data and value weighting. Time interleaving is the concatenation of examples from multiple time periods, thus increasing the number of training examples, and hence the number of minority examples. Value weighting assigns importance to minority examples in proportion to the dollar value of take-up, thus biasing the system to identify high value customers. These modifications create a novel algorithm that makes the prediction system practical and usable.Comparison with other techniques designed for similar problems shows that the expected average improvement in ranking accuracy achieved using these modifications is 3.7%. TIPPPS has been in operation for several months and has been successful in identifying many upsell opportunities that were not identified by using the previous manual system.
7B80C6EE	Knowledge Discovery and Data Mining	michinari momma	2005	Efficient computations via scalable sparse kernel partial least squares and boosted latent features	 + loss function + prediction model + support vector machine + Machine learning + robust regression + kernel method + Computing methodologies + kernel regression + partial least squares + nonlinear optimization	+Categories and Subject Descriptors+I.5.1 Pattern Recog- nition+Computing Methodologies General Terms+Algorithms Keywords+Scalable and sparse kernel method+Partial Least Squares+Boosted Latent Features	"Kernel partial least squares (KPLS) has been known as a generic kernel regression method and proven to be competitive with other kernel regression methods such as support vector machines for regression (SVM) and kernel ridge regression. Kernel boosted latent features (KBLF) is a variant of KPLS for any differentiable convex loss functions. It provides a more flexible framework for various predictive modeling tasks such as classification with logistic loss and robust regression with L1 norm loss, etc. However, KPLS and KBLF solutions are dense and thus not suitable for large-scale computations. Sparsification of KPLS solutions has been studied for dual and primal forms. For dual sparsity, it requires solving a nonlinear optimization problem at every iteration step and its computational burden limits its applicability to general regression tasks.In this paper, we propose simple heuristics to approximate sparse solutions for KPLS and the framework is also applied for sparsifying KBLF solutions. The algorithm provides an interesting ""path"" from a maximum residual criterion based algorithm with orthogonality conditions to the dense KPLS/KBLF. With the orthogonality, it differentiates itself from many existing forward selection-type algorithms. The computational advantage is illustrated by benchmark datasets and comparison to SVM is done."
7EAE4EB3	Knowledge Discovery and Data Mining	alok choudhary + ying liu + weikeng liao	2005	A two-phase algorithm for fast discovery of high utility itemsets	 + association rule mining		"
Traditional association rules mining cannot meet the demands arising from some real applications. By considering the different values of individual items as utilities, utility mining focuses on identifying the itemsets with high utilities. In this paper, we present a Two-Phase algorithm to efficiently prune down the number of candidates and precisely obtain the complete set of high utility itemsets. It performs very efficiently in terms of speed and memory cost both on synthetic and real databases, even on large databases that are difficult for existing algorithms to handle.
"
5E601591	Knowledge Discovery and Data Mining	leuohong wang + tongwen lee	2005	Collecting topic-related web pages for link structure analysis by using a potential hub and authority first approach	web mining + anchor text + web pages		Constructing a base set consisting of topic-related web pages is a preliminary step for those web mining algorithms which use the link structure analysis technique based on HITS. However, except checking the anchor text of links and the content of pages, there has been few of research addressing other possibilities to improve topic relevance while collecting the base set. In this paper, we propose a potential hub and authority first (PHA-first) approach utilizing the concept of hub and authority to filter web pages. We investigate the satisfaction of dozens of users about the pages recommended by our method and HITS on different topics. The results indicate that our method is superior to HITS in most cases. In addition, we also evaluate the recall and precision measures of our method. The results show that our method is with relative high precision and low recall for all topics.
802875B1	Knowledge Discovery and Data Mining	edward h herskovits + rong chen	2005	A Bayesian network classifier with inverse tree structure for voxelwise magnetic resonance image analysis	 + bayesian network + Multivariate statistics + Life and medical sciences + tree structure + magnetic resonance image + Health care information systems + hidden variables + Applied computing + classifier + Probability and statistics + variable selection + Mathematics of computing + latent variable + markov blanket	+systems	We propose a Bayesian-network classifier with inverse-tree structure (BNCIT) for joint classification and variable selection. The problem domain of voxelwise magnetic-resonance image analysis often involves millions of variables but only dozens of samples. Judicious variable selection may render classification tractable, avoid over-fitting, and improve classifier performance. BNCIT embeds the variable-selection process within the classifier-training process, which makes this algorithm scalable. BNCIT is based on a Bayesian-network model with inverse-tree structure, i.e., the class variable C is a leaf node, and predictive variables are parents of C; thus, the classifier-training process returns a parent set for C, which is a subset of the Markov blanket of C. BNCIT uses voxels in the parent set, and voxels that are probabilistically equivalent to them, as variables for classification of new image data. Since the data set has a limited number of samples, we use the jackknife method to determine whether the classifier generated by BNCIT is a statistical artifact. In order to enhance stability and improve classification accuracy, we model the state of the probabilistically equivalent voxels with a latent variable. We employ an efficient method for determining states of hidden variables, thus reducing dramatically the computational cost of model generation. Experimental results confirm the accuracy and efficiency of BNCIT.
76EA17E1	Knowledge Discovery and Data Mining	saharon rosset	2005	Robust boosting and its relation to bagging	 + loss function + boosting + bagging + Machine learning + Computing methodologies + Probability and statistics + Mathematics of computing + gradient descent + function space	+Bagging+Robust Fitting	"Several authors have suggested viewing boosting as a gradient descent search for a good fit in function space. At each iteration observations are re-weighted using the gradient of the underlying loss function. We present an approach of weight decay for observation weights which is equivalent to ""robustifying"" the underlying loss function. At the extreme end of decay this approach converges to Bagging, which can be viewed as boosting with a linear underlying loss function. We illustrate the practical usefulness of weight decay for improving prediction performance and present an equivalence between one form of weight decay and ""Huberizing"" --- a statistical method for making loss functions more robust."
5D7192ED	Knowledge Discovery and Data Mining	asavin meengen + arit thammano	2005	A new evolutionary neural network classifier	artificial neural network + evolutionary algorithm		This paper proposes two new concepts: (1) the new evolutionary algorithm and (2) the new approach to deal with the classification problems by applying the concepts of the fuzzy c-means algorithm and the evolutionary algorithm to the artificial neural network. During training, the fuzzy c-means algorithm is initially used to form the clusters in the cluster layer; then the evolutionary algorithm is employed to optimize those clusters and their parameters. During testing, the class whose cluster node returns the maximum output value is the result of the prediction. This proposed model has been benchmarked against the standard backpropagation neural network, the fuzzy ARTMAP, C4.5, and CART. The results on six benchmark problems are very encouraging.
5D7483B2	Knowledge Discovery and Data Mining	prasanna desikan + colin delong + jaideep srivastava	2005	USER: User-Sensitive Expert Recommendations for Knowledge-Dense Environments	recommender system + e commerce		Traditional recommender systems tend to focus on e-commerce applications, recommending products to users from a large catalog of available items. The goal has been to increase sales by tapping into the userâs interests by utilizing information from various data sources to make relevant recommendations. Education, government, and policy websites face parallel challenges, except the product is information and their users may not be aware of what is relevant and what isnât. Given a large, knowledge-dense website and a nonexpert user searching for information, making relevant recommendations becomes a significant challenge. This paper addresses the problem of providing recommendations to non-experts, helping them understand what they need to know, as opposed to what is popular among other users. The approach is usersensitive in that it adopts a âmodel of learningâ whereby the userâs context is dynamically interpreted as they browse and then leveraging that information to improve our recommendations.
7B7AC54C	Knowledge Discovery and Data Mining	hwanjo yu	2005	SVM selective sampling for ranking with application to data retrieval	 + data retrieval + information retrieval + Computing methodologies + Information systems applications + relational database + machine learning + Information systems + active learning + support vector machine + sampling technique + ranking + partial order	+Categories and Subject Descriptors I.m [Computing Methodologies+Miscellaneous+H.4 [Information Systems Applications+Miscellaneous General Terms Algorithms Keywords Support vector machine+Ranking+Selective sampling+Active learn- ing	"Learning ranking (or preference) functions has been a major issue in the machine learning community and has produced many applications in information retrieval. SVMs (Support Vector Machines) - a classification and regression methodology - have also shown excellent performance in learning ranking functions. They effectively learn ranking functions of high generalization based on the ""large-margin"" principle and also systematically support nonlinear ranking by the ""kernel trick"". In this paper, we propose an SVM selective sampling technique for learning ranking functions. SVM selective sampling (or active learning with SVM) has been studied in the context of classification. Such techniques reduce the labeling effort in learning classification functions by selecting only the most informative samples to be labeled. However, they are not extendable to learning ranking functions, as the labeled data in ranking is relative ordering, or partial orders of data. Our proposed sampling technique effectively learns an accurate SVM ranking function with fewer partial orders. We apply our sampling technique to the data retrieval application, which enables fuzzy search on relational databases by interacting with users for learning their preferences. Experimental results show a significant reduction of the labeling effort in inducing accurate ranking functions."
7B1CE18E	Knowledge Discovery and Data Mining	hiroki arimura + satoshi morinaga + yosuke sakao + susumu akamine + takahiro ikeda	2005	Key semantics extraction by dependency tree mining	 + natural language processing + dependence analysis + Information systems applications + text mining + knowledge discovery + Data mining + data collection + natural language + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Management+Database Applications- Data Mining General Terms+Algorithms+Languages Keywords+text mining+syntactic dependency+tree enu- meration+redundancy reduction+phrase/sentence reconstruc- tion	We propose a new text mining system which extracts characteristic contents from given documents. We define Key semantics as characteristic sub-structures of syntactic dependencies in the given documents, and consider the following three tasks in this paper: 1)Key semantics extraction: extracting characteristic syntactic dependency structures not only as ordered trees but also as unordered trees and free trees, 2)Redundancy reduction: from the result of extraction, deleting redundant dependency structures such as sub-structures or equivalent structures of the others, and 3)Phrase/sentence reconstruction: generating a phrase or sentence in a natural language corresponding to the extracted structure.Our system is a combination of natural language processing techniques and tree mining techniques. The system consists of the following five units: 1) syntactic dependency analysis unit, 2) input filters, 3) characteristic ordered subtree extraction unit, 4) output filters, and 5) phrase/sentence reconstruction unit. Although ordered trees are extracted in the third unit, the overall behavior of the system can be switched into the extraction of ordered trees, unordered trees, or free trees depending on which of the input filters is/are applied in the second step. The output filters delete redundant trees from the extraction result for efficient knowledge discovery. Finally, phrases or sentences corresponding to the extracted subtrees are reconstructed by utilizing the input documents.We demonstrate the validity of our system by showing experimental results using real data collected at a help desk and TDT pilot corpus.
78C5B477	Knowledge Discovery and Data Mining	david emmitt + steven greco + robert atlas + bilahari akkiraju + sara j graves + rahul ramachandran + xiang li + sunil movva + joseph terry + juancarlos jusem	2005	Automated detection of frontal systems from numerical model-generated data	 + k means clustering + Applied computing + feature space + bayes classifier + atmospheric science + feature vector + Physical sciences and engineering	+spatial data mining	Fronts are significant meteorological phenomena of interest. The extraction of frontal systems from observations and model data can greatly benefit many kinds of research and applications in atmospheric sciences. Due to the huge amount of observational and model data available nowadays, automated extraction of front systems is necessary. This paper presents an automated method to detect frontal systems from numerical model-generated data. In this method, a frontal system is characterized by a vector of features, comprised of parameters derived from the model wind field. K-means clustering is applied to the generated sample set of the feature vectors to partition the feature space and to identify clusters representing the fronts. The probability that a model grid belongs to a front is estimated based on its feature vector. The probability image is generated corresponding to the model grids. A hierarchical thresholding technique is applied to the probability image to identify the frontal systems and a Gaussian Bayes classifier is trained to determine the proper threshold value. This is followed by post processing to filter out false signatures. Experiment results from this method are in good agreement with the ones identified by the domain experts.
7D93BCAF	Knowledge Discovery and Data Mining	boulos harb + sudipto guha	2005	Wavelet synopsis for data streams: minimizing non-euclidean error	Theory of computation +  + algorithms + relative error + Information storage systems + Discrete mathematics + Information retrieval + Mathematics of computing + streaming algorithm + streaming algorithms + Design and analysis of algorithms + Information systems	Wavelet Synopses + Streaming Algorithms	We consider the wavelet synopsis construction problem for data streams where given n numbers we wish to estimate the data by constructing a synopsis, whose size, say B is much smaller than n. The B numbers are chosen to minimize a suitable error between the original data and the estimate derived from the synopsis.Several good one-pass wavelet construction streaming algorithms minimizing the l2 error exist. For other error measures, the problem is less understood. We provide the first one-pass small space streaming algorithms with provable error guarantees (additive approximation) for minimizing a variety of non-Euclidean error measures including all weighted lp (including lâ) and relative error lp metrics.In several previous works solutions (for weighted l2, lâ and maximum relative error) where the B synopsis coefficients are restricted to be wavelet coefficients of the data were proposed. This restriction yields suboptimal solutions on even fairly simple examples. Other lines of research, such as probabilistic synopsis, imposed restrictions on how the synopsis was arrived at. To the best of our knowledge this paper is the first paper to address the general problem, without any restriction on how the synopsis is arrived at, as well as provide the first streaming algorithms with guaranteed performance for these classes of error measures.
5A86DB4C	Knowledge Discovery and Data Mining	mafruz zaman ashrafi + kate a smith + david taniar	2005	An efficient compression technique for frequent itemset generation in association rule mining	association rule + data mining + information extraction + statistical association + association rule mining + data analysis + knowledge discovery + compression ratio		Association Rule mining is one of the widely used data mining techniques. To achieve a better performance, many efficient algorithms have been proposed. Despite these efforts, we are often unable to complete a mining task because these algorithms require a large amount of main memory to enumerate all frequent itemsets, especially when dataset is large or the user-specified support is low. Thus, it becomes apparent that we need to have an efficient main memory handling technique, which allows association rule mining algorithms to handle larger datasets in main memory. To achieve this goal, in this paper we propose an algorithm for vertical association rule mining that compresses a vertical dataset in an efficient manner, using bit vectors. Our performance evaluations show that the compression ratio attained by our proposed technique is better than those of the other well known techniques.
7FC6490B	Knowledge Discovery and Data Mining	li yang	2005	Building connected neighborhood graphs for isometric data embedding	 + graph connectivity + dimensionality reduction + Information systems applications + Discrete mathematics + Graph theory + Graph algorithms + Mathematics of computing + Data mining + manifold learning + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Management+Database Applications â Data mining+I.5.1 [Pattern Recognition+Models â Geometric+Sta- tistical+G2.2 [Discrete Mathematics+Graph Theory â Graph algorithms General Terms+Algorithms+Experimentation Keywords+Data embedding+dimensionality reduction+manifold	Neighborhood graph construction is usually the first step in algorithms for isometric data embedding and manifold learning that cope with the problem of projecting high dimensional data to a low space. This paper begins by explaining the algorithmic fundamentals of techniques for isometric data embedding and derives a general classification of these techniques. We will see that the nearest neighbor approaches commonly used to construct neighborhood graphs do not guarantee connectedness of the constructed neighborhood graphs and, consequently, may cause an algorithm fail to project data to a single low dimensional coordinate system. In this paper, we review three existing methods to construct k-edge-connected neighborhood graphs and propose a new method to construct k-connected neighborhood graphs. These methods are applicable to a wide range of data including data distributed among clusters. Their features are discussed and compared through experiments.
5E08DBC7	Knowledge Discovery and Data Mining	thu hoang + richard de veaux	2005	Comparison of tree based methods on mammography data	error rate + health care + false positive rate + breast cancer + developing nations + physical examination + data mining		X-ray film mammography and physical examination of the breast are the mainstays for early detection of breast cancer. Unfortunately, error rates for mammograms read by radiologists are high. We examine a particularly difficult to read series of 1618 mammograms where in order to achieve a false positive rate lower than 50%, the false negative rate of radiologists is nearly 25%. We examine a variety of automatic data mining tools in an attempt to improve the accuracy of the diagnosis. Our results suggest that roughly the same or higher accuracy rate than the radiologists can be attained at a much reduced cost. This potential cost savings could have a major financial impact for health care in developing nations.
6E6C16ED	Knowledge Discovery and Data Mining	tran a dung + cao d nguyen + tru h cao	2005	Text classification for DAG-Structured categories	hierarchies + artificial intelligent + svm + tree structure + directed acyclic graph		Hierarchical text classification concerning the relationship among categories has become an interesting problem recently. Most research has focused on tree-structured categories, but in reality directed acyclic graph (DAG) â structured categories, where a child category may have more than one parent category, appear more often. In this paper, we introduce three approaches, namely, flat, tree-based, and DAG-based, for solving the multi-label text classification problem in which categories are organized as a DAG, and documents are classified into both leaf and internal categories. We also present experimental results of the methods using SVMs as classifiers on the Reuters-21578 collection and our data set of research papers in Artificial Intelligence.
5B1F95CC	Knowledge Discovery and Data Mining	xiangdong he + senmiao yuan + xiaolin li	2005	Learning bayesian networks structures from incomplete data: an efficient approach based on extended evolutionary programming	evolutionary programming + premature convergence + minimum description length + fitness function + bayesian network		This paper describes a new data mining algorithm to learn Bayesian networks structures from incomplete data based on extended Evolutionary programming (EP) method and the Minimum Description Length (MDL) metric. This problem is characterized by a huge solution space with a highly multimodal landscape. The algorithm presents fitness function based on expectation, which converts incomplete data to complete data utilizing current best structure of evolutionary process. Aiming at preventing and overcoming premature convergence, the algorithm combines the restart strategy into EP. The experimental results illustrate that our algorithm can learn a good structure from incomplete data.
7D6931F0	Knowledge Discovery and Data Mining	glenn fung + r bharat rao + sathyakama sandilya	2005	Rule extraction from linear support vector machines	 + support vector machine + Machine learning + Computing methodologies + mathematical programming	Rule extraction + Linear classifiers + Mathematical programming + medical decision-support	"We describe an algorithm for converting linear support vector machines and any other arbitrary hyperplane-based linear classifiers into a set of non-overlapping rules that, unlike the original classifier, can be easily interpreted by humans. Each iteration of the rule extraction algorithm is formulated as a constrained optimization problem that is computationally inexpensive to solve. We discuss various properties of the algorithm and provide proof of convergence for two different optimization criteria We demonstrate the performance and the speed of the algorithm on linear classifiers learned from real-world datasets, including a medical dataset on detection of lung cancer from medical images. The ability to convert SVM's and other ""black-box"" classifiers into a set of human-understandable rules, is critical not only for physician acceptance, but also to reducing the regulatory barrier for medical-decision support systems based on such classifiers."
77A32611	Knowledge Discovery and Data Mining	robert jedicke + andrew j connolly + andrew w moore + jeremy kubica	2005	A multiple tree algorithm for the efficient association of asteroid observations	Trees +  + Record storage systems + temporal data + Graph theory + Information systems + Physical sciences and engineering + Applied computing + Information storage systems + B-trees + Discrete mathematics + Directory structures + Mathematics of computing + Astronomy	+Track Initiation	In this paper we examine the problem of efficiently finding sets of observations that conform to a given underlying motion model. While this problem is often phrased as a tracking problem, where it is called track initiation, it is useful in a variety of tasks where we want to find correspondences or patterns in spatial-temporal data. Unfortunately, this problem often suffers from a combinatorial explosion in the number of potential sets that must be evaluated. We consider the problem with respect to large-scale asteroid observation data, where the goal is to find associations among the observations that correspond to the same underlying asteroid. In this domain, it is vital that we can efficiently extract the underlying associations.We introduce a new methodology for track initiation that exhaustively considers all possible linkages. We then introduce an exact tree-based algorithm for tractably finding all compatible sets of points. Further, we extend this approach to use multiple trees, exploiting structure from several time steps at once. We compare this approach to a standard sequential approach and show how the use of multiple trees can provide a significant benefit.
769DD402	Knowledge Discovery and Data Mining	valery a petrushin	2005	Mining rare and frequent events in multi-camera surveillance video using self-organizing maps	 + visualization + Special purpose systems + Learning paradigms + Computing methodologies + color histogram + unsupervised learning + self organizing maps + Computer systems organization + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + Other architectures + Supervised learning by classification + Architectures	+General Terms Algorithms+Experimentation. Keywords Indoor surveillance+rare event detection+unsupervised learning+self-organizing	"This paper describes a method for unsupervised classification of events in multi-camera indoors surveillance video. This research is a part of the Multiple Sensor Indoor Surveillance (MSIS) project which uses 32 AXIS-2100 webcams that observe an office environment. The research was inspired by the following practical problem: how automatically classify and visualize a 24 hour long video captured by 32 cameras? Raw data are sequences of JPEG images captured by webcams at the rate 2-6 Hz. The following features are extracted from the image data: foreground pixels' spatial distribution and color histogram. The data are integrated by event by averaging motion and color features and creating a ""summary"" frame which accumulates all foreground pixels of frames of the event into one image. The self-organizing map (SOM) approach is applied to event data for clustering and visualization. One-level and two-level SOM clustering are used. A tool for browsing results allows exploring units of the SOM maps at different levels of hierarchy, clusters of units and distances between units in 3D space. A special technique has been developed to visualize rare events. The results are presented and discussed."
5CDA6F3D	Knowledge Discovery and Data Mining	sangwook kim + jeehee yoon + sanghyun park + jungim won + woocheol kim	2005	A DNA index structure using frequency and position information of genetic alphabet	dna sequence + expressed sequence tag + nucleotides + molecular biology + indexing + sliding window + genetics + transcription factor + indexation		Exact match queries, wildcard match queries, and k-mismatch queries are widely used in lots of molecular biology applications including the searching of ESTs (Expressed Sequence Tag) and DNA transcription factors. In this paper, we suggest an efficient indexing and processing mechanism for such queries. Our indexing method places a sliding window at every possible location of a DNA sequence and extracts its signature by considering the occurrence frequency of each nucleotide. It then stores a set of signatures using a multi-dimensional index, such as the R*-tree. Also, by assigning a weight to each position of a window, it prevents signatures from being concentrated around a few spots in indexing space. Our query processing method converts a query sequence into a multi-dimensional rectangle and searches the index for the signatures overlapped with the rectangle.
7BA755F7	Knowledge Discovery and Data Mining	charu c aggarwal	2005	Towards exploratory test instance specific algorithms for high dimensional classification	 + Information systems applications + decision support + classification + Information systems	Bad Record	In an interactive classification application, a user may find it more valuable to develop a diagnostic decision support method which can reveal significant classification behavior of exemplar records. Such an approach has the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification methods. In this paper, we propose the Subspace Decision Path method which provides the user with the ability to interactively explore a small number of nodes of a hierarchical decision process so that the most significant classification characteristics for a given test instance are revealed. In addition, the SD-Path method can provide enormous interpretability by constructing views of the data in which the different classes are clearly separated out. Even in cases where the classification behavior of the test instance is ambiguous, the SD-Path method provides a diagnostic understanding of the characteristics which result in this ambiguity. Therefore, this method combines the abilities of the human and the computer in creating an effective diagnostic tool for instance-centered high dimensional classification.
76B46EC3	Knowledge Discovery and Data Mining	rayid ghani	2005	Price prediction and insurance for online auctions	profitability +  + feature extraction + data mining + ecommerce + Machine learning + Computing methodologies + classification + machine learning + auctions	+Categories and Subject Descriptors I.2.6 [Artificial Intelligence+Learning. General Terms Algorithms+Experimentation+Economics Keywords Price Prediction+Price Insurance+Classification+Data Mining+Auctions+ECommerce	Online auctions are generating a new class of fine-grained data about online transactions. This data lends itself to a variety of applications and services that can be provided to both buyers and sellers in online marketplaces. We collect data from online auctions and use several classification algorithms to predict the probable-end prices of online auction items. This paper describes the feature extraction and selection process, and several machine learning formulations of the price prediction problem. As a prototype application, we developed Auction Price Insurance that uses the predicted end-price to offer price insurance to sellers in online auctions. We define Price Insurance as a service that offers insurance to auction sellers that guarantees a price for their goods, for an appropriate premium. If the item sells for less than the insured price, the seller is reimbursed for the difference. We show that our price prediction techniques are accurate enough to offer price insurance as a profitable business. While this paper deals specifically with online auctions, we believe that this is an interesting case study that applies to dynamic markets where the price of the goods is variable and is affected by both internal and external factors that change over time.
5D9DB473	Knowledge Discovery and Data Mining	bo zhang + qiang li	2005	Cluster-Based rough set construction	incomplete information + rough set theory + rough set + decision rule + cluster analysis		In many data mining applications, cluster analysis is widely used and its results are expected to be interpretable, comprehensible, and usable. Rough set theory is one of the techniques to induce decision rules and manage inconsistent and incomplete information. This paper proposes a method to construct equivalence classes during the clustering process, isolate outlier points and finally deduce a rough set model from the clustering results. By the rough set model, attribute reduction and decision rule induction can be implemented efficiently and effectively. Experiments on real world data show that our method is useful and robust in handling data with noise.
7F71E650	Knowledge Discovery and Data Mining	david jensen + andrew fast + brian neil levine	2005	Creating social networks to improve peer-to-peer networking	 + overlay networks + social networks + Machine learning + Information systems applications + Computing methodologies + Data mining + Information systems	peer-to-peer networks + hierarchical dirichlet processes + social networks + distributed hash tables + overlay networks	We use knowledge discovery techniques to guide the creation of efficient overlay networks for peer-to-peer file sharing. An overlay network specifies the logical connections among peers in a network and is distinct from the physical connections of the network. It determines the order in which peers will be queried when a user is searching for a specific file. To better understand the role of the network overlay structure in the performance of peer-to-peer file sharing protocols, we compare several methods for creating overlay networks. We analyze the networks using data from a campus network for peer-to-peer file sharing that recorded anonymized data on 6,528 users sharing 291,925 music files over an 81-day period. We propose a novel protocol for overlay creation based on a model of user preference identified by latent-variable clustering with hierarchical Dirichlet processes (HDPs). Our simulations and empirical studies show that the clusters of songs created by HDPs effectively model user behavior and can be used to create desirable network overlays that outperform alternative approaches.
7AD9E69C	Knowledge Discovery and Data Mining	behrokh samadi + sean cox + douglas holt + daniel r jeske + ryan rich + theodore younglove + pengyue j lin + lan ye + rui xiao + minh ly	2005	Generation of synthetic data sets for evaluating the accuracy of knowledge discovery systems	Software defect analysis +  + Software creation and management + Software verification and validation + data mining + knowledge discovery + homeland security + Theory of computation + Semantic networks + Semantics and reasoning + synthetic data + Process validation + Program reasoning + false positive + Software and its engineering + Validation + Cross-computing tools and techniques + Empirical software validation + Computing methodologies + Knowledge representation and reasoning + Program verification + rule based + data generation + Software testing and debugging + error rate + Artificial intelligence + General and reference	+Categories and Subject Descriptors D.2.5 [Testing and Debugging+Testing Tools. D.2.4 [Software/Program Verification+Statistical Methods+Validation. I.2.4 [Knowledge Representations Formalisms and Methods+Semantic Graphs. General Terms Algorithms+Design+Verification Keywords Information Discovery+Data Mining+Data Generation	Information Discovery and Analysis Systems (IDAS) are designed to correlate multiple sources of data and use data mining techniques to identify potential significant events. Application domains for IDAS are numerous and include the emerging area of homeland security.Developing test cases for an IDAS requires background data sets into which hypothetical future scenarios can be overlaid. The IDAS can then be measured in terms of false positive and false negative error rates. Obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting a diverse set of data sources.In this paper, we give an overview of the design and architecture of an IDAS Data Set Generator (IDSG) that enables a fast and comprehensive test of an IDAS. The IDSG generates data using statistical and rule-based algorithms and also semantic graphs that represent interdependencies between attributes. A credit card transaction application is used to illustrate the approach.
815418F4	Knowledge Discovery and Data Mining	susumu horiguchi + xuanhieu phan + tubao ho + leminh nguyen	2005	Improving discriminative sequential learning with rare--but--important associations	conditional random field +  + association rule + information extraction + natural language processing + data mining + Machine learning + Computing methodologies + Information systems applications + feature selection + text segmentation + Information systems	Discriminative sequential learning + feature selection + information extraction + text segmentation + association rule	Discriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various non--independent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from an imbalance among classes/labels, irregular phenomena, and potential ambiguity in the training data. This paper presents a data--driven approach that can deal with such hard--to--predict data instances by discovering and emphasizing rare--but--important associations of statistics hidden in the training data. Mined associations are then incorporated into these models to deal with difficult examples. Experimental results of English phrase chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large dataset.
77FCD905	Knowledge Discovery and Data Mining	xiaodan song + mingting sun + chingyung lin + belle l tseng	2005	Modeling and predicting personal information dissemination behavior	social network +  + social capital + personal information management + natural language processing + content analysis + human behavior + Machine learning + Computing methodologies + machine learning + latent dirichlet allocation	user behavior modeling + personal information management + information dissemination	"In this paper, we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communications. A personal profile, called CommunityNet, is established for each individual based on a novel algorithm incorporating contact, content, and time information simultaneously. It can be used for personal social capital management. Clusters of CommunityNets provide a view of informal networks for organization management. Our new algorithm is developed based on the combination of dynamic algorithms in the social network field and the semantic content classification methods in the natural language processing and machine learning literatures. We tested CommunityNets on the Enron Email corpus and report experimental results including filtering, prediction, and recommendation capabilities. We show that the personal behavior and intention are somewhat predictable based on these models. For instance, ""to whom a person is going to send a specific email"" can be predicted by one's personal social network and content analysis. Experimental results show the prediction accuracy of the proposed adaptive algorithm is 58% better than the social network-based predictions, and is 75% better than an aggregated model based on Latent Dirichlet Allocation with social network enhancement. Two online demo systems we developed that allow interactive exploration of CommunityNet are also discussed."
5C552CD7	Knowledge Discovery and Data Mining	tamas horvath	2005	Cyclic pattern kernels revisited	data analysis + knowledge discovery + cardinal number + polynomial time + cycle graph + treewidth + data mining		The cyclic pattern kernel (CPK) is a powerful graph kernel based on patterns formed by simple cycles of labeled graphs. In a recent work, we proposed a method for computing CPK which is restricted to graphs containing polynomial number of simple cycles. In this work, we present two approaches relaxing this limitation. We first show that for graphs of bounded treewidth, CPK can be computed in time polynomial in the number of cyclic patterns, which in turn can be exponentially smaller than that of simple cycles. We then propose an alternative CPK based on the set of relevant cycles which is known to be enumerable with polynomial delay and its cardinality is typically only cubic in the number of vertices. Empirical results on the NCI-HIV dataset indicate that there is no significant difference in predictive performance between CPK based on simple cycles and that based on relevant cycles.
5940B196	Knowledge Discovery and Data Mining	ben kao + k k loo + ivy tong	2005	Online algorithms for mining inter-stream associations from large sensor networks	online algorithm + sensor network		We study the problem of mining frequent value sets from a large sensor network. We discuss how sensor stream data could be represented that facilitates efficient online mining and propose the interval-list representation. Based on Lossy Counting, we propose ILB, an interval-list-based online mining algorithm for discovering frequent sensor value sets. Through extensive experiments, we compare the performance of ILB against an application of Lossy Counting (LC) using a weighted transformation method. Results show that ILB outperforms LC significantly for large sensor networks.
NE8402	Knowledge Discovery and Data Mining	Chunsheng Yang+Sylvain LÃ©tourneau	2005	Learning to predict train wheel failures.	 + Decision support systems + Operations research + Learning paradigms + Information systems applications + Computing methodologies + Data mining + Decision analysis + Information systems + Applied computing + Supervised learning + Classification and regression trees + Machine learning + Machine learning approaches + Supervised learning by classification	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applications â data mining+I.2.6 [Artificial Intelligence+Learning- concept Learning+H.4.2 [Information Systems Applications+Types of Systems-- Decision support+I.5.2 [Pattern Recognition+Design Methodology-Classifier design and evaluation General Terms Algorithms+Performance+Reliability+Experimentation Keywords+Data Mining+Machine Learning+Methodology+Model Building+Model Evaluation+Model Fusion+Wheel Failure Prediction	This paper describes a successful but challenging application of data mining in the railway industry. The objective is to optimize maintenance and operation of trains through prognostics of wheel failures. In addition to reducing maintenance costs, the proposed technology will help improve railway safety and augment throughput. Building on established techniques from data mining and machine learning, we present a methodology to learn models to predict train wheel failures from readily available operational and maintenance data. This methodology addresses various data mining tasks such as automatic labeling, feature extraction, model building, model fusion, and evaluation. After a detailed description of the methodology, we report results from large-scale experiments. These results clearly show the great potential of this innovative application of data mining in the railway industry.
NE8412	Knowledge Discovery and Data Mining	Cinda Heeren+Leonard Pitt	2005	Maximal boasting.	 + Information systems applications + Data mining + Information systems	+Categories and Subject Descriptors H.2.8 [Database Applications+Data mining General Terms Algorithms+Theory	"We introduce the boasting problem, wherein useful trends in historical ordinal data (rankings) are discovered. Claims of the form ""our object was ranked r or better in x of the last t time units,"" are formalized, and maximal claims (boasts) of this form are defined under two natural partial orders. For the first partial order, we give an efficient and optimal algorithm for finding all such maximal claims. For the second, we apply a classical result from computational geometry to achieve an algorithm whose running time is significantly more efficient than that of a naÃ¯ve one. Finally, we connect this boasting problem to a novel variation of the problem of finding optimized confidence association rules as originally posed by Fukuda, et al. [2], and give an efficient algorithm for solving a simplification of the new problem."
NE8425	Knowledge Discovery and Data Mining	Fabian MÃ¶rchen+Alfred Ultsch	2005	Optimizing time series discretization for knowledge discovery.	 + Machine learning + Computing methodologies	+Categories and Subject Descriptors+I.5 [Computing Methodologies+Pattern Recognition General Terms+Algorithms Keywords+time series+discretization+persistence	Knowledge Discovery in time series usually requires symbolic time series. Many discretization methods that convert numeric time series to symbolic time series ignore the temporal order of values. This often leads to symbols that do not correspond to states of the process generating the time series and cannot be interpreted meaningfully. We propose a new method for meaningful unsupervised discretization of numeric time series called Persist. The algorithm is based on the Kullback-Leibler divergence between the marginal and the self-transition probability distributions of the discretization symbols. Its performance is evaluated on both artificial and real life data in comparison to the most common discretization methods. Persist achieves significantly higher accuracy than existing static methods and is robust against noise. It also outperforms Hidden Markov Models for all but very simple cases.
NE8422	Knowledge Discovery and Data Mining	Daniel Lowd+Christopher Meek	2005	Adversarial learning.	 + Theory of computation + Machine learning + Computing methodologies + Design and analysis of algorithms	+Categories and Subject Descriptors I.2.6 [Artificial Intelligence+LearningâConcept learn- ing+F.2 [Analysis of Algorithms and Problem Com- plexity+Miscellaneous General Terms Algorithms+Theory Keywords Adversarial classification+linear classifiers+spam	Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.
NE8436	Knowledge Discovery and Data Mining	Osmar R. ZaÃ¯ane+Mohammad El-Hajj	2005	Pattern lattice traversal by selective jumps.	 + Game tree search + Heuristic function construction + Information systems applications + Computing methodologies + Data mining + Search methodologies + Information systems + Theory of computation + Machine learning + Design and analysis of algorithms + Discrete space search + Artificial intelligence	+search strategies+Heuristic methods General Terms	Regardless of the frequent patterns to discover, either the full frequent patterns or the condensed ones, either closed or maximal, the strategy always includes the traversal of the lattice of candidate patterns. We study the existing depth versus breadth traversal approaches for generating candidate patterns and propose in this paper a new traversal approach that jumps in the search space among only promising nodes. Our leaping approach avoids nodes that would not participate in the answer set and reduce drastically the number of candidate patterns. We use this approach to efficiently pinpoint maximal patterns at the border of the frequent patterns in the lattice and collect enough information in the process to generate all subsequent patterns.
7A52AC30	Knowledge Discovery and Data Mining	shlomo zilberstein + andrew arnt	2005	Learning policies for sequential time and cost sensitive classification	state space + discrete time + data mining + search cost		In time and cost sensitive classification, the utility of labeling an instance depends not only on the correctness of the labeling, but also the amount of time taken to label the instance. Instance attributes are initially unknown, and may take significant time to measure. This results in a difficult problem, trying to manage the tradeoff between time and accuracy. The problem is further complicated when we consider a sequence of time-sensitive classification instances, where time spent measuring attributes in one instance can adversely affect the costs of future instances. We solve these problems using a decision theoretic approach. The problem is modeled as an MDP with a potentially very large state space. We discuss how to intelligently discretize time and approximate the effects of measurement actions in the current instance given all waiting instances. The results offer an effective approach to attribute measurement and classification for a variety of time sensitive applications. Copyright 2005 ACM.
NE8346	Knowledge Discovery and Data Mining	Prabhakar Raghavan	2005	Incentive networks.		No keyword found	We propose a notion of incentive networks, modeling online settings in which multiple participants in a network help each other find information. Within this general setting, we study query incentive networks, a natural abstraction of question-answering systems with rewards for finding answers. We analyze strategic behavior in such networks and under a simple model of networks, show that the Nash equilibrium for participants' strategies exhibits an unexpected threshold phenomenon.
7E1487F0	Knowledge Discovery and Data Mining	alok choudhary + ying liu + weikeng liao	2005	A fast high utility itemsets mining algorithm	 + property + search space + association rule mining	eol>utility mining + association rules mining + downward closure property + transaction-weighted utilization	"
Association rule mining (ARM) identifies frequent itemsets from databases and generates association rules by considering each item in equal value. However, items are actually different in many aspects in a number of real applications, such as retail marketing, network log, etc. The difference between items makes a strong impact on the decision making in these applications. Therefore, traditional ARM cannot meet the demands arising from these applications. By considering the different values of individual items as utilities, utility mining focuses on identifying the itemsets with high utilities. As âdownward closure propertyâ doesn't apply to utility mining, the generation of candidate itemsets is the most costly in terms of time and memory space. In this paper, we present a Two-Phase algorithm to efficiently prune down the number of candidates and can precisely obtain the complete set of high utility itemsets. In the first phase, we propose a model that applies the âtransaction-weighted downward closure propertyâ on the search space to expedite the identification of candidates. In the second phase, one extra database scan is performed to identify the high utility itemsets. We also parallelize our algorithm on shared memory multi-process architecture using Common Count Partitioned Database (CCPD) strategy. We verify our algorithm by applying it to both synthetic and real databases. It performs very efficiently in terms of speed and memory cost, and shows good scalability on multiple processors, even on large databases that are difficult for existing algorithms to handle.
"
NE8382	Knowledge Discovery and Data Mining	Xifeng Yan+X. Jasmine Zhou+Jiawei Han	2005	Mining closed relational graphs with connectivity constraints.	 + Information systems applications + Data mining + Information systems	+Categories and Subject Descriptors+H.2.8 [Database Management+Database Applications- Data Mining General Terms+Algorithms Keywords+graph+closed pattern+connectivity	Relational graphs are widely used in modeling large scale networks such as biological networks and social networks. In this kind of graph, connectivity becomes critical in identifying highly associated groups and clusters. In this paper, we investigate the issues of mining closed frequent graphs with connectivity constraints in massive relational graphs where each graph has around 10K nodes and 1M edges. We adopt the concept of edge connectivity and apply the results from graph theory, to speed up the mining process. Two approaches are developed to handle different mining requests: CloseCut, a pattern-growth approach, and splat, a pattern-reduction approach. We have applied these methods in biological datasets and found the discovered patterns interesting.
NE8396	Knowledge Discovery and Data Mining	G. Niklas NorÃ©n+Roland Orre+Andrew Bate	2005	A hit-miss model for duplicate detection in the WHO drug safety database.	 + Statistical graphics + Life and medical sciences + Information systems applications + Record storage systems + Record layout alternatives + Data mining + Information systems + Statistical paradigms + Applied computing + Information storage systems + Consumer health + Database administration + Data management systems + Probability and statistics + Mathematics of computing + Health informatics	No keyword found	The WHO Collaborating Centre for International Drug Monitoring in Uppsala, Sweden, maintains and analyses the world's largest database of reports on suspected adverse drug reaction incidents that occur after drugs are introduced on the market. As in other post-marketing drug safety data sets, the presence of duplicate records is an important data quality problem and the detection of duplicates in the WHO drug safety database remains a formidable challenge, especially since the reports are anonymised before submitted to the database. However, to our knowledge no work has been published on methods for duplicate detection in post-marketing drug safety data. In this paper, we propose a method for probabilistic duplicate detection based on the hit-miss model for statistical record linkage described by Copas & Hilton. We present two new generalisations of the standard hit-miss model: a hit-miss mixture model for errors in numerical record fields and a new method to handle correlated record fields. We demonstrate the effectiveness of the hit-miss model for duplicate detection in the WHO drug safety database both at identifying the most likely duplicate for a given record (94.7% accuracy) and at discriminating duplicates from random matches (63% recall with 71% precision). The proposed method allows for more efficient data cleaning in post-marketing drug safety data sets, and perhaps other applications throughout the KDD community.
NE8395	Knowledge Discovery and Data Mining	Jennifer Neville+ÃzgÃ¼r ÅimÅek+David Jensen+John Komoroske+Kelly Palmer+Henry Goldberg	2005	Using relational knowledge discovery to prevent securities fraud.	 + Machine learning + Information systems applications + Computing methodologies + Data mining + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data Mining+I.2.6 [Artificial Intelligence+Learning General Terms Algorithms+Design+Experimentation. Keywords Fraud detection+statistical relational learning+relational probability	We describe an application of relational knowledge discovery to a key regulatory mission of the National Association of Securities Dealers (NASD). NASD is the world's largest private-sector securities regulator, with responsibility for preventing and discovering misconduct among securities brokers. Our goal was to help focus NASD's limited regulatory resources on the brokers who are most likely to engage in securities violations. Using statistical relational learning algorithms, we developed models that rank brokers with respect to the probability that they would commit a serious violation of securities regulations in the near future. Our models incorporate organizational relationships among brokers (e.g., past coworker), which domain experts consider important but have not been easily used before now. The learned models were subjected to an extensive evaluation using more than 18 months of data unseen by the model developers and comprising over two person weeks of effort by NASD staff. Model predictions were found to correlate highly with the subjective evaluations of experienced NASD examiners. Furthermore, in all performance measures, our models performed as well as or better than the handcrafted rules that are currently in use at NASD.
78DE138B	Knowledge Discovery and Data Mining	paul r cohen + clayton t morrison	2005	Noisy information value in utility-based decision making	decision theory + intelligence analysis + information value + value of information + noise + uncertainty	Value of information + Uncertainty + Noise + Economic Utility + Decision Theory + Intelligence Analysis	How much is information worth? In the context of decisions, the value of information is the expected increase in utility of the decision as a result of having the information. When the information might be noisy, the model is slightly more complicated. We develop a model of the value of noisy information in the context of a plausible intelligence information gathering and decision making scenario. Copyright 2005 ACM.
815A47F4	Knowledge Discovery and Data Mining	lawrence b holder + nikhil s ketkar + diane j cook	2005	Qualitative comparison of graph-based and logic-based multi-relational data mining: a case study	information retrieval + relational database + aggregation + data mining		The goal of this paper is to generate insights about the differences between graph-based and logic-based approaches to multi-relational data mining by performing a case study of graph-based system, Subdue and the inductive logic programming system, CProgol. We identify three key factors for comparing graph-based and logic-based multi-relational data mining; namely, the ability to discover structurally large concepts, the ability to discover semantically complicated concepts and the ability to effectively utilize background knowledge. We perform an experimental comparison of Subdue and CProgol on the Mutagenesis domain and various artificially generated Bongard problems. Experimental results indicate that Subdue can significantly outperform CProgol while discovering structurally large multi-relational concepts. It is also observed that CProgol is better at learning semantically complicated concepts and it tends to use background knowledge more effectively than Subdue.
NE8378	Knowledge Discovery and Data Mining	Antti Ukkonen+Mikael Fortelius+Heikki Mannila	2005	Finding partial orders from unordered 0-1 data.	 + Information systems applications + Data mining + Information systems	+hidden ordering+consecutive ones property	In applications such as paleontology and medical genetics the 0-1 data has an underlying unknown order (the ages of the fossil sites, the locations of markers in the genome). The order might be total or partial: for example, two sites in different parts of the globe might be ecologically incomparable, or the ordering of certain markers might be different in different subgroups of the data. We consider the following problem. Given a table over a set of 0-1 variables, find a partial order for the rows minimizing a score function and being as specific as possible. The score function can be, e.g., the number of changes from 1 to 0 in a column (for paleontology) or the likelihood of the marker sequence (for genomic data). Our solution for this task first constructs small totally ordered fragments of the partial order, then finds good orientations for the fragments, and finally uses a simple and efficient heuristic method for finding a partial order that corresponds well with the collection of fragments. We describe the method, discuss its properties, and give empirical results on paleontological data demonstrating the usefulness of the method. In the application the use of the method highlighted some previously unknown properties of the data and pointed out probable errors in the data.
760C2271	Knowledge Discovery and Data Mining	mohandsaid hacid + jianping fan + hangzai luo	2005	Mining images on semantics via statistical learning	 + em algorithm + image classification + model selection + Multimedia databases + Information systems applications + parameter estimation + mixture model + Multimedia information systems + Information systems	Image classification + hierarchical mixture model + adaptive EM algorithm	In this paper, we have proposed a novel framework to enable hierarchical image classification via statistical learning. By integrating the concept hierarchy for semantic image concept organization, a hierarchical mixture model is proposed to enable multi-level modeling of semantic image concepts and hierarchical classifier combination. Thus, learning the classifiers for the semantic image concepts at the high level of the concept hierarchy can be effectively achieved by detecting the presences of the relevant base-level atomic image concepts. To effectively learn the base-level classifiers for the atomic image concepts at the first level of the concept hierarchy, we have proposed a novel adaptive EM algorithm to achieve more effective model selection and parameter estimation. In addition, a novel penalty term is proposed to effectively eliminate the misleading effects of the outlying unlabeled images on semi-supervised classifier training. Our experimental results in a specific image domain of outdoor photos are very attractive.
77562D43	Knowledge Discovery and Data Mining	omid madani + dennis decoste	2005	Contextual recommender problems [extended abstract]	multi armed bandit problem + web pages + regression + utility + personalization + data mining + multi armed bandit + reinforcement learning		The contextual recommender task is the problem of making useful offers, e.g., placing ads or related links on a web page, based on the context information, e.g., contents of the page and information about the user visiting, and information on the available alternatives, i.e., the advertisements or relevant links. In the case of ads for example, the goal is to select ads that result in high click rates, where the (ad) click rate is some unknown function of the attributes of the context and ad. We describe the task and make connections to related problems including recommender and multi-armed bandit problems. Copyright 2005 ACM.
NE8360	Knowledge Discovery and Data Mining	Aleks Jakulin+Martin MoÅ¾ina+Janez DemÅ¡ar+Ivan Bratko+BlaÅ¾ Zupan	2005	Nomograms for visualizing support vector machines.	 + Interaction design theory, concepts and paradigms + HCI theory, concepts and models + Multivariate statistics + Interaction design + Human-centered computing + Probability and statistics + Mathematics of computing + Human computer interaction (HCI)	+Theory+Human Factors Keywords nomogram+visualization+support vector machines+machine	We propose a simple yet potentially very effective way of visualizing trained support vector machines. Nomograms are an established model visualization technique that can graphically encode the complete model on a single page. The dimensionality of the visualization does not depend on the number of attributes, but merely on the properties of the kernel. To represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms, we employ logistic regression to convert the distance from the separating hyperplane into a probability. Case studies on selected data sets show that for a technique thought to be a black-box, nomograms can clearly expose its internal structure. By providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predictive factors.
799F0C4B	Knowledge Discovery and Data Mining	chengxiang zhai + qiaozhu mei	2005	Discovering evolutionary theme patterns from text: an exploration of temporal text mining	 + life cycle + Retrieval tasks and goals + Information systems applications + Clustering and classification + Information retrieval + Clustering + Data mining + Information systems + text mining + clustering + probabilistic method	Temporal text mining + evolutionary theme pat-	Temporal Text Mining (TTM) is concerned with discovering temporal patterns in text information collected over time. Since most text information bears some time stamps, TTM has many applications in multiple domains, such as summarizing events in news articles and revealing research trends in scientific literature. In this paper, we study a particular TTM task -- discovering and summarizing the evolutionary patterns of themes in a text stream. We define this new text mining problem and present general probabilistic methods for solving this problem through (1) discovering latent themes from text; (2) constructing an evolution graph of themes; and (3) analyzing life cycles of themes. Evaluation of the proposed methods on two different domains (i.e., news articles and literature) shows that the proposed methods can discover interesting evolutionary theme patterns effectively.
8123F43F	Knowledge Discovery and Data Mining	shengquan wang + nan zhang + wei zhao	2005	A new scheme on privacy-preserving data classification	 + private information + distributed system + Information systems applications + privacy + Data mining + Information systems + Theory and algorithms for application domains + Theory of computation + Database and storage security + Security and privacy + Theory of database privacy and security + Database theory + middleware	Privacy + Privacy-preserving data mining	We address privacy-preserving classification problem in a distributed system. Randomization has been the approach proposed to preserve privacy in such scenario. However, this approach is now proven to be insecure as it has been discovered that some privacy intrusion techniques can be used to reconstruct private information from the randomized data tuples. We introduce an algebraic-technique-based scheme. Compared to the randomization approach, our new scheme can build classifiers more accurately but disclose less private information. Furthermore, our new scheme can be readily integrated as a middleware with existing systems.
6AA1F209	Knowledge Discovery and Data Mining	nan zhang + wei zhao + jianer chen	2005	Performance measurements for privacy preserving data mining	data mining + sample size		This paper establishes the foundation for the performance measurements of privacy preserving data mining techniques. The performance is measured in terms of the accuracy of data mining results and the privacy protection of sensitive data. On the accuracy side, we address the problem of previous measures and propose a new measure, named âeffective sample sizeâ, to solve this problem. We show that our new measure can be bounded without any knowledge of the data being mined, and discuss when the bound can be met. On the privacy protection side, we identify a tacit assumption made by previous measures and show that the assumption is unrealistic in many situations. To solve the problem, we introduce a game theoretic framework for the measurement of privacy.
7A8B549C	Knowledge Discovery and Data Mining	carlos rojas + cesar cardona + olfa nasraoui	2005	Using retrieval measures to assess similarity in mining dynamic web clickstreams	 + web usage mining + personalization + web mining + data mining + Information systems applications + clustering + Data mining + artificial immune systems + artificial immune system + Information systems	artificial immune systems + web mining + personalization + clustering + stream data mining + mining evolving data	While scalable data mining methods are expected to cope with massive Web data, coping with evolving trends in noisy data in a continuous fashion, and without any unnecessary stoppages and reconfigurations is still an open challenge. This dynamic and single pass setting can be cast within the framework of mining evolving data streams. In this paper, we explore the task of mining mass user profiles by discovering evolving Web session clusters in a single pass with a recently proposed scalable immune based clustering approach (TECNO-STREAMS), and study the effect of the choice of different similarity measures on the mining process and on the interpretation of the mined patterns. We propose a simple similarity measure that has the advantage of explicitly coupling the precision and coverage criteria to the early learning stages, and furthermore requiring that the affinity of the data to the learned profiles or summaries be defined by the minimum of their coverage or precision, hence requiring that the learned profiles are simultaneously precise and complete, with no compromises.In our experiments, we study the task of mining evolving user profiles from Web clickstream data (web usage mining) in a single pass, and under different trend sequencing scenarios, showing that compared oto the cosine similarity measure, the proposed similarity measure explicitly based on precision and coverage allows the discovery of more correct profiles at the same precision or recall quality levels.
7695E333	Knowledge Discovery and Data Mining	yanzan zhou + xin jin + bamshad mobasher	2005	A maximum entropy web recommendation system: combining collaborative and content features	 + web usage mining + maximum entropy principle + recommender system + Machine learning + maximum entropy + Information systems applications + Computing methodologies + Data mining + user model + latent dirichlet allocation + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Applicationsâ Data Mining+I.2.6 [Artificial Intelligence+Learning+I.5.1 [Pattern Recognition+ModelsâStatistical General Terms Algorithms Keywords Maximum Entropy+Recommendation+Web Usage Mining+User Profiling	Web users display their preferences implicitly by navigating through a sequence of pages or by providing numeric ratings to some items. Web usage mining techniques are used to extract useful knowledge about user interests from such data. The discovered user models are then used for a variety of applications such as personalized recommendations. Web site content or semantic features of objects provide another source of knowledge for deciphering users' needs or interests. We propose a novel Web recommendation system in which collaborative features such as navigation or rating data as well as the content features accessed by the users are seamlessly integrated under the maximum entropy principle. Both the discovered user patterns and the semantic relationships among Web objects are represented as sets of constraints that are integrated to fit the model. In the case of content features, we use a new approach based on Latent Dirichlet Allocation (LDA) to discover the hidden semantic relationships among items and derive constraints used in the model. Experiments on real Web site usage data sets show that this approach can achieve better recommendation accuracy, when compared to systems using only usage information. The integration of semantic information also allows for better interpretation of the generated recommendations.
7FE3BEF1	Knowledge Discovery and Data Mining	bart goethals + eveline hoekx + jan van den bussche	2005	Mining tree queries in a graph	 + canonical form + Information systems applications + Data mining + graph + sql + Information systems + conjunctive queries + food web + conjunctive query + citation analysis + equivalence checking	Canonical form + conjunctive query + equivalence checking + graph + levelwise + redundancy checking + SQL + tree query	We present an algorithm for mining tree-shaped patterns in a large graph. Novel about our class of patterns is that they can contain constants, and can contain existential nodes which are not counted when determining the number of occurrences of the pattern in the graph. Our algorithm has a number of provable optimality properties, which are based on the theory of conjunctive database queries. We propose a database-oriented implementation in SQL, and report upon some initial experimental results obtained with our implementation on graph data about food webs, about protein interactions, and about citation analysis.
7A836BED	Knowledge Discovery and Data Mining	fabian morchen + alfred ultsch	2005	Optimizing time series discretization for knowledge discovery	kullback leibler divergence + time series + knowledge discovery + transition probability + persistence + discretization + hidden markov model	time series + discretization + persistence	"
Knowledge Discovery in time series usually requires symbolic time series. Many discretization methods that convert numeric time series to symbolic time series ignore the temporal order of values. This often leads to symbols that do not correspond to states of the process generating the time series and cannot be interpreted meaningfully. We propose a new method for meaningful unsupervised discretization of numeric time series called Persist. The algorithm is based on the Kullback-Leibler divergence between the marginal and the self-transition probability distributions of the discretization symbols. Its performance is evaluated on both arti cial and real life data in comparison to the most common discretization methods. Persist achieves signi cantly higher accuracy than existing static methods and is robust against noise. It also outperforms Hidden Markov Models for all but very simple cases.
"
784545F8	Knowledge Discovery and Data Mining	kenny daniel + andrew w moore + maheshkumar sabhnani + daniel b neill	2005	Detection of emerging space-time clusters	time series analysis +  + Information systems applications + space time + Data mining + false positive + real time + Information systems	+Categories and Subject Descriptors H.2.8 [Database Management+Database Apps- Data Mining General Terms Algorithms Keywords Cluster detection+space-time scan statistics+biosurveillance	"We propose a new class of spatio-temporal cluster detection methods designed for the rapid detection of emerging space-time clusters. We focus on the motivating application of prospective disease surveillance: detecting space-time clusters of disease cases resulting from an emerging disease outbreak. Automatic, real-time detection of outbreaks can enable rapid epidemiological response, potentially reducing rates of morbidity and mortality. Building on the prior work on spatial and space-time scan statistics, our methods combine time series analysis (to determine how many cases we expect to observe for a given spatial region in a given time interval) with new ""emerging cluster"" space-time scan statistics (to decide whether an observed increase in cases in a region is significant), enabling fast and accurate detection of emerging outbreaks. We evaluate these methods on two types of simulated outbreaks: aerosol release of inhalational anthrax (e.g. from a bioterrorist attack) and FLOO (""Fictional Linear Onset Outbreak""), injected into actual baseline data (Emergency Department records and over-the-counter drug sales data from Allegheny County). We demonstrate that our methods are successful in rapidly detecting both outbreak types while keeping the number of false positives low, and show that our new ""emerging cluster"" scan statistics consistently outperform the standard ""persistent cluster"" scan statistics approach."
80D8713A	Knowledge Discovery and Data Mining	blaz zupan + ivan bratko + gregor leban + minca mramor	2005	Simple and effective visual models for gene expression cancer diagnostics	 + gene expression analysis + Life and medical sciences + data mining + machine learning + neural network + Applied computing + computer and information science + support vector machine + Systems biology + Genetics + data visualization + gene expression + Computational biology	+Cancer Diagnosis+Machine Learn- ing+Data Mining+Data Visualization	In the paper we show that diagnostic classes in cancer gene expression data sets, which most often include thousands of features (genes), may be effectively separated with simple two-dimensional plots such as scatterplot and radviz graph. The principal innovation proposed in the paper is a method called VizRank, which is able to score and identify the best among possibly millions of candidate projections for visualizations. Compared to recently much applied techniques in the field of cancer genomics that include neural networks, support vector machines and various ensemble-based approaches, VizRank is fast and finds visualization models that can be easily examined and interpreted by domain experts. Our experiments on a number of gene expression data sets show that VizRank was always able to find data visualizations with a small number of (two to seven) genes and excellent class separation. In addition to providing grounds for gene expression cancer diagnosis, VizRank and its visualizations also identify small sets of relevant genes, uncover interesting gene interactions and point to outliers and potential misclassifications in cancer data sets.
7B9B0F5E	Sigkdd Explorations	shenzhi li + tianhao wu + william m pottenger	2005	Distributed higher order association rule mining using information extracted from textual data	distributed database + artificial intelligent +  + higher order + data mining + knowledge discovery + distributed environment + machine learning + distributive law + artificial intelligence + association rule mining + evaluation + information extraction + rule based + terrorism + text mining + criminal justice	Distributed data mining + distributed association rule mining + knowledge discovery + artificial intelligence + machine learning + data mining + association rule mining + text mining + evaluation + privacy-preserving + terrorism + law enforcement + criminal justice	"
The burgeoning amount of textual data in distributed sources combined with the obstacles involved in creating and maintaining central repositories motivates the need for effective distributed information extraction and mining techniques. Recently, as the need to mine patterns across distributed databases has grown, Distributed Association Rule Mining (D-ARM) algorithms have been developed. These algorithms, however, assume that the databases are either horizontally or vertically distributed. In the special case of databases populated from information extracted from textual data, existing D-ARM algorithms cannot discover rules based on higher-order associations between items in distributed textual documents that are neither vertically nor horizontally distributed, but rather a hybrid of the two. In this article we present D-HOTM, a framework for Distributed Higher Order Text Mining. D-HOTM is a hybrid approach that combines information extraction and distributed data mining. We employ a novel information extraction technique to extract meaningful entities from unstructured text in a distributed environment. The information extracted is stored in local databases and a mapping function is applied to identify globally unique keys. Based on the extracted information, a novel distributed association rule mining algorithm is applied to discover higher-order associations between items (i.e., entities) in records fragmented across the distributed databases using the keys. Unlike existing algorithms, D-HOTM requires neither knowledge of a global schema nor that the distribution of data be horizontal or vertical. Evaluation methods are proposed to incorporate the performance of the mapping function into the traditional support metric used in ARM evaluation. An example application of the algorithm on distributed law enforcement data demonstrates the relevance of DHOTM in the fight against terrorism.
"
79632E84	Sigkdd Explorations	fred popowich	2005	Using text mining and natural language processing for health care claims processing	categorization + health care + information retrieval + text mining + pattern matching + specification language + natural language processing + rule based + information extraction	Information extraction + information retrieval + categorization + pattern matching + other party liability indicators	"
A health care claims processing application is introduced which processes both structured and unstructured information associated with medical insurance claims. The application makes use of a natural language processing (NLP) engine, together with application-specific knowledge, written in a concept specification language. Using NLP techniques, the entities and relationships that act as indicators of recoverable claims are mined from management notes, call centre logs and patient records to identify medical claims that require further investigation. Text mining techniques can then be applied to find dependencies between different entities, and to combine indicators to provide scores to individual claims. Claims are scored to determine whether they involve potential fraud or abuse, or to determine whether claims should be paid by or in conjunction with other insurers or organizations. Dependencies between claims and other records can then be combined to create cases. Issues related to the design of the application are discussed, specifically the use of rule-based techniques which provide a capability for deeper analysis than traditionally found in statistical techniques.
"
79F7F6CC	Knowledge Discovery and Data Mining	philip s yu + haixun wang + jian pei	2005	Pattern-based similarity search for microarray data	 + distance function + distance metric + dna microarray + similarity search + microarray data + Information systems applications + Computing methodologies + Data mining + lp norm + Information systems + Machine learning + pattern recognition	pattern recognition + near neighbor + distance function	One fundamental task in near-neighbor search as well as other similarity matching efforts is to find a distance function that can efficiently quantify the similarity between two objects in a meaningful way. In DNA microarray analysis, the expression levels of two closely related genes may rise and fall synchronously in response to a set of experimental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very similar. Unfortunately, none of the conventional distance metrics such as the Lp norm can model this similarity effectively. In this paper, we study the near-neighbor search problem based on this new type of similarity. We propose to measure the distance between two genes by subspace pattern similarity, i.e., whether they exhibit a synchronous pattern of rise and fall on a subset of dimensions. We then present an efficient algorithm for subspace near-neighbor search based on pattern similarity distance, and we perform tests on various data sets to show its effectiveness.
7A8D4EBB	Knowledge Discovery and Data Mining	aleksandar lazarevic + vipin kumar	2005	Feature bagging for outlier detection	 + Visualization + bagging + false + Information systems applications + Human-centered computing + Data mining + outlier detection + Visualization application domains + Information systems + difference set + Geographic visualization + performance + Spatial-temporal systems + design + Scientific visualization	eol>Outlier detection + bagging + feature subsets + integration + detection rate + false alarm	Outlier detection has recently become an important problem in many industrial and financial applications. In this paper, a novel feature bagging approach for detecting outliers in very large, high dimensional and noisy databases is proposed. It combines results from multiple outlier detection algorithms that are applied using different set of features. Every outlier detection algorithm uses a small subset of features that are randomly selected from the original feature set. As a result, each outlier detector identifies different outliers, and thus assigns to all data records outlier scores that correspond to their probability of being outliers. The outlier scores computed by the individual outlier detection algorithms are then combined in order to find the better quality outliers. Experiments performed on several synthetic and real life data sets show that the proposed methods for combining outputs from multiple outlier detection algorithms provide non-trivial improvements over the base algorithm.
7FB9B6E3	Knowledge Discovery and Data Mining	shoujian yu + jiajin le + jianwei liu + michai vlachos + jessica lin + dimitrios gunopulos + eamonn keogh	2005	A MPAA-Based iterative clustering algorithm augmented by nearest neighbors search for time-series data streams	 + nearest neighbor search + nearest neighbor + time series + time series data		"
In streaming time series the Clustering problem is more complex, since the dynamic nature of streaming data makes previous clustering methods inappropriate. In this paper, we propose firstly a new method to evaluate Clustering in streaming time series databases. First, we introduce a novel multiresolution PAA (MPAA) transform to achieve our iterative clustering algorithm. The method is based on the use of a multi-resolution piecewise aggregate approximation representation, which is used to extract features of time series. Then, we propose our iterative clustering approach for streaming time series. We take advantage of the multiresolution property of MPPA and equip a stopping criteria based on Hoeffding bound in order to achieve fast response time. Our streaming time-series clustering algorithm also works by leveraging off the nearest neighbors of the incoming streaming time series datasets and fulfill incremental clustering approach. The comprehensive experiments based on several publicly available real data sets shows that significant performance improvement is achieved and produce high-quality clusters in comparison to the previous methods.
"
7D7A509C	Knowledge Discovery and Data Mining	jiuyong li + jie chen + ross sparks + hongxing he + damien mcaullay + huidong jin + graham williams + chris w kelman + ada waichee fu	2005	Mining risk patterns in medical data	 + Applied computing + Life and medical sciences + Information systems applications + Consumer health + Data mining + relative risk + Health informatics + Information systems	Relative risk + rule + optimal risk pattern set + medical application	In this paper, we discuss a problem of finding risk patterns in medical data. We define risk patterns by a statistical metric, relative risk, which has been widely used in epidemiological research. We characterise the problem of mining risk patterns as an optimal rule discovery problem. We study an anti-monotone property for mining optimal risk pattern sets and present an algorithm to make use of the property in risk pattern discovery. The method has been applied to a real world data set to find patterns associated with an allergic event for ACE inhibitors. The algorithm has generated some useful results for medical researchers.
7B7C2FED	Sigkdd Explorations	steve poteet + anne kao	2005	Text mining and natural language processing: introduction for the special issue	text mining + natural language processing + text analysis		"
This paper provides an introduction to this special issue of SIGKDD Explorations devoted to Natural Language Processing and Text Mining. NLP has been around for a number of decades. It has developed various techniques that are typically linguistically inspired, i.e. text is typically syntactically parsed using information from a formal grammar and a lexicon, the resulting information is then interpreted semantically and used to extract information about what was said. NLP may be deep (parsing every part of every sentence and attempting to account semantically for every part) or shallow (parsing only certain passages or phrases within sentences or producing only limited semantic analysis), and may even use statistical means to disambiguate word senses or multiple parses of the same sentence. It tends to focus on one document or piece of text at a time and be rather computationally expensive. It includes techniques like word stemming (removing suffixes) or a related technique, lemmatization (replacing an inflected word with its base form), multiword phrase grouping, synonym normalization, part-of-speech (POS) tagging (elaborations on noun, verb, preposition etc.), word-sense disambiguation, anaphora resolution (who does âheâ or âthe CEOâ refer to), and role determination (e.g. subject and object).
"
772934DC	Knowledge Discovery and Data Mining	jiji zhang + ricardo silva + james g shanahan	2005	Probabilistic workflow mining	 + business process + graphical model + graphical models + polynomial time + Probability and statistics + Mathematics of computing + machine learning + causal models	+graphical models+causal models	In several organizations, it has become increasingly popular to document and log the steps that makeup a typical business process. In some situations, a normative workflow model of such processes is developed, and it becomes important to know if such a model is actually being followed by analyzing the available activity logs. In other scenarios, no model is available and, with the purpose of evaluating cases or creating new production policies, one is interested in learning a workflow representation of such activities. In either case, machine learning tools that can mine workflow models are of great interest and still relatively unexplored. We present here a probabilistic workflow model and a corresponding learning algorithm that runs in polynomial time. We illustrate the algorithm on example data derived from a real world workflow.
7E200B81	Knowledge Discovery and Data Mining	jiawei han + hong cheng + xifeng yan + dong xin	2005	Summarizing itemset patterns: a profile-based approach	 + summarization + Information systems applications + empirical study + Data mining + probabilistic model + Information systems	frequent pattern + summarization + probabilistic model	Frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets, sequences, and graphs. However, the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability, due to the huge number of patterns generated by the mining process.In this paper, we examine how to summarize a collection of itemset patterns using only K representatives, a small number of patterns that a user can handle easily. The K representatives should not only cover most of the frequent patterns but also approximate their supports. A generative model is built to extract and profile these representatives, under which the supports of the patterns can be easily recovered without consulting the original dataset. Based on the restoration error, we propose a quality measure function to determine the optimal value of parameter K. Polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement. Empirical studies indicate that we can obtain compact summarization in real datasets.
7E3BCE3E	Knowledge Discovery and Data Mining	qiansheng cheng + bin gao + xin zheng + tieyan liu + weiying ma	2005	Consistent bipartite graph co-partitioning for star-structured high-order heterogeneous data co-clustering	Cluster analysis +  + co clustering + Learning paradigms + Machine learning + Computing methodologies + text mining + bipartite graph + consistency + Unsupervised learning	eol>Co-clustering + High-Order Heterogeneous Data + Consistency + Spectral Graph	Heterogeneous data co-clustering has attracted more and more attention in recent years due to its high impact on various applications. While the co-clustering algorithms for two types of heterogeneous data (denoted by pair-wise co-clustering), such as documents and terms, have been well studied in the literature, the work on more types of heterogeneous data (denoted by high-order co-clustering) is still very limited. As an attempt in this direction, in this paper, we worked on a specific case of high-order co-clustering in which there is a central type of objects that connects the other types so as to form a star structure of the inter-relationships. Actually, this case could be a very good abstract for many real-world applications, such as the co-clustering of categories, documents and terms in text mining. In our philosophy, we treated such kind of problems as the fusion of multiple pair-wise co-clustering sub-problems with the constraint of the star structure. Accordingly, we proposed the concept of consistent bipartite graph co-partitioning, and developed an algorithm based on semi-definite programming (SDP) for efficient computation of the clustering results. Experiments on toy problems and real data both verified the effectiveness of our proposed method.
7B9F5FEE	Knowledge Discovery and Data Mining	ravi kumar + andrew tomkins + anirban dasgupta + prabhakar raghavan	2005	Variable latent semantic indexing	svd +  + latent semantic indexing + low rank approximation + Symbolic and algebraic manipulation + Computing methodologies + Information retrieval + Linear algebra algorithms + approximation error + Information systems + Numerical analysis + Mathematical analysis + Computations on matrices + Symbolic and algebraic algorithms + vlsi + Mathematics of computing + linear algebra	eol>Linear algebra + SVD + Matrix approximation + LSI + VLSI	"Latent Semantic Indexing is a classical method to produce optimal low-rank approximations of a term-document matrix. However, in the context of a particular query distribution, the approximation thus produced need not be optimal. We propose VLSI, a new query-dependent (or ""variable"") low-rank approximation that minimizes approximation error for any specified query distribution. With this tool, it is possible to tailor the LSI technique to particular settings, often resulting in vastly improved approximations at much lower dimensionality. We validate this method via a series of experiments on classical corpora, showing that VLSI typically performs similarly to LSI with an order of magnitude fewer dimensions."
80746CC7	Knowledge Discovery and Data Mining	jiawei han + shengnan cong + david padua	2005	Parallel mining of closed sequential patterns	 + parallel algorithms + linux cluster + load balancing + data mining + parallel algorithm + sampling + Information systems applications + Language types + Parallel computing methodologies + Computing methodologies + Software notations and tools + Parallel programming languages + dynamic scheduling + Data mining + Information systems + distributed memory + General programming languages + sequential pattern mining + divide and conquer + load balance + Software and its engineering	parallel algorithms + load balancing + sampling	Discovery of sequential patterns is an essential data mining task with broad applications. Among several variations of sequential patterns, closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it. Unfortunately, there is no parallel closed sequential pattern mining method proposed yet. In this paper we develop an algorithm, called Par-CSP (Parallel Closed Sequential Pattern mining), to conduct parallel mining of closed sequential patterns on a distributed memory system. Par-CSP partitions the work among the processors by exploiting the divide-and-conquer property so that the overhead of interprocessor communication is minimized. Par-CSP applies dynamic scheduling to avoid processor idling. Moreover, it employs a technique, called selective sampling to address the load imbalance problem. We implement Par-CSP using MPI on a 64-node Linux cluster. Our experimental results show that Par-CSP attains good parallelization efficiencies on various input datasets.
8114A580	Knowledge Discovery and Data Mining	michelle ciraco + gary m weiss + michael rogalewski	2005	Improving classifier utility by altering the misclassification cost ratio	 + data mining + machine learning + false positive		"
This paper examines whether classifier utility can be improved by altering the misclassification cost ratio (the ratio of false positive misclassification costs to false negative misclassification costs) associated with two-class datasets. This is evaluated by varying the cost ratio passed into two cost-sensitive learners and then evaluating the results using the actual (or presumed actual) cost information. Our results indicate that a cost ratio other than the true ratio often maximizes classifier utility. Furthermore, by using a hold out set to identify the âbestâ cost ratio for learning, we are able to take advantage of this behavior and generate classifiers that outperform the accepted strategy of always using the actual cost information during the learning phase.
"
7EC63C4D	Knowledge Discovery and Data Mining	szymon jaroszewicz + tobias scheffer	2005	Fast discovery of unexpected patterns in data, relative to a Bayesian network	 + bayesian network + association rule + sampling + association rules + Information systems applications + very large database + Data mining + bayesian networks + Information systems	Bayesian Networks + Association Rules + Sampling	We consider a model in which background knowledge on a given domain of interest is available in terms of a Bayesian network, in addition to a large database. The mining problem is to discover unexpected patterns: our goal is to find the strongest discrepancies between network and database. This problem is intrinsically difficult because it requires inference in a Bayesian network and processing the entire, potentially very large, database. A sampling-based method that we introduce is efficient and yet provably finds the approximately most interesting unexpected patterns. We give a rigorous proof of the method's correctness. Experiments shed light on its efficiency and practicality for large-scale Bayesian networks and databases.
76AC2940	Knowledge Discovery and Data Mining	bianca zadrozny	2005	One-Benefit learning: cost-sensitive learning with restricted cost information	synthetic data + data mining	eol>data mining + cost-sensitive learning	"
This paper presents a new formulation for cost-sensitive learning that we call the One-Bene t formulation. Instead of having the correct label for each training example as in the standard classi er learning formulation, in this formulation we have one possible label for each example (which may not be the correct one) and the bene t (or cost) associated with that label. The goal of learning in this formulation is to nd the classi er that maximizes the expected benet of the labelling using only these examples. We present a reduction from One-Bene t learning to standard classi er learning that allows us to use any existing error-minimizing classi er learner to maximize the expected bene t in this formulation by correctly weighting the examples. We also show how to evaluate a classi er using test examples for which we only the bene t for one of the labels. We present preliminary experimental results using a synthetic data generator that allows us to test both our learning method and our evaluation method.
"
79317DF9	Knowledge Discovery and Data Mining	d sivakumar + ravi kumar + ravi sundaram + ratan k guha	2005	Unweaving a web of documents	 + algorithms + directed acyclic graph + Information retrieval + maximum matching + Information systems	News threads + Graph algorithms + Graph decomposition	We develop an algorithmic framework to decompose a collection of time-stamped text documents into semantically coherent threads. Our formulation leads to a graph decomposition problem on directed acyclic graphs, for which we obtain three algorithms --- an exact algorithm that is based on minimum cost flow and two more efficient algorithms based on maximum matching and dynamic programming that solve specific versions of the graph decomposition problem. Applications of our algorithms include superior summarization of news search results, improved browsing paradigms for large collections of text-intensive corpora, and integration of time-stamped documents from a variety of sources. Experimental results based on over 250,000 news articles from a major newspaper over a period of four years demonstrate that our algorithms efficiently identify robust threads of varying lengths and time-spans.
7B74ABCC	Knowledge Discovery and Data Mining	c c chen + meng chang chen	2005	LIPED: HMM-based life profiles for adaptive event detection	 + Retrieval tasks and goals + Time series analysis + Information systems applications + Clustering and classification + Information retrieval + Clustering + Data mining + Information systems + Statistical paradigms + hidden markov models + hidden markov model + clustering + Probability and statistics + Mathematics of computing	eol>Event Detection + Life Profiles + Hidden Markov Models + Clustering	In this paper, the proposed LIPED (LIfe Profile based Event Detection) employs the concept of life profiles to predict the activeness of event for effective event detection. A group of events with similar activeness patterns shares a life profile, modeled by a hidden Markov model. Considering the burst-and-diverse property of events, LIPED identifies the activeness status of event. As a result, LIPED balances the clustering precision and recall to achieve better F1 scores than other well known approaches evaluated on the official TDT1 corpus.
7FD68FFA	Knowledge Discovery and Data Mining	chase krumpelman + joydeep ghosh + raymond j mooney + sugato basu + arindam banerjee	2005	Model-based overlapping clustering	 + gaussian mixture model + graphical model + synthetic data + Machine learning + Information systems applications + Computing methodologies + Data mining + exponential family + Information systems	eol>Overlapping clustering + exponential model + Bregman divergences + high-dimensional clustering + graphical model	While the vast majority of clustering algorithms are partitional, many real world datasets have inherently overlapping clusters. Several approaches to finding overlapping clusters have come from work on analysis of biological datasets. In this paper, we interpret an overlapping clustering model proposed by Segal et al. [23] as a generalization of Gaussian mixture models, and we extend it to an overlapping clustering model based on mixtures of any regular exponential family distribution and the corresponding Bregman divergence. We provide the necessary algorithm modifications for this extension, and present results on synthetic data as well as subsets of 20-Newsgroups and EachMovie datasets.
7F499422	Knowledge Discovery and Data Mining	moshe koppel + jonathan schler + kfir zigdon	2005	Determining an author's native language by mining a text for errors	 + Machine learning theory + Learning paradigms + Computing methodologies + Learning settings + native language + Theory and algorithms for application domains + Theory of computation + Logical and relational learning + Inductive logic learning + support vector machine + Markov decision processes + Neural networks + Machine learning + Machine learning approaches + Instance-based learning + text mining + Natural language processing + Artificial intelligence	eol>Text mining + author profiling	In this paper, we show that stylistic text features can be exploited to determine an anonymous author's native language with high accuracy. Specifically, we first use automatic tools to ascertain frequencies of various stylistic idiosyncrasies in a text. These frequencies then serve as features for support vector machines that learn to classify texts according to author native language.
6BA9C96F	Knowledge Discovery and Data Mining	yinghan liu + sanyih hwang + jengkuen chiu + eepeng lim	2005	Mining mobile group patterns: a trajectory-based approach	mobile device + information system + 		"
In this paper, we present a group pattern mining approach to derive the grouping information of mobile device users based on a trajectory model. Group patterns of users are determined by distance threshold and minimum time duration. A trajectory model of user movement is adopted to save storage space and to cope with untracked or disconnected location data. To discover group patterns, we propose ATGP algorithm and TVG-growth that are derived from the Apriori and VG-growth algorithms respectively.
"
5C247D65	Knowledge Discovery and Data Mining	boi faltings + vincent schickelzuber	2005	Overcoming Incomplete User Models in Recommendation Systems Via an Ontology	prediction model + user model + recommender system		To make accurate recommendations, recommendation systems currently require more data about a customer than is usually available. We conjecture that the weaknesses are due to a lack of inductive bias in the learning methods used to build the prediction models. We propose a new method that extends the utility model and assumes that the structure of user preferences follows an ontology of product attributes. Using the data of the MovieLens system, we show experimentally that real user preferences indeed closely follow an ontology based on movie attributes. Furthermore, a recommender based just on a single individualâs preferences and this ontology performs better than collaborative filtering, with the greatest differences when little data about the user is available. This points the way to how proper inductive bias can be used for significantly more powerful recommender systems in the future.
6A565A3D	Knowledge Discovery and Data Mining	anthony bagnall + stefano lonardi + eamonn keogh + chotirat ann ratanamahatana	2005	A novel bit level time series representation with implication of similarity search and clustering	 + similarity search + data mining + time series + lower bound		"
Because time series are a ubiquitous and increasingly prevalent type of data, there has been much research effort devoted to time series data mining recently. As with all data mining problems, the key to effective and scalable algorithms is choosing the right representation of the data. Many high level representations of time series have been proposed for data mining. In this work, we introduce a new technique based on a bit level approximation of the data. The representation has several important advantages over existing techniques. One unique advantage is that it allows raw data to be directly compared to the reduced representation, while still guaranteeing lower bounds to Euclidean distance. This fact can be exploited to produce faster exact algorithms for similarly search. In addition, we demonstrate that our new representation allows time series clustering to scale to much larger datasets.
"
7E0678C3	Knowledge Discovery and Data Mining	ellen spertus + mehran sahami + orkut buyukkokten	2005	Evaluating similarity measures: a large-scale study in the orkut social network	 + collaborative filtering + Collaborative and social computing systems and tools + Collaborative and social computing + social networks + data mining + Information systems applications + Computing methodologies + Human-centered computing + World Wide Web + Data mining + Information systems + social network + recommender system + social issues + Machine learning	Algorithms + measurement + human factors	Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering, which makes recommendations to users based on their collective past behavior. While many similarity measures have been proposed and individually evaluated, they have not been evaluated relative to each other in a large real-world environment. We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network. We determine the usefulness of the different recommendations by actually measuring users' propensity to visit and join recommended communities. We also examine how the ordering of recommendations influenced user selection, as well as interesting social issues that arise in recommending communities within a real social network.
78B3C615	Sigkdd Explorations	yiming yang + hao wan + tieyan liu + weiying ma + huajun zeng + zheng chen	2005	Support vector machines classification with a very large-scale taxonomy	 + support vector machine + data analysis + technology assessment + time complexity	eol>Text Categorization + Very Large Web Taxonomies + Threshold Tuning Strategies and Algorithm Complexity	"
Very large-scale classification taxonomies typically have hundreds of thousands of categories, deep hierarchies, and skewed category distribution over documents. However, it is still an open question whether the state-of-the-art technologies in automated text categorization can scale to (and perform well on) such large taxonomies. In this paper, we report the first evaluation of Support Vector Machines (SVMs) in web-page classification over the full taxonomy of the Yahoo! categories. Our accomplishments include: 1) a data analysis on the Yahoo! taxonomy; 2) the development of a scalable system for large-scale text categorization; 3) theoretical analysis and experimental evaluation of SVMs in hierarchical and non-hierarchical settings for classification; 4) an investigation of threshold tuning algorithms with respect to time complexity and their effect on the classification accuracy of SVMs. We found that, in terms of scalability, the hierarchical use of SVMs is efficient enough for very large-scale classification; however, in terms of effectiveness, the performance of SVMs over the Yahoo! Directory is still far from satisfactory, which indicates that more substantial investigation is needed.
"
5BEA49CC	Knowledge Discovery and Data Mining	alexandros kalousis + adam woÅºnica + melanie hilario	2005	Kernels over relational algebra structures	kernel function + relational data + relation algebra + instance based learning		In this paper we present a novel and general framework based on concepts of relational algebra for kernel-based learning over relational schema. We exploit the notion of foreign keys to define a new attribute that we call instance-set and we use this type of attribute to define a tree like structured representation of the learning instances. We define kernel functions over relational schemata which are instances of (Re)-Convolution kernels and use them as a basis for a relational instance-based learning algorithm. These kernels can be considered as being defined over typed and unordered trees where elementary kernels are used to compute the graded similarity between nodes. We investigate their formal properties and evaluate the performance of the relational instance-based algorithm on a number of relational data sets.
7DDE1DBD	Knowledge Discovery and Data Mining	jon kleinberg + christos faloutsos + jure leskovec	2005	Graphs over time: densification laws, shrinking diameters and possible explanations	 + heavy tail + Information systems applications + degree distribution + heavy tailed distribution + Data mining + power law + Information systems	No keyword found	"How do real graphs evolve over time? What are ""normal"" growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time, with the number of edges growing super-linearly in the number of nodes. Second, the average distance between nodes often shrinks over time, in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)).Existing graph generation models do not exhibit these types of behavior, even at a qualitative level. We provide a new graph generator, based on a ""forest fire"" spreading process, that has a simple, intuitive justification, requires very few parameters (like the ""flammability"" of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study."
7F5E78AB	Knowledge Discovery and Data Mining	philip s yu + zhongfei zhang + bo long	2005	Co-clustering by block value decomposition	 + Cluster analysis + data analysis + Retrieval tasks and goals + co occurrence matrix + Data compression + co clustering + Learning paradigms + Information systems applications + Computing methodologies + Data structures + Clustering and classification + Information retrieval + Data layout + matrix decomposition + Clustering + Data mining + Unsupervised learning + Information systems + Machine learning + Data management systems + clustering	+Categories and Subject Descriptors E.4 [Coding and Information Theory+Data compaction and compression+H.3.3 [Information search and Re- trieval+Clustering+I.5.3 [Pattern Recognition+Clus- tering General Terms Algorithms Keywords Co-clustering+Clustering+Matrix Decomposition+Dyadic Data+Hidden Block Structure+Block Value Decomposition (BVD+Non-negative Block Value Decomposition (NBVD	Dyadic data matrices, such as co-occurrence matrix, rating matrix, and proximity matrix, arise frequently in various important applications. A fundamental problem in dyadic data analysis is to find the hidden block structure of the data matrix. In this paper, we present a new co-clustering framework, block value decomposition(BVD), for dyadic data, which factorizes the dyadic data matrix into three components, the row-coefficient matrix R, the block value matrix B, and the column-coefficient matrix C. Under this framework, we focus on a special yet very popular case -- non-negative dyadic data, and propose a specific novel co-clustering algorithm that iteratively computes the three decomposition matrices based on the multiplicative updating rules. Extensive experimental evaluations also demonstrate the effectiveness and potential of this framework as well as the specific algorithms for co-clustering, and in particular, for discovering the hidden block structure in the dyadic data.
7B3A2005	Knowledge Discovery and Data Mining	ruoming jin + gagan agrawal + kaushik sinha	2005	Simultaneous optimization of complex mining tasks with a knowledgeable cache	 + query optimization + data mining + system architecture + Information systems applications + Data mining + Information systems	Fraph pattern mining + multiple query optimization + knowledgeable cache	With an increasing use of data mining tools and techniques, we envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.This paper presents a systematic mechanism to optimize for the above cases, targeting the class of mining queries involving frequent pattern mining on one or multiple datasets. We present a system architecture and propose new algorithms to simultaneously optimize multiple such queries and use a knowledgeable cache to store and utilize the past query results. We have implemented and evaluated our system with both real and synthetic datasets. Our experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries.
7E22E605	Knowledge Discovery and Data Mining	kamal nigam + robert stockton + takashi tomokiyo + matthew hurst + matthew a siegler + natalie glance	2005	Deriving marketing intelligence from online discussion	 + algorithms + computational linguistics + information retrieval + Information retrieval + text mining + machine learning + Information systems	text mining + content systems + computational linguistics + machine learning + information retrieval	Weblogs and message boards provide online forums for discussion that record the voice of the public. Woven into this mass of discussion is a wide range of opinion and commentary about consumer products. This presents an opportunity for companies to understand and respond to the consumer by analyzing this unsolicited feedback. Given the volume, format and content of the data, the appropriate approach to understand this data is to use large-scale web and text data mining technologies.This paper argues that applications for mining large volumes of textual data for marketing intelligence should provide two key elements: a suite of powerful mining and visualization technologies and an interactive analysis environment which allows for rapid generation and testing of hypotheses. This paper presents such a system that gathers and annotates online discussion relating to consumer products using a wide variety of state-of-the-art techniques, including crawling, wrapping, search, text classification and computational linguistics. Marketing intelligence is derived through an interactive analysis framework uniquely configured to leverage the connectivity and content of annotated online discussion.
759B8AF8	Knowledge Discovery and Data Mining	jennifer g dy + ting su + david kaeli + kaushal sanghai	2005	A multinomial clustering model for fast simulation of computer architecture designs	 + Cluster analysis + k means clustering + text clustering + simulation + Learning paradigms + em + Computing methodologies + Unsupervised learning + Computer systems organization + computer architecture + Serial architectures + Machine learning + clustering + k means + Architectures	Program Phase + simulation + clustering + EM + K-means + mixture of multinomials	Computer architects utilize simulation tools to evaluate the merits of a new design feature. The time needed to adequately evaluate the tradeoffs associated with adding any new feature has become a critical issue. Recent work has found that by identifying execution phases present in common workloads used in simulation studies, we can apply clustering algorithms to significantly reduce the amount of time needed to complete the simulation. Our goal in this paper is to demonstrate the value of this approach when applied to the set of industry-standard benchmarks most commonly used in computer architecture studies. We also look to improve upon prior work by applying more appropriate clustering algorithms to identify phases, and to further reduce simulation time.We find that the phase clustering in computer architecture simulation has many similarities to text clustering. In prior work on clustering techniques to reduce simulation time, K-means clustering was used to identify representative program phases. In this paper we apply a mixture of multinomials to the clustering problem and show its advantages over using K-means on simulation data. We have implemented these two clustering algorithms and evaluate how well they can characterize program behavior. By adopting a mixture of multinomials model, we find that we can maintain simulation result fidelity, while greatly reducing overall simulation time. We report results for a range of applications taken from the SPEC2000 benchmark suite.
80AF01A5	Knowledge Discovery and Data Mining	mihai surdeanu + jordi turmo + alicia ageno	2005	A hybrid unsupervised approach for document clustering	 + document clustering + hierarchical clustering + Information retrieval + expectation maximization + Information systems	Unsupervised clustering + EM initialization	We propose a hybrid, unsupervised document clustering approach that combines a hierarchical clustering algorithm with Expectation Maximization. We developed several heuristics to automatically select a subset of the clusters generated by the first algorithm as the initial points of the second one. Furthermore, our initialization algorithm generates not only an initial model for the iterative refinement algorithm but also an estimate of the model dimension, thus eliminating another important element of human supervision. We have evaluated the proposed system on five real-world document collections. The results show that our approach generates clustering solutions of higher quality than both its individual components.
778F119F	Knowledge Discovery and Data Mining	ira s cohen + moises goldszmidt + rob powers	2005	Short term performance forecasting in enterprise systems	three dimensions +  + bayesian network + Management of computing and information systems + System management + File systems management + data mining + Professional topics + Computing methodologies + time series + machine learning + enterprise systems + enterprise system + Modeling and simulation + data gathering + Social and professional topics + Model development and analysis + job scheduling + Modeling methodologies	+Categories and Subject Descriptors+K.6.4 System Man- agement+C.4 Performance of Systems- Modeling techniques+I.5.1 Models- Statistical General Terms+Management+Performance Keywords+performance forecasting+enterprise systems âThis work was conducted during an internship with HP Labs	We use data mining and machine learning techniques to predict upcoming periods of high utilization or poor performance in enterprise systems. The abundant data available and complexity of these systems defies human characterization or static models and makes the task suitable for data mining techniques. We formulate the problem as one of classification: given current and past information about the system's behavior, can we forecast whether the system will meet its performance targets over the next hour? Using real data gathered from several enterprise systems in Hewlett-Packard, we compare several approaches ranging from time series to Bayesian networks. Besides establishing the predictive power of these approaches our study analyzes three dimensions that are important for their application as a stand alone tool. First, it quantifies the gain in accuracy of multivariate prediction methods over simple statistical univariate methods. Second, it quantifies the variations in accuracy when using different classes of system and workload features. Third, it establishes that models induced using combined data from various systems generalize well and are applicable to new systems, enabling accurate predictions on systems with insufficient historical data. Together this analysis offers a promising outlook on the development of tools to automate assignment of resources to stabilize performance, (e.g., adding servers to a cluster) and allow opportunistic job scheduling (e.g., backups or virus scans).
7C011CCB	Knowledge Discovery and Data Mining	martin scholz	2005	Sampling-based sequential subgroup mining	 + Logical and relational learning + Inductive logic learning + sampling + Machine learning + Information systems applications + Machine learning approaches + Computing methodologies + Data mining + Information systems	subgroup discovery + sampling + prior knowledge	Subgroup discovery is a learning task that aims at finding interesting rules from classified examples. The search is guided by a utility function, trading off the coverage of rules against their statistical unusualness. One shortcoming of existing approaches is that they do not incorporate prior knowledge. To this end a novel generic sampling strategy is proposed. It allows to turn pattern mining into an iterative process. In each iteration the focus of subgroup discovery lies on those patterns that are unexpected with respect to prior knowledge and previously discovered patterns. The result of this technique is a small diverse set of understandable rules that characterise a specified property of interest. As another contribution this article derives a simple connection between subgroup discovery and classifier induction. For a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of stratified resampling. The proposed techniques are empirically compared to state of the art subgroup discovery algorithms.
803D9C45	Knowledge Discovery and Data Mining	tao tao + chengxiang zhai	2005	Mining comparable bilingual text corpora for cross-language information integration	 + algorithms + information integration + Information retrieval + text mining + natural language + Information systems	Cross-lingual text mining + comparable corpora + frequency correlation + document alignment	Integrating information in multiple natural languages is a challenging task that often requires manually created linguistic resources such as a bilingual dictionary or examples of direct translations of text. In this paper, we propose a general cross-lingual text mining method that does not rely on any of these resources, but can exploit comparable bilingual text corpora to discover mappings between words and documents in different languages. Comparable text corpora are collections of text documents in different languages that are about similar topics; such text corpora are often naturally available (e.g., news articles in different languages published in the same time period). The main idea of our method is to exploit frequency correlations of words in different languages in the comparable corpora and discover mappings between words in different languages. Such mappings can then be used to further discover mappings between documents in different languages, achieving cross-lingual information integration. Evaluation of the proposed method on a 120MB Chinese-English comparable news collection shows that the proposed method is effective for mapping words and documents in English and Chinese. Since our method only relies on naturally available comparable corpora, it is generally applicable to any language pairs as long as we have comparable corpora.
79FAEEBF	Knowledge Discovery and Data Mining	sven f crone + stefan lessmann + robert stahlbock	2005	Utility based data mining for time series analysis: cost-sensitive learning for neural network predictors	objective function + neural network + multilayer perceptron + data mining + cost efficiency + sum of squares + computational intelligence + ordinary least square + cost function + neural networks + time series analysis	eol>Data Mining + cost-sensitive learning + asymmetric costs + neural networks + time series analysis	In corporate data mining applications, cost-sensitive learning is firmly established for predictive classification algorithms. Conversely, data mining methods for regression and time series analysis generally disregard economic utility and apply simple accuracy measures. Methods from statistics and computational intelligence alike minimise a symmetric statistical error, such as the sum of squared errors, to model ordinary least squares predictors. However, applications in business elucidate that real forecasting problems contain non-symmetric errors. The costs arising from over- versus underprediction are dissimilar for errors of identical magnitude, requiring an ex-post correction of the prediction to derive valid decisions. To reflect this, an asymmetric cost function is developed and employed as the objective function for neural network training, deriving superior forecasts and a cost efficient decision. Experimental results for a business scenario of inventory-levels are computed using a multilayer perceptron trained with different objective functions, evaluating the performance in competition to statistical forecasting methods. Copyright 2005 ACM.
7AB61C49	Knowledge Discovery and Data Mining	g niklas noren + andrew bate + roland orre	2005	A hit-miss model for duplicate detection in the WHO drug safety database	mixture models + record linkage + mixture model + data quality + data cleaning + drug safety	Duplicate detection + hit-miss model + mixture models	"
The WHO Collaborating Centre for International Drug Monitoring in Uppsala, Sweden, maintains and analyses the world's largest database of reports on suspected adverse drug reaction incidents that occur after drugs are introduced on the market. As in other post-marketing drug safety data sets, the presence of duplicate records is an important data quality problem and the detection of all duplicates in the WHO drug safety database remains a formidable challenge, especially since the reports are anonymised before submitted to the database. However, to our knowledge no work has been published on methods for duplicate detection in post-marketing drug safety data. In this paper, we propose a method for probabilistic duplicate detection based on the hit-miss model for statistical record linkage described by Copas & Hilton. We present two new generalisations of the standard hit-miss model: a hit-miss mixture model for errors in numerical record elds and a new method to handle correlated record elds. We demonstrate the e ectiveness of the hit-miss model for duplicate detection in the WHO drug safety database both at identifying the most likely duplicate for a given record (94.7% accuracy) and at discriminating duplicates from random matches (63% recall with 71% precision). The proposed method allows for more e cient data cleaning in post-marketing drug safety data sets, and perhaps other applications throughout the KDD community.
"
7BB64477	Knowledge Discovery and Data Mining	shaul markovitch + saher esmeir	2005	Interruptible anytime algorithms for iterative improvement of decision trees	 + np complete problem + decision tree + resource allocation + decision trees + global optimization + time allocation	eol>Decision trees + Anytime algorithms + Anytime learning + Costquality tradeoÂ® + Hard concepts	"
Finding a minimal decision tree consistent with the examples is an NP-complete problem. Therefore, most of the existing algorithms for decision tree induction use a greedy approach based on local heuristics. These algorithms usually require a Â¯xed small amount of time and result in trees that are not globally optimal. Recently, the LSID3 contract anytime algorithm was introduced to allow using extra resources for building better decision trees. A contract anytime algorithm needs to get its resource allocation a priori. In many cases, however, the time allocation is not known in advance, disallowing the use of contract algorithms. To overcome this problem, in this work we present two interruptible anytime algorithms for inducing decision trees. Interruptible anytime algorithms do not require their resource allocation in advance and thus must be ready to be interrupted and return a valid solution at any moment. The Â¯rst interruptible algorithm we propose is based on a general technique for converting a contract algorithm to an interruptible one by sequencing. The second is an iterative improvement algorithm that repeatedly selects a subtree whose reconstruction is estimated to yield the highest marginal utility and rebuilds it with higher resource allocation. Empirical evaluation shows a good anytime behavior for both algorithms. The iterative improvement algorithm shows smoother performance proÂ¯les which allow more reÂ¯ned control.
"
7FAE0EEF	Knowledge Discovery and Data Mining	xiaodong zhou + giuseppe carenini + raymond t ng	2005	Scalable discovery of hidden emails from large folders	 + algorithms + forensics + Information systems applications + text mining + Data mining + Information systems	text mining + hidden email + forensics	The popularity of email has triggered researchers to look for ways to help users better organize the enormous amount of information stored in their email folders. One challenge that has not been studied extensively in text mining is the identification and reconstruction of hidden emails. A hidden email is an original email that has been quoted in at least one email in a folder, but does not present itself in the same folder. It may have been (un)intentionally deleted or may never have been received. The discovery and reconstruction of hidden emails is critical for many applications including email classification, summarization and forensics. This paper proposes a framework for reconstructing hidden emails using the embedded quotations found in messages further down the thread hierarchy. We evaluate the robustness and scalability of our framework by using the Enron public email corpus. Our experiments show that hidden emails exist widely in that corpus and also that our optimization techniques are effective in processing large email folders.
7F3129C8	Knowledge Discovery and Data Mining	gary m weiss + bibi zabar + kate mccarthy	2005	Does cost-sensitive learning beat sampling for classifying rare classes?	 + data mining + decision trees + sampling + medical diagnosis + sampling technique	eol>Cost-sensitive learning + sampling + data mining + induction + decision trees + rare classes + class imbalance	"
A highly-skewed class distribution usually causes the learned classifier to predict the majority class much more often than the minority class. This is a consequence of the fact that most classifiers are designed to maximize accuracy. In many instances, such as for medical diagnosis, the minority class is the class of primary interest and hence this classification behavior is unacceptable. In this paper, we compare two basic strategies for dealing with data that has a skewed class distribution and non-uniform misclassification costs. One strategy is based on cost-sensitive learning while the other strategy employs sampling to create a more balanced class distribution in the training set. We compare two sampling techniques, up-sampling and down-sampling, to the cost-sensitive learning approach. The purpose of this paper is to determine which technique produces the best overall classifier-and under what circumstances.
"
7D8DA7F0	Knowledge Discovery and Data Mining	inderjit s dhillon + yuqiang guan + brian kulis	2005	A fast kernel-based multilevel algorithm for graph clustering	 + Cluster analysis + graph cut + Partial differential equations + Learning paradigms + Data mining + Information systems + spectral method + Mathematical analysis + Mathematics of computing + k means + spectral clustering + objective function + Retrieval tasks and goals + kernel method + Information systems applications + Computing methodologies + Clustering and classification + Information retrieval + Clustering + Unsupervised learning + graph clustering + eigenvectors + Differential equations + Machine learning + kernel methods + call graph	+Pattern Recognition+Clustering Algorithms	Graph clustering (also called graph partitioning) --- clustering the nodes of a graph --- is an important problem in diverse data mining applications. Traditional approaches involve optimization of graph clustering objectives such as normalized cut or ratio association; spectral methods are widely used for these objectives, but they require eigenvector computation which can be slow. Recently, graph clustering with a general cut objective has been shown to be mathematically equivalent to an appropriate weighted kernel k-means objective function. In this paper, we exploit this equivalence to develop a very fast multilevel algorithm for graph clustering. Multilevel approaches involve coarsening, initial partitioning and refinement phases, all of which may be specialized to different graph clustering objectives. Unlike existing multilevel clustering approaches, such as METIS, our algorithm does not constrain the cluster sizes to be nearly equal. Our approach gives a theoretical guarantee that the refinement step decreases the graph cut objective under consideration. Experiments show that we achieve better final objective function values as compared to a state-of-the-art spectral clustering algorithm: on a series of benchmark test graphs with up to thirty thousand nodes and one million edges, our algorithm achieves lower normalized cut values in 67% of our experiments and higher ratio association values in 100% of our experiments. Furthermore, on large graphs, our algorithm is significantly faster than spectral methods. Finally, our algorithm requires far less memory than spectral methods; we cluster a 1.2 million node movie network into 5000 clusters, which due to memory requirements cannot be done directly with spectral methods.
7F3441E6	Knowledge Discovery and Data Mining	thorsten joachims + filip radlinski	2005	Query chains: learning to rank from implicit feedback	support vector machine + search engines + machine learning + learning to rank + support vector machines + algorithms + search engine + information need		"

"
81023312	Knowledge Discovery and Data Mining	sheng ma + feng liang + tao li + wei peng	2005	An integrated framework on mining logs files for computing system management	 + domain knowledge + system management + Machine learning + Computing methodologies + text mining	System Management + Log Categorization + Event Relationship + Temporal Pattern	Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process that translates domain knowledge into operating rules and policies. This has been well known and experienced as a cumbersome, labor intensive, and error prone process. In addition, this process is difficult to keep up with the rapidly changing environments. In this paper, we will describe our research efforts on establishing an integrated framework for mining system log files for automatic management. In particular, we apply text mining techniques to categorize messages in log files into common situations, improve categorization accuracy by considering the temporal characteristics of log messages, develop temporal mining techniques to discover the relationships between different events, and utilize visualization tools to evaluate and validate the interesting temporal patterns for system management.
76AD8FBE	Knowledge Discovery and Data Mining	srinivasan parthasarathy + hui yang + sameep mehta	2005	A generalized framework for mining spatio-temporal patterns in scientific data	 + Visualization + distance metric + Information systems applications + Human-centered computing + Data mining + scientific data + Visualization application domains + Information systems + Geographic visualization + fluid flow + Spatial-temporal systems + molecular dynamic + Scientific visualization	+Categories and Subject Descriptors H.2.8 [Database Applications]âData Mining+Scientific Databases+Spatial Databases and GIS General Terms Algorithms+Experimentation Keywords Spatial Object Association+Spatio-temporal Association/Episode+Scientific Data	In this paper, we present a general framework to discover spatial associations and spatio-temporal episodes for scientific datasets. In contrast to previous work in this area, features are modeled as geometric objects rather than points. We define multiple distance metrics that take into account objects' extent and thus are more robust in capturing the influence of an object on other objects in spatial neighborhood. We have developed algorithms to discover four different types of spatial object interaction (association) patterns. We also extend our approach to accommodate temporal information and propose a simple algorithm to derive spatio-temporal episodes. We show that such episodes can be used to reason about critical events. We evaluate our framework on real datasets to demonstrate its efficacy. The datasets originate from two different areas: Computational Molecular Dynamics and Computational Fluid Flow. We present results highlighting the importance of the identified patterns and episodes by using knowledge from the underlying domains. We also show that the proposed algorithms scale linearly with respect to the dataset size.
58E50B02	Knowledge Discovery and Data Mining	mirco nanni	2005	Speeding-Up hierarchical agglomerative clustering in presence of expensive metrics	 + empirical study		"
In several contexts and domains, hierarchical agglomerative clustering (HAC) offers best-quality results, but at the price of a high complexity which reduces the size of datasets which can be handled. In some contexts, in particular, computing distances between objects is the most expensive task. In this paper we propose a pruning heuristics aimed at improving performances in these cases, which is well integrated in all the phases of the HAC process and can be applied to two HAC variants: single-linkage and complete-linkage. After describing the method, we provide some theoretical evidence of its pruning power, followed by an empirical study of its effectiveness over different data domains, with a special focus on dimensionality issues.
"
75B8DEAF	Knowledge Discovery and Data Mining	naoki abe	2005	Machine learning paradigms for utility-based data mining	machine learning + reinforcement learning + data mining + active learning	eol>Machine Learning + Cost-sensitive Learning + Reinforcement Learning + Active Learning + Data Mining	"
In this talk, I will describe a number of machine learning paradigms that are relevant to utility-based data mining, and review some key techniques and results in each. learning paradigm, which we might collectively refer to as active on-line learning addresses the issue of optimizing the combination, and trade-off, of losses incurred during data acquisition, and those associated with the predictive quality of the final hypothesis. Some examples of learning paradigms that fall within this general class include the classic bandit problem [3] and its generalizations and associative reinforcement learning [5, 1]. Theories have been developed on these learning paradigms, which provide learning strategies that come with theoretical guarantee on the total losses, inclusive of the two types of losses. Finally, a comprehensive paradigm of machine learning, which includes all of the ones mentioned so far as special cases, is reinforcement learning. Indeed, some authors have embedded instances of utilitybased data mining problems within the MDP framework (e.g. [6]). While the MDP formulation is the most general, it does not necessarily follow that it will be the most effective in practice. When the problem at hand falls into one of the special cases discussed, the theory and methodology in that special case may be the most effective. I hope to draw some examples of real world applications, for which some of these special cases have indeed proved to be satisfactory. [7] F. Provost. Economic machine learning. In Proc. ACM SIGKDD Workshop on Utility-based Data Mining, 2005.
"
796A2DFC	Knowledge Discovery and Data Mining	jiawei han + xiaoxin yin + hongyan liu	2005	An efficient multi-relational NaÃ¯ve Bayesian classifier based on semantic relationship graph	naive bayes + relational database system + classification + design + data mining + management + relational data + search space + bayesian classifier + performance	eol>Classification + NaÃ¯ve Bayes + Data Mining	Classification is one of the most popular data mining tasks with a wide range of applications, and lots of algorithms have been proposed to build accurate and scalable classifiers. Most of these algorithms only take a single table as input, whereas in the real world most data are stored in multiple tables and managed by relational database systems. As transferring data from multiple tables into a single one usually causes many problems, development of multi-relational classification algorithms becomes important and attracts many researchers interests. Existing works about extending NaÃ¯ve Bayes to deal with multi-relational data either have to transform data stored in tables to mainmemory Prolog facts, or limit the search space to only a small subset of real world applications. In this work, we aim at solving these problems and building an efficient, accurate NaÃ¯ve Bayesian classifier to deal with data in multiple tables directly. We propose an algorithm named Graph-NB, which upgrades NaÃ¯ve Bayesian classifier to deal with multiple tables directly. In order to take advantage of linkage relationships among tables, and treat different tables linked to the target table differently, a semantic relationship graph is developed to describe the relationship and to avoid unnecessary joins. Furthermore, to improve accuracy, a pruning strategy is given to simplify the graph to avoid examining too many weakly linked tables. Experimental study on both realworld and synthetic databases shows its high efficiency and good accuracy.
7642CB89	Knowledge Discovery and Data Mining	nitesh v chawla + ajay joshi + lawrence o hall	2005	Wrapper-based computation and evaluation of sampling methods for imbalanced datasets	region of interest + intrusion detection + sampling methods + evaluation function	cost-sensitive learning and evaluation + imbalanced datasets + wrapper + under-sampling + SMOTE	"
Learning from imbalanced datasets presents an interesting problem both from modeling and economy standpoints. When the imbalance is large, classification accuracy on the smaller class(es) tends to be lower. In particular, when a class is of great interest but occurs relatively rarely such as cases of fraud, instances of disease, and regions of interest in largescale simulations, it is important to accurately identify it. It then becomes more costly to misclassify the interesting class. In this paper, we implement a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (SMOTE) to improve minority class accuracy. The f-value serves as the evaluation function. Experimental results show the wrapper approach is effective in optimization of the composite f-value, and reduces the average cost per test example for the datasets considered. We report both average cost per test example and the cost curves in the paper. The true positive rate of the minority class increases significantly without causing a significant change in the f-value. We also obtain the lowest cost per test example, compared to any result we are aware of for the KDD Cup-99 intrusion detection data set.
"
7F5E7D96	Knowledge Discovery and Data Mining	osmar r zaiane + mohammad elhajj	2005	Pattern lattice traversal by selective jumps	manifold learning + search space + dimensionality reduction + graph connectivity		"
Regardless of the frequent patterns to discover, either the full frequent patterns or the condensed ones, either closed or maximal, the strategy always includes the traversal of the lattice of candidate patterns. We study the existing depth versus breadth traversal approaches for generating candidate patterns and propose in this paper a new traversal approach that jumps in the search space among only promising nodes. Our leaping approach avoids nodes that would not participate in the answer set and reduce drastically the number of candidate patterns. We use this approach to efficiently pinpoint maximal patterns at the border of the frequent patterns in the lattice and collect enough information in the process to generate all subsequent patterns.
"
5E9DECC4	Knowledge Discovery and Data Mining	maciej zakrzewicz + marek wojciechowski	2005	On multiple query optimization in data mining	query optimization + data mining + knowledge discovery + management system		Traditional multiple query optimization methods focus on identifying common subexpressions in sets of relational queries and on constructing their global execution plans. In this paper we consider the problem of optimizing sets of data mining queries submitted to a Knowledge Discovery Management System. We describe the problem of data mining query scheduling and we introduce a new algorithm called CCAgglomerative to schedule data mining queries for frequent itemset discovery.
7D90EEC9	Knowledge Discovery and Data Mining	daxin jiang + aidong zhang + jian pei	2005	On mining cross-graph quasi-cliques	 + data mining + patterns + bioinformatics + Information systems applications + Data mining + Information systems	Graph mining + mining multiple data sets + quasi-cliques + bioinformatics	Joint mining of multiple data sets can often discover interesting, novel, and reliable patterns which cannot be obtained solely from any single source. For example, in cross-market customer segmentation, a group of customers who behave similarly in multiple markets should be considered as a more coherent and more reliable cluster than clusters found in a single market. As another example, in bioinformatics, by joint mining of gene expression data and protein interaction data, we can find clusters of genes which show coherent expression patterns and also produce interacting proteins. Such clusters may be potential pathways.In this paper, we investigate a novel data mining problem, mining cross-graph quasi-cliques, which is generalized from several interesting applications such as cross-market customer segmentation and joint mining of gene expression data and protein interaction data. We build a general model for mining cross-graph quasi-cliques, show why the complete set of cross-graph quasi-cliques cannot be found by previous data mining methods, and study the complexity of the problem. While the problem is difficult, we develop an efficient algorithm, Crochet, which exploits several interesting and effective techniques and heuristics to efficaciously mine cross-graph quasi-cliques. A systematic performance study is reported on both synthetic and real data sets. We demonstrate some interesting and meaningful cross-graph quasi-cliques in bioinformatics. The experimental results also show that algorithm Crochet is efficient and scalable.
59B5D161	Knowledge Discovery and Data Mining	licheng jiao + liefeng bo + ling wang	2005	Training support vector machines using greedy stagewise algorithm	support vector machine + computational complexity		Hard margin support vector machines (HM-SVMs) have a risk of getting overfitting in the presence of the noise. Soft margin SVMs deal with this problem by the introduction of the capacity control term and obtain the state of the art performance. However, this disposal leads to a relatively high computational cost. In this paper, an alternative method, greedy stagewise algorithm, named GS-SVMs is presented to deal with the overfitting of HM-SVMs without the introduction of capacity control term. The most attractive property of GS-SVMs is that its computational complexity scales quadratically with the size of training samples in the worst case. Extensive empirical comparisons confirm the feasibility and validity GS-SVMs.
805A1791	Knowledge Discovery and Data Mining	david jensen + kelly palmer + jennifer neville + ozgur Åimsek + john komoroske + henry g goldberg	2005	Using relational knowledge discovery to prevent securities fraud	private sector + knowledge discovery + statistical relational learning	Fraud detection + statistical relational learning + relational probability trees	"
We describe an application of relational knowledge discovery to a key regulatory mission of the National Association of Securities Dealers (NASD). NASD is the world's largest private-sector securities regulator, with responsibility for preventing and discovering misconduct among securities brokers. Our goal was to help focus NASD's limited regulatory resources on the brokers who are most likely to engage in securities violations. Using statistical relational learning algorithms, we developed models that rank brokers with respect to the probability that they would commit a serious violation of securities regulations in the near future. Our models incorporate organizational relationships among brokers (e.g., past coworker), which domain experts consider important but have not been easily used before now. The learned models were subjected to an extensive evaluation using more than 18 months of data unseen by the model developers and comprising over two person weeks of eoffrt by NASD sta.ff Model predictions were found to correlate highly with the subjective evaluations of experienced NASD examiners. Furthermore, in all performance measures, our models performed as well as or better than the handcrafted rules that are currently in use at NASD.
"
75CC6BB8	Knowledge Discovery and Data Mining	alexander hinneburg + spiros papadimitriou + aristides gionis + panayiotis tsaparas	2005	Dimension induced clustering	 + correlation dimension + fractal dimension + clustering + Information systems	Fractal Dimension + Clustering	It is commonly assumed that high-dimensional datasets contain points most of which are located in low-dimensional manifolds. Detection of low-dimensional clusters is an extremely useful task for performing operations such as clustering and classification, however, it is a challenging computational problem. In this paper we study the problem of finding subsets of points with low intrinsic dimensionality. Our main contribution is to extend the definition of fractal correlation dimension, which measures average volume growth rate, in order to estimate the intrinsic dimensionality of the data in local neighborhoods. We provide a careful analysis of several key examples in order to demonstrate the properties of our measure. Based on our proposed measure, we introduce a novel approach to discover clusters with low dimensionality. The resulting algorithms extend previous density based measures, which have been successfully used for clustering. We demonstrate the effectiveness of our algorithms for discovering low-dimensional m-flats embedded in high dimensional spaces, and for detecting low-rank sub-matrices.
7FF01ED6	Knowledge Discovery and Data Mining	mohammed j zaki + ira assent + thomas seidl + markus peters	2005	CLICKS: an effective algorithm for mining subspace clusters in categorical datasets	clustering + data mining + categorical data	Clustering + Categorical data + k-Partite graph + Maximal cliques	We present a novel algorithm called CLICKS, that finds clusters in categorical datasets based on a search for k-partite maximal cliques. Unlike previous methods, CLICKS mines subspace clusters. It uses a selective vertical method to guarantee complete search. CLICKS outperforms previous approaches by over an order of magnitude and scales better than any of the existing method for high-dimensional datasets. These results are demonstrated in a comprehensive performance study on real and synthetic datasets. Copyright 2005 ACM.
7F763AA6	Knowledge Discovery and Data Mining	sangwook kim + jeehee yoon + sanghyun park + jungim won	2005	A novel indexing method for efficient sequence matching in large DNA database environment	indexing + indexation + molecular biology + dna sequence	DNA databases + DNA sequence matching + indexing	In molecular biology, DNA sequence matching is one of the most crucial operations. Since DNA databases contain a huge volume of sequences, fast indexes are essential for efficient processing of DNA sequence matching. In this paper, we first point out the problems of the suffix tree, an index structure widely-used for DNA sequence matching, in the respects of the storage overhead, search performance, and difficulty in seamless integration with DBMS. Then, we propose a new index structure that resolves such problems. The proposed index structure consists of the two parts: the primary part realizes the trie as binary bit-string representation without any pointers, and the secondary part helps fast accesses of leaf nodes of the trie that need to be accessed for post-processing. We also suggest efficient algorithms based on that index for DNA sequence matching. To verify the superiority of the proposed approach, we conduct performance evaluation via a series of experiments. The results reveal that the proposed approach, which requires smaller storage space, can be a few orders of magnitude faster than the suffix tree.
7968250B	Knowledge Discovery and Data Mining	srinivasan parthasarathy + ruoming jin + gagan agrawal + chao wang + d a polshakov	2005	Discovering frequent topological structures from graph datasets	protein structure +  + drug discovery + social network analysis + Information systems applications + Data mining + Information systems	Graph mining + topological minor + frequent graph pattern	The problem of finding frequent patterns from graph-based datasets is an important one that finds applications in drug discovery, protein structure analysis, XML querying, and social network analysis among others. In this paper we propose a framework to mine frequent large-scale structures, formally defined as frequent topological structures, from graph datasets. Key elements of our framework include, fast algorithms for discovering frequent topological patterns based on the well known notion of a topological minor, algorithms for specifying and pushing constraints deep into the mining process for discovering constrained topological patterns, and mechanisms for specifying approximate matches when discovering frequent topological patterns in noisy datasets. We demonstrate the viability and scalability of the proposed algorithms on real and synthetic datasets and also discuss the use of the framework to discover meaningful topological structures from protein structure data.
7F93CB4B	Knowledge Discovery and Data Mining	aleks jakulin + janez demsar + blaÅ¾ zupan + ivan bratko + martin moÅ¾ina	2005	Nomograms for visualizing support vector machines	support vector machines + support vector machine + odd ratio + computer and information science + visualization + nomogram + theory + logistic regression + machine + machine learning	nomogram + visualization + support vector machines + machine learning	"
We propose a simple yet potentially very effective way of visualizing trained support vector machines. Nomograms are an established model visualization technique that can graphically encode the complete model on a single page. The dimensionality of the visualization does not depend on the number of attributes, but merely on the properties of the kernel. To represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms, we employ logistic regression to convert the distance from the separating hyperplane into a probability. Case studies on selected data sets show that for a technique thought to be a black-box, nomograms can clearly expose its internal structure. By providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predictive factors.
"
7FC2DE18	Knowledge Discovery and Data Mining	wenchieh yang + mingsyan chen + kunta chuang	2005	Progressive sampling for association rules based on sampling error estimation	association rule + sample size + sampling error + synthetic data		We explore in this paper a progressive sampling algorithm, called Sampling Error Estimation (SEE), which aims to identify an appropriate sample size for mining association rules. SEE has two advantages over previous works in the literature. First, SEE is highly efficient because an appropriate sample size can be determined without the need of executing association rules. Second, the identified sample size of SEE is very accurate, meaning that association rules can be highly efficiently executed on a sample of this size to obtain a sufficiently accurate result. This is attributed to the merit of SEE for being able to significantly reduce the influence of randomness by examining several samples with the same size in one database scan. As validated by experiments on various real data and synthetic data, SEE can achieve very prominent improvement in efficiency and also the resulting accuracy over previous works.
7F349FEC	Knowledge Discovery and Data Mining	naren ramakrishnan + mohammed j zaki	2005	Reasoning about sets using redescription mining	 + conceptual clustering + data mining + Machine learning + Information systems applications + Computing methodologies + Data mining + association rule mining + Information systems	redescription + data mining + minimal generators + closed itemsets	Redescription mining is a newly introduced data mining problem that seeks to find subsets of data that afford multiple definitions. It can be viewed as a generalization of association rule mining, from finding implications to equivalences; as a form of conceptual clustering, where the goal is to identify clusters that afford dual characterizations; and as a form of constructive induction, to build features based on given descriptors that mutually reinforce each other. In this paper, we present the use of redescription mining as an important tool to reason about a collection of sets, especially their overlaps, similarities, and differences. We outline algorithms to mine all minimal (non-redundant) redescriptions underlying a dataset using notions of minimal generators of closed itemsets. We also show the use of these algorithms in an interactive context, supporting constraint-based exploration and querying. Specifically, we showcase a bioinformatics application that empowers the biologist to define a vocabulary of sets underlying a domain of genes and to reason about these sets, yielding significant biological insight.
7C21F0A8	Knowledge Discovery and Data Mining	mark sandler	2005	On the use of linear programming for unsupervised text classification	 + l1 norm + latent semantic indexing + supervised learning + linear programming + Information extraction + mixture model + Data mining + Information systems + Theory of computation + dimensionality reduction + computer science + Document filtering + singular value decomposition + latent class model + feature space + technical report + Retrieval tasks and goals + Information systems applications + linear program + Clustering and classification + Information retrieval + Clustering + unsupervised learning + support vector machine + mixture models + Design and analysis of algorithms	+Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval+Clustering+Infor- mation Filtering+F.2.2 [Analysis of Algorithms and Problem Complexity+Non-numerical Algorithms and Problems+H.1.2 [Information Systems+Models and PrinciplesUser/Machine sys- temsHuman information processing General Terms Algorithms+theory+experimentation. Keywords Dimensionality reduction+generative models+mixture models+un- supervised learning+L1 norm	We propose a new algorithm for dimensionality reduction and unsupervised text classification. We use mixture models as underlying process of generating corpus and utilize a novel, L1-norm based approach introduced by Kleinberg and Sandler [19]. We show that our algorithm performs extremely well on large datasets, with peak accuracy approaching that of supervised learning based on Support Vector Machines (SVMs) with large training sets. The method is based on the same idea that underlies Latent Semantic Indexing (LSI). We find a good low-dimensional subspace of a feature space and project all documents into it. However our projection minimizes different error, and unlike LSI we build a basis, that in many cases corresponds to the actual topics. We present results of testing of our algorithm on the abstracts of arXiv - an electronic repository of scientific papers, and the 20 Newsgroup dataset - a small snapshot of 20 specific newsgroups.
77FA8AA8	Knowledge Discovery and Data Mining	herna l viktor + hongyu guo	2005	Mining relational databases with multi-view learning	data representation + structured data + aggregation + relational database + data mining	eol>Data Mining + Multi-view Classification + Relational Database + Multi-relational Data Mining + Aggregation	"
Most of today's structured data resides in relational databases where multiple relations are formed by foreign key joins. In recent years, the field of data mining has played a key role in helping humans analyze and explore large databases. Unfortunately, most methods only utilize âflatâ data representations. Thus, to apply these single-table data mining techniques, we are forced to incur a computational penalty by first converting the data into this âflatâ form. As a result of this transformation, the data not only loses its compact representation but the semantic information present in the relations are reduced or eliminated. In this paper, we describe a classification approach, which addresses this issue by operating directly on relational databases. The approach, called MVC (Multi-View Classification), is based on a multi-view learning framework. In this framework, the target concept is represented in different views and then independently learned using single-table data mining techniques. After constructing multiple classifiers for the target concept in each view, the learners are validated and combined by a meta-learning algorithm. Two methods are employed in the MVC approach, namely (1) target concept propagation and (2) multi-view learning. The propagation method constructs training sets directly from relational databases for use by the multi-view learners. The learning method employs traditional single-table mining techniques to mine data straight from a multi-relational database. Our experiments on benchmark real-world databases show that the MVC method achieves promising results in terms of overall accuracy obtained and run time, when compared with the FOIL and CrossMine learning methods.
"
7665DA44	Sigkdd Explorations	raymond j mooney + razvan c bunescu	2005	Mining knowledge from text using information extraction	 + information extraction + structured data + data mining + text mining + natural language		"
An important approach to text mining involves the use of natural-language information extraction. Information extraction (IE) distills structured data or knowledge from unstructured text by identifying references to named entities as well as stated relationships between such entities. IE systems can be used to directly extricate abstract knowledge from a text corpus, or to extract concrete data from a set of documents which can then be further analyzed with traditional data-mining techniques to discover more general patterns. We discuss methods and implemented systems for both of these approaches and summarize results on mining real text corpora of biomedical abstracts, job announcements, and product descriptions. We also discuss challenges that arise when employing current information extraction technology to discover knowledge in text.
"
7909BB85	Knowledge Discovery and Data Mining	ying yang + xingquan zhu + xindong wu	2005	Combining proactive and reactive predictions for data streams	 + prediction model + Machine learning + Information systems applications + Computing methodologies + proactive learning + empirical evidence + Data mining + Information systems	No keyword found	Mining data streams is important in both science and commerce. Two major challenges are (1) the data may grow without limit so that it is difficult to retain a long history; and (2) the underlying concept of the data may change over time. Different from common practice that keeps recent raw data, this paper uses a measure of conceptual equivalence to organize the data history into a history of concepts. Along the journey of concept change, it identifies new concepts as well as re-appearing ones, and learns transition patterns among concepts to help prediction. Different from conventional methodology that passively waits until the concept changes, this paper incorporates proactive and reactive predictions. In a proactive mode, it anticipates what the new concept will be if a future concept change takes place, and prepares prediction strategies in advance. If the anticipation turns out to be correct, a proper prediction model can be launched instantly upon the concept change. If not, it promptly resorts to a reactive mode: adapting a prediction model to the new data. A system RePro is proposed to implement these new ideas. Experiments compare the system with representative existing prediction methods on various benchmark data sets that represent diversified scenarios of concept change. Empirical evidence demonstrates that the proposed methodology is an effective and efficient solution to prediction for data streams.
764C4670	Knowledge Discovery and Data Mining	richard cole + xiaojian zhao + dennis shasha	2005	Fast window correlations over uncooperative time series	 + Language features + wavelet transform + Software notations and tools + fast fourier transform + Information systems + Theory and algorithms for application domains + Theory of computation + white noise + Data types and structures + Data management systems + Database theory + Software and its engineering + time series + Record storage systems + Data structures + randomized algorithm + sliding window + correlation + Data structures and algorithms for data management + Information storage systems + General programming languages + randomized algorithms + Data structures design and analysis + combinatorial design + Design and analysis of algorithms	+Correlation+Randomized algorithms	"Data arriving in time order (a data stream) arises in fields including physics, finance, medicine, and music, to name a few. Often the data comes from sensors (in physics and medicine for example) whose data rates continue to improve dramatically as sensor technology improves. Further, the number of sensors is increasing, so correlating data between sensors becomes ever more critical in order to distill knowlege from the data. In many applications such as finance, recent correlations are of far more interest than long-term correlation, so correlation over sliding windows (windowed correlation) is the desired operation. Fast response is desirable in many applications (e.g., to aim a telescope at an activity of interest or to perform a stock trade). These three factors -- data size, windowed correlation, and fast response -- motivate this work.Previous work [10, 14] showed how to compute Pearson correlation using Fast Fourier Transforms and Wavelet transforms, but such techniques don't work for time series in which the energy is spread over many frequency components, thus resembling white noise. For such ""uncooperative"" time series, this paper shows how to combine several simple techniques -- sketches (random projections), convolution, structured random vectors, grid structures, and combinatorial design -- to achieve high performance windowed Pearson correlation over a variety of data sets."
80148FE5	Knowledge Discovery and Data Mining	raymond j mooney + maytal saartsechansky + prem melville + foster provost	2005	Economical active feature-value acquisition through Expected Utility estimation	expected utility +  + active learning + prediction model + data mining + cost effectiveness + machine learning + missing values		"
In many classification tasks training data have missing feature values that can be acquired at a cost. For building accurate predictive models, acquiring all missing values is often prohibitively expensive or unnecessary, while acquiring a random subset of feature values may not be most effective. The goal of active feature-value acquisition is to incrementally select feature values that are most cost-effective for improving the model's accuracy. We present two policies, Sampled Expected Utility and Expected Utility-ES, that acquire feature values for inducing a classification model based on an estimation of the expected improvement in model accuracy per unit cost. A comparison of the two policies to each other and to alternative policies demonstrate that Sampled Expected Utility is preferable as it effectively reduces the cost of producing a model of a desired accuracy and exhibits a consistent performance across domains.
"
75F9A7E6	Knowledge Discovery and Data Mining	katsumi takahashi + noriaki kawamae	2005	Information retrieval based on collaborative filtering with latent interest semantic map	 + collaborative filtering + Retrieval models and ranking + latent semantic indexing + Retrieval tasks and goals + probabilistic latent semantic analysis + information retrieval + Information systems applications + Clustering and classification + Information retrieval + Information extraction + latent semantic analysis + Clustering + Data mining + user model + Information systems + Document filtering + bayesian statistics	+Categories and Subject Descriptors H.3.1 [Analysis and Indexing+Clustering+Information filtering+MAP+PLSA+Retrieval models General Terms+Documentation Keywords Collaborative Filtering+User behavior+Query suggestion+Document categorization+Relationship analysis+Search results+Latent semantic indexing+Bayesian statistics	In this paper, we propose an information retrieval model called Latent Interest Semantic Map (LISM), which features retrieval composed of both Collaborative Filtering(CF) and Probabilistic Latent Semantic Analysis (PLSA). The motivation behind this study is that the relation between users and documents can be explained by the two different latent classes, where users belong probabilistically in one or more classes with the same interest groups, while documents also belong probabilistically in one or more class with the same topic groups. The novel aspect of LISM is that it simultaneously provides a user model and latent semantic analysis in one map. This benefit of LISM is to enable collaborative filtering in terms of user interest and document topic and thus solve the cold start problem.
7DAEDCC3	Knowledge Discovery and Data Mining	edward y chang + gang wu + navneet panda	2005	Formulating distance functions via the kernel trick	projective space +  + kernel trick + context dependent + distance function + Information storage systems + data mining + information retrieval + Information retrieval + Information systems	+kernel trick	"Tasks of data mining and information retrieval depend on a good distance function for measuring similarity between data instances. The most effective distance function must be formulated in a context-dependent (also application-, data-, and user-dependent) way. In this paper, we propose to learn a distance function by capturing the nonlinear relationships among contextual information provided by the application, data, or user. We show that through a process called the ""kernel trick,"" such nonlinear relationships can be learned efficiently in a projected space. Theoretically, we substantiate that our method is both sound and optimal. Empirically, using several datasets and applications, we demonstrate that our method is effective and useful."
78FDCAC9	Knowledge Discovery and Data Mining	gian fulgoni	2005	Mining the internet: the eighth wonder of the world	 + distance function + pattern recognition	No keyword found	The Internet takes behavioral consumer research to a new level by providing the ability to passively and continuously monitor the complete online behavior of millions of consumers in an opt-in, privacy protected manner. Imagine the analytical possibilities if every site visited, every page viewed, content seen, transaction conducted ..... all of this granularity in behavior --- was continuously captured with explicit consumer permission for millions of consumers and privacy was protected. What unique insights could one gain into consumers' behavior, their interests, passions and lifestyles? What behavior could be predicted? What commercial applications would be possible.
7AC243CA	Knowledge Discovery and Data Mining	luis torgo	2005	Regression error characteristic surfaces	 + Software selection and adaptation + Management of computing and information systems + Cross-computing tools and techniques + Software performance + Professional topics + Extra-functional properties + Software management + Performance + Social and professional topics + General and reference + Software organization and properties + Software and its engineering	Model comparisons + evaluation metrics + regression problems	This paper presents a generalization of Regression Error Characteristic (REC) curves. REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems. REC curves provide useful information for analyzing the performance of models, particularly when compared to error statistics like for instance the Mean Squared Error. In this paper we present Regression Error Characteristic (REC) surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable, i.e. the joint cumulative distribution function of the errors and the target variable. This provides a more detailed analysis of the performance of models when compared to REC curves. This extra detail is particularly relevant in applications with non-uniform error costs, where it is important to study the performance of models for specific ranges of the target variable. In this paper we present the notion of REC surfaces, describe how to use them to compare the performance of models, and illustrate their use with an important practical class of applications: the prediction of rare extreme values.
5880E10A	Knowledge Discovery and Data Mining	nguyen anh tuan + le hoai bac	2005	Using rough set in feature selection and reduction in face recognition problem	feature selection + rough set theory + rough set + face recognition + pattern recognition		Feature selection and reduction are fundamental steps in pattern recognition problems. The idea of reducts in rough set theory has encouraged many researchers in studying the effectiveness of rough set theory in the problem mentioned above. Through results of experiments in this article, we will show that rough set theory, accompanied by appropriate heuristics, can increase significantly the systemâs recognition accuracy.
75650ECF	Knowledge Discovery and Data Mining	christine korner + stefan wrobel	2005	Bias-free hypothesis evaluation in multirelational domains	machine learning + unbiased estimator + sampling error + relational database + aggregation + data mining + cross validation + random sampling + sampling technique		"
In propositional domains, using a separate test set via random sampling or cross validation is generally considered to be an unbiased estimator of true error. In multirelational domains, previous work has already noted that linkage of objects may cause these procedures to be biased, and has proposed corrected sampling procedures. However, as we show in this paper, the existing procedures only address one particular case of bias introduced by linkage. We recall that in the propositional case cross validation measures off-training set (OTS) error and not true error and illustrate the difference with a small experiment. In the multirelational case, we show that the distinction between training and test set needs to be carefully extended based on a graph of potentially linked objects, and on their assumed probabilities of reoccurrence. We demonstrate that the bias due to linkage to known objects varies with the chosen proportion of the training/test split and present an algorithm, generalized subgraph sampling, that is guaranteed to avoid bias in the test set for more generalized cases.
"
5D5AF0B0	Knowledge Discovery and Data Mining	nematollaah shiri + sudhir p mudur + bhushan shankar suryavanshi	2005	Adaptive Web Usage Profiling	behavior change + user experience		Web usage models and profiles capture significant interests and trends from past accesses. They are used to improve user experience, say through recommendation of pages, pre-fetching of pages, etc. While browsing behavior changes dynamically over time, many web usage modeling techniques are static due to prohibitive model compilation times and also lack of fast incremental update mechanism. However, profiles have to be maintained so that they dynamically adapt to new interests and trends, since otherwise their use can lead to poor, irrelevant, and mis-targeted recommendations in personalization systems. We present a new profile maintenance scheme, which extends the Relational Fuzzy Subtractive Clustering (RFSC) technique and enables efficient incremental update of usage profiles. An impact factor is defined whose value can be used to decide the need for recompilation. The results from extensive experiments on a large real dataset of web logs show that the proposed maintenance technique, with considerably reduced computational costs, is almost as good as complete remodeling.
80A53AC1	Knowledge Discovery and Data Mining	tao li	2005	A general model for clustering binary data	 + Cluster analysis + Philosophical/theoretical foundations of artificial intelligence + bag of words + Learning paradigms + Machine learning + data clustering + Computing methodologies + clustering + Artificial intelligence + Unsupervised learning	Clustering + Binary Data + Matrix Approximation + General Model	"Clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity classes. This paper studies the problem of clustering binary data. This is the case for market basket datasets where the transactions contain items and for document datasets where the documents contain ""bag of words"". The contribution of the paper is three-fold. First a general binary data clustering model is presented. The model treats the data and features equally, based on their symmetric association relations, and explicitly describes the data assignments as well as feature assignments. We characterize several variations with different optimization procedures for the general model. Second, we also establish the connections between our clustering model with other existing clustering methods. Third, we also discuss the problem for determining the number of clusters for binary clustering. Experimental results show the effectiveness of the proposed clustering model."
75DA2821	Knowledge Discovery and Data Mining	joachim m buhmann + tilman lange	2005	Combining partitions by probabilistic label aggregation	 + exploratory data analysis + Machine learning + model selection + data clustering + Computing methodologies + clustering + probabilistic model	+Consensus Partition+Re-sampling	Data clustering represents an important tool in exploratory data analysis. The lack of objective criteria render model selection as well as the identification of robust solutions particularly difficult. The use of a stability assessment and the combination of multiple clustering solutions represents an important ingredient to achieve the goal of finding useful partitions. In this work, we propose a novel way of combining multiple clustering solutions for both, hard and soft partitions: the approach is based on modeling the probability that two objects are grouped together. An efficient EM optimization strategy is employed in order to estimate the model parameters. Our proposal can also be extended in order to emphasize the signal more strongly by weighting individual base clustering solutions according to their consistency with the prediction for previously unseen objects. In addition to that, the probabilistic model supports an out-of-sample extension that (i) makes it possible to assign previously unseen objects to classes of the combined solution and (ii) renders the efficient aggregation of solutions possible. In this work, we also shed some light on the usefulness of such combination approaches. In the experimental result section, we demonstrate the competitive performance of our proposal in comparison with other recently proposed methods for combining multiple classifications of a finite data set.
59A615C4	Knowledge Discovery and Data Mining	kenneth sosrensen + gerrit k janssens + koen vanhoof + arthur limere	2005	Analysis of company growth data using genetic algorithms on binary trees	genetic algorithm + classification tree + data mining + binary tree		This paper investigates why some companies grow faster than others, by data mining a survey of a large number of companies in Flanders (the northern part of Belgium). Faster or slower average growth over a time period is explained by building a classification tree containing several categorical variables (both quantitative and qualitative). The technique used â called genAID â splits the population at different levels. It is inspired by the Automatic Interaction Detector (AID) technique to find trees that explain the variability in average growth but uses a genetic algorithm to overcome some of the drawbacks of AID. Classical AID or other tree-growing techniques usually generate a single tree for interpretation. This approach has been criticized because, due to the artifacts of data, spurious interactions may occur. genAID offers the user-analyst a set of trees, which are the best ones found over a number of generations of the genetic algorithm. The user-analyst is then offered the choice of choosing a tree by trading off explanatory power against either the ease of understanding or the conformity with an existing theory.
78DD5171	Knowledge Discovery and Data Mining	x jasmine zhou + jiawei han + xifeng yan	2005	Mining closed relational graphs with connectivity constraints	relational graph + large scale network modeling + graph theory + data mining + pattern discovery + connectivity constraints + relational databases + graph + social network + graph connectivity + biological network + connectivity + pattern clustering + data structures + connected graph	AuthorProvided Keywords Not Found	Relational graphs are widely used in modeling large scale networks such as biological networks and social networks. In a relational graph, each node represents a distinct entity while each edge represents a relationship between entities. Various algorithms were developed to discover interesting patterns from a single relational graph (Z. Wu et al., 1993). However, little attention has been paid to the patterns that are hidden in multiple relational graphs. One interesting pattern in relational graphs is frequent highly connected subgraph which can identify recurrent groups and clusters. In social networks, this kind of pattern corresponds to communities where people are strongly associated. For example, if several researchers co-author some papers, attend the same conferences, and refer their works from each other, it strongly indicates that they are studying the same research theme.
5AA610D3	Knowledge Discovery and Data Mining	michal malowiecki + ngoc thanh nguyen	2005	Using consensus susceptibility and consistency measures for inconsistent knowledge management	knowledge management		Conflicts may appear during knowledge processing, where some knowledge pieces are different but they refer to the same subject. Consensus methods are useful in processing inconsistent knowledge. However, for almost conflict situations consensus may be determined but it is not always sensible. In this paper we investigate the aspect of reasonableness of consensus. For this aim we define two notions: consensus susceptibility and consistency measure. Owing to them one may get to know when it is worth to determine a consensus for a conflict. We show the dependencies between consistency and the consensus susceptibility for conflict situations. Some results of the analysis are presented.
5C8DCBD0	Knowledge Discovery and Data Mining	yanping zhang + tao wu + ling zhang	2005	A kernel function method in clustering	k means clustering + data mining + kernel function + k means + cluster analysis + clustering		Cluster analysis is one of main methods used in data mining. So far there have existed many cluster analysis approaches such as partitioning method, density-based, k-means, k-nearest neighborhood, etc. Recently, some researchers have explored a few kernel-based clustering methods, e.g., kernel-based K-means clustering. The new algorithms have demonstrated some advantages. So itâs needed to explore the basic principle underlain the algorithms such as whether the kernel function transformation can increase the separability of the input data in clustering and how to use the principle to construct new clustering methods. In this paper, we will discuss the problems.
